{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16bb7d4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\LUISHE~1\\AppData\\Local\\Temp/ipykernel_12468/1280087314.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;31m# Open pickle file and load data: d\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data.pkl'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m     \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data.pkl'"
     ]
    }
   ],
   "source": [
    "#Loading a pickled file\n",
    "\n",
    "#There are a number of datatypes that cannot be saved \n",
    "#easily to flat files, such as lists and dictionaries. If you want \n",
    "#your files to be human readable, you may want to save \n",
    "#them as text files in a clever manner. JSONs, which you will \n",
    "#see in a later chapter, are appropriate for Python \n",
    "#dictionaries.\n",
    "\n",
    "#However, if you merely want to be able to import them into \n",
    "#Python, you can serialize them. All this means is \n",
    "#converting the object into a sequence of bytes, or a \n",
    "#bytestream.\n",
    "\n",
    "#In this exercise, you'll import the pickle package, open a \n",
    "#previously pickled data structure from a file and load it.\n",
    "\n",
    "#Instructions\n",
    "\n",
    "#Import the pickle package.\n",
    "\n",
    "#Complete the second argument of open() so that it is \n",
    "#read only for a binary file. This argument will be a string \n",
    "#of two letters, one signifying 'read only', the other 'binary'.\n",
    "\n",
    "#Pass the correct argument to pickle.load(); it should \n",
    "#use the variable that is bound to open.\n",
    "\n",
    "#Print the data, d.\n",
    "\n",
    "#Print the datatype of d; take your mind back to your \n",
    "#previous use of the function type().\n",
    "\n",
    "# Import pickle package\n",
    "import pickle\n",
    "\n",
    "# Open pickle file and load data: d\n",
    "with open('data.pkl', 'rb') as file:\n",
    "    d = pickle.load(file)\n",
    "\n",
    "# Print d\n",
    "print(d)\n",
    "\n",
    "# Print datatype of d\n",
    "print(type(d))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37c380c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2002', '2004']\n"
     ]
    }
   ],
   "source": [
    "#Listing sheets in Excel files\n",
    "\n",
    "#Whether you like it or not, any working data scientist will \n",
    "#need to deal with Excel spreadsheets at some point in \n",
    "#time. You won't always want to do so in Excel, however!\n",
    "\n",
    "#Here, you'll learn how to use pandas to import Excel \n",
    "#spreadsheets and how to list the names of the sheets in \n",
    "#any loaded .xlsx file.\n",
    "\n",
    "#Recall from the video that, given an Excel file imported \n",
    "#into a variable spreadsheet, you can retrieve a list of the \n",
    "#sheet names using the attribute spreadsheet.sheet_names.\n",
    "\n",
    "#Specifically, you'll be loading and checking out the \n",
    "#spreadsheet 'battledeath.xlsx', modified from the \n",
    "#Peace Research Institute Oslo's (PRIO) dataset. This data \n",
    "#contains age-adjusted mortality rates due to war \n",
    "#in various countries over several years.\n",
    "\n",
    "#Instructions\n",
    "\n",
    "#Assign the spreadsheet filename (provided above) to the \n",
    "#variable file.\n",
    "\n",
    "#Pass the correct argument to pd.ExcelFile() to load \n",
    "#the file using pandas, assigning the result to the variable \n",
    "#xls.\n",
    "\n",
    "#Print the sheetnames of the Excel spreadsheet by \n",
    "#passing the necessary argument to the print() \n",
    "#function.\n",
    "\n",
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Assign spreadsheet filename: file\n",
    "file = 'battledeath.xlsx'\n",
    "\n",
    "# Load spreadsheet: xls\n",
    "xls = pd.ExcelFile(file)\n",
    "\n",
    "# Print sheet names\n",
    "print(xls.sheet_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18ca978c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  War(country)      2004\n",
      "0  Afghanistan  9.451028\n",
      "1      Albania  0.130354\n",
      "2      Algeria  3.407277\n",
      "3      Andorra  0.000000\n",
      "4       Angola  2.597931\n",
      "  War, age-adjusted mortality due to       2002\n",
      "0                        Afghanistan  36.083990\n",
      "1                            Albania   0.128908\n",
      "2                            Algeria  18.314120\n",
      "3                            Andorra   0.000000\n",
      "4                             Angola  18.964560\n"
     ]
    }
   ],
   "source": [
    "#Importing sheets from Excel files\n",
    "\n",
    "#In the previous exercises, you saw that the Excel file \n",
    "#contains two sheets, '2002' and '2004'. The next step is \n",
    "#to import these.\n",
    "\n",
    "#In this exercise, you'll learn how to import any given sheet \n",
    "#of your loaded .xlsx file as a DataFrame. You'll be able to \n",
    "#do so by specifying either the sheet's name or its index.\n",
    "\n",
    "#The spreadsheet 'battledeath.xlsx' is already loaded as xls.\n",
    "\n",
    "#Instructions\n",
    "\n",
    "#Load the sheet '2004' into the DataFrame df1 using \n",
    "#its name as a string.\n",
    "\n",
    "#Print the head of df1 to the shell.\n",
    "\n",
    "#Load the sheet 2002 into the DataFrame df2 using its \n",
    "#index (0).\n",
    "\n",
    "#Print the head of df2 to the shell.\n",
    "\n",
    "# Load a sheet into a DataFrame by name: df1\n",
    "df1 = xls.parse('2004')\n",
    "\n",
    "# Print the head of the DataFrame df1\n",
    "print(df1.head())\n",
    "\n",
    "# Load a sheet into a DataFrame by index: df2\n",
    "df2=xls.parse(0)\n",
    "\n",
    "# Print the head of the DataFrame df2\n",
    "print(df2.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0cbae2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Country  AAM due to War (2002)\n",
      "0              Albania               0.128908\n",
      "1              Algeria              18.314120\n",
      "2              Andorra               0.000000\n",
      "3               Angola              18.964560\n",
      "4  Antigua and Barbuda               0.000000\n",
      "               Country\n",
      "0              Albania\n",
      "1              Algeria\n",
      "2              Andorra\n",
      "3               Angola\n",
      "4  Antigua and Barbuda\n"
     ]
    }
   ],
   "source": [
    "#Customizing your spreadsheet import\n",
    "\n",
    "#Here, you'll parse your spreadsheets and use additional \n",
    "#arguments to skip rows, rename columns and select only \n",
    "#particular columns.\n",
    "\n",
    "#The spreadsheet 'battledeath.xlsx' is already loaded as xls.\n",
    "\n",
    "#As before, you'll use the method parse(). This time, \n",
    "#however, you'll add the additional arguments skiprows, \n",
    "#names and usecols. These skip rows, name the columns \n",
    "#and designate which columns to parse, respectively. All \n",
    "#these arguments can be assigned to lists containing the \n",
    "#specific row numbers, strings and column numbers, as \n",
    "#appropriate.\n",
    "\n",
    "#Instructions\n",
    "\n",
    "#Parse the first sheet by index. In doing so, skip the first \n",
    "#row of data and name the columns 'Country' and \n",
    "#'AAM due to War (2002)' using the argument names. \n",
    "#The values passed to skiprows and names all need to \n",
    "#be of type list.\n",
    "\n",
    "#Parse the second sheet by index. In doing so, parse only \n",
    "#the first column with the usecols parameter, skip the \n",
    "#first row and rename the column 'Country'. The \n",
    "#argument passed to usecols also needs to be of type \n",
    "#list.\n",
    "\n",
    "# Parse the first sheet and rename the columns: df1\n",
    "df1 = xls.parse(0, skiprows=[0,0], names=['Country','AAM due to War (2002)'])\n",
    "\n",
    "# Print the head of the DataFrame df1\n",
    "print(df1.head())\n",
    "\n",
    "# Parse the first column of the second sheet and rename the column: df2\n",
    "df2 = xls.parse('2002', usecols=[0], skiprows=[0], names=['Country'])\n",
    "\n",
    "# Print the head of the DataFrame df2\n",
    "print(df2.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6e0c5a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     YEAR     P           S\n",
      "0  1950.0  12.9  181.899994\n",
      "1  1951.0  11.9  245.000000\n",
      "2  1952.0  10.7  250.199997\n",
      "3  1953.0  11.3  265.899994\n",
      "4  1954.0  11.2  248.500000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Importing SAS files\n",
    "\n",
    "#In this exercise, you'll figure out how to import a SAS file as \n",
    "#a DataFrame using SAS7BDAT and pandas. The file \n",
    "#'sales.sas7bdat' is already in your working directory \n",
    "#and both pandas and matplotlib.pyplot have already \n",
    "#been imported as follows:\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#The data are adapted from the website of the \n",
    "#undergraduate text book Principles of Econometrics by \n",
    "#Hill, Griffiths and Lim.\n",
    "\n",
    "#Instructions\n",
    "\n",
    "#Import the module SAS7BDAT from the library sas7bdat.\n",
    "\n",
    "#In the context of the file 'sales.sas7bdat', load its \n",
    "#contents to a DataFrame df_sas, using the method \n",
    "#to_data_frame() on the object file.\n",
    "\n",
    "#Print the head of the DataFrame df_sas.\n",
    "\n",
    "#Execute your entire script to produce a histogram plot!\n",
    "\n",
    "# Import sas7bdat package\n",
    "from sas7bdat import SAS7BDAT\n",
    "\n",
    "# Save file to a DataFrame: df_sas\n",
    "with SAS7BDAT('sales.sas7bdat') as file:\n",
    "    df_sas=file.to_data_frame()\n",
    "\n",
    "# Print head of DataFrame\n",
    "print(df_sas.head())\n",
    "\n",
    "# Plot histogram of DataFrame features (pandas and pyplot already imported)\n",
    "pd.DataFrame.hist(df_sas[['P']])\n",
    "plt.ylabel('count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbdfcfe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  wbcode               country  disa1  disa2  disa3  disa4  disa5  disa6  \\\n",
      "0    AFG           Afghanistan   0.00   0.00   0.76   0.73    0.0   0.00   \n",
      "1    AGO                Angola   0.32   0.02   0.56   0.00    0.0   0.00   \n",
      "2    ALB               Albania   0.00   0.00   0.02   0.00    0.0   0.00   \n",
      "3    ARE  United Arab Emirates   0.00   0.00   0.00   0.00    0.0   0.00   \n",
      "4    ARG             Argentina   0.00   0.24   0.24   0.00    0.0   0.23   \n",
      "\n",
      "   disa7  disa8  ...  disa16  disa17  disa18  disa19  disa20  disa21  disa22  \\\n",
      "0   0.00    0.0  ...     0.0     0.0     0.0    0.00    0.00     0.0    0.00   \n",
      "1   0.56    0.0  ...     0.0     0.4     0.0    0.61    0.00     0.0    0.99   \n",
      "2   0.00    0.0  ...     0.0     0.0     0.0    0.00    0.00     0.0    0.00   \n",
      "3   0.00    0.0  ...     0.0     0.0     0.0    0.00    0.00     0.0    0.00   \n",
      "4   0.00    0.0  ...     0.0     0.0     0.0    0.00    0.05     0.0    0.00   \n",
      "\n",
      "   disa23  disa24  disa25  \n",
      "0    0.02    0.00    0.00  \n",
      "1    0.98    0.61    0.00  \n",
      "2    0.00    0.00    0.16  \n",
      "3    0.00    0.00    0.00  \n",
      "4    0.01    0.00    0.11  \n",
      "\n",
      "[5 rows x 27 columns]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Importing Stata files\n",
    "\n",
    "#Here, you'll gain expertise in importing Stata files as \n",
    "#DataFrames using the pd.read_stata() function from \n",
    "#pandas. The last exercise's file, 'disarea.dta', is still in \n",
    "#your working directory.\n",
    "\n",
    "#Instructions\n",
    "\n",
    "#Use pd.read_stata() to load the file 'disarea.dta' \n",
    "#into the DataFrame df.\n",
    "\n",
    "#Print the head of the DataFrame df.\n",
    "\n",
    "#Visualize your results by plotting a histogram of the\n",
    " #column disa10. Weâ€™ve already provided this code for \n",
    "#you, so just run it!\n",
    "\n",
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Load Stata file into a pandas DataFrame: df\n",
    "df=pd.read_stata('disarea.dta')\n",
    "\n",
    "# Print the head of the DataFrame df\n",
    "print(df.head())\n",
    "\n",
    "# Plot histogram of one column of the DataFrame\n",
    "pd.DataFrame.hist(df[['disa10']])\n",
    "plt.xlabel('Extent of disease')\n",
    "plt.ylabel('Number of countries')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "906a57be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'h5py._hl.files.File'>\n",
      "meta\n",
      "quality\n",
      "strain\n"
     ]
    }
   ],
   "source": [
    "#Using h5py to import HDF5 files\n",
    "\n",
    "#The file 'LIGO_data.hdf5' is already in your working \n",
    "#directory. In this exercise, you'll import it using the h5py \n",
    "#library. You'll also print out its datatype to confirm you \n",
    "#have imported it correctly. You'll then study the structure \n",
    "#of the file in order to see precisely what HDF groups it \n",
    "#contains.\n",
    "\n",
    "#You can find the LIGO data plus loads of documentation \n",
    "#and tutorials here. There is also a great tutorial on Signal \n",
    "#Processing with the data here.\n",
    "\n",
    "#Instructions\n",
    "\n",
    "#Import the package h5py.\n",
    "\n",
    "#Assign the name of the file to the variable file.\n",
    "\n",
    "#Load the file as read only into the variable data.\n",
    "\n",
    "#Print the datatype of data.\n",
    "\n",
    "#Print the names of the groups in the HDF5 file \n",
    "#'LIGO_data.hdf5'.\n",
    "\n",
    "\n",
    "# Import packages\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "# Assign filename: file\n",
    "file='LIGO_data.hdf5'\n",
    "\n",
    "# Load file: data\n",
    "data = h5py.File(file, 'r')\n",
    "\n",
    "# Print the datatype of the loaded file\n",
    "print(type(data))\n",
    "\n",
    "# Print the keys of the file\n",
    "for key in data.keys():\n",
    "    print(key)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee84a15d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Group' object has no attribute 'value'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\LUISHE~1\\AppData\\Local\\Temp/ipykernel_12468/2333360938.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;31m# Get the HDF5 group: group\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[0mgroup\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'strain'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;31m# Check out keys of group\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Group' object has no attribute 'value'"
     ]
    }
   ],
   "source": [
    "#Extracting data from your HDF5 file\n",
    "\n",
    "#In this exercise, you'll extract some of the LIGO \n",
    "#experiment's actual data from the HDF5 file and you'll \n",
    "#visualize it.\n",
    "\n",
    "#To do so, you'll need to first explore the HDF5 group \n",
    "#'strain'.\n",
    "\n",
    "#Instructions\n",
    "\n",
    "#Assign the HDF5 group data['strain'] to group.\n",
    "\n",
    "#In the for loop, print out the keys of the HDF5 group in \n",
    "#group.\n",
    "\n",
    "#Assign to the variable strain the values of the time \n",
    "#series data data['strain']['Strain'] using the \n",
    "#attribute .value.\n",
    "\n",
    "#Set num_samples equal to 10000, the number of time \n",
    "#points we wish to sample.\n",
    "\n",
    "#Execute the rest of the code to produce a plot of the time \n",
    "#series data in LIGO_data.hdf5.\n",
    "\n",
    "# Get the HDF5 group: group\n",
    "group=data['strain'].value\n",
    "\n",
    "# Check out keys of group\n",
    "for key in group.keys():\n",
    "    print(key)\n",
    "\n",
    "# Set variable equal to time series data: strain\n",
    "strain=data['strain']['Strain']\n",
    "\n",
    "# Set number of time points to sample: num_samples\n",
    "num_samples=10000\n",
    "\n",
    "# Set time vector\n",
    "time = np.arange(0, 1, 1/num_samples)\n",
    "\n",
    "# Plot data\n",
    "plt.plot(time, strain[:num_samples])\n",
    "plt.xlabel('GPS Time (s)')\n",
    "plt.ylabel('strain')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3406572f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "#Loading .mat files\n",
    "\n",
    "#In this exercise, you'll figure out how to load a MATLAB file \n",
    "#using scipy.io.loadmat() and you'll discover what \n",
    "#Python datatype it yields.\n",
    "\n",
    "#The file 'albeck_gene_expression.mat' is in your working \n",
    "#directory. This file contains gene expression data from the \n",
    "#Albeck Lab at UC Davis. You can find the data and some \n",
    "#great documentation here.\n",
    "\n",
    "#Instructions\n",
    "\n",
    "#Import the package scipy.io.\n",
    "\n",
    "#Load the file 'albeck_gene_expression.mat' into the \n",
    "#variable mat; do so using the function \n",
    "#scipy.io.loadmat().\n",
    "\n",
    "#Use the function type() to print the datatype of mat to \n",
    "#the IPython shell.\n",
    "\n",
    "# Import package\n",
    "import scipy.io\n",
    "\n",
    "# Load MATLAB file: mat\n",
    "mat=scipy.io.loadmat('albeck_gene_expression.mat')\n",
    "\n",
    "# Print the datatype type of mat\n",
    "print(type(mat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60b261fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__header__', '__version__', '__globals__', 'rfpCyt', 'rfpNuc', 'cfpNuc', 'cfpCyt', 'yfpNuc', 'yfpCyt', 'CYratioCyt'])\n",
      "<class 'numpy.ndarray'>\n",
      "(200, 137)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEVCAYAAAD6u3K7AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAABYOElEQVR4nO2deZhcZZW431Nrb+n0ln1fICEkhCUQVpEdcUN0VBQFBdERHfd1dJyR3ww6MzrjgiIqgiOioICIKCD7FiAJkAWSkH1Pd7rT+1Lb9/vj3ltd3V3LvdVd3VX0eZ+nnu66t+5Xp6u7v3PPLsYYFEVRFGUwvrEWQFEURSlOVEEoiqIoaVEFoSiKoqRFFYSiKIqSFlUQiqIoSlpUQSiKoihpUQWhKIqipEUVhKIoipKWQLaTIlIGvA04C5gO9AAbgL8YYzYWXjxFURRlrJBMldQi8m9YyuFxYA3QCJQBRwPn2N9/wRizblQkVRRFUUaVbArircaYv2S8UGQyMNsYs7pQwimKoihjR0YFoSiKooxvssYgAETkaOBLwJzU1xtjzi2gXIqiKMoYk9OCEJFXgJuw4hBx57gxZk1hRVMURVHGEjcKYo0x5qRRkkdRFEUpEtwoiH/FymC6B+hzjhtjWgoqmaIoijKmuFEQO9IcNsaY+YURSVEURSkGNItJURRFSYubLKYg8I/Am+xDjwM/M8ZECyiXoiiKMsa4cTH9AggCt9mHPgTEjTHXFFg2RVEUZQxxleZqjFme61gx0NDQYObOnTvWYiiKopQMa9asOWyMmZTuXE4XExAXkQXGmG0AIjKflHqIYmLu3LmsXq2dPxRFUdwiIrsynXOjIL4EPCYi2wHBqqj+yAjJpiiKohQpORWEMeYRETkKWGQf2myM6ct2jaIoilL6ZFQQInKuMeZREbls0KmFIoIx5u4Cy6YoiqKMIdksiLOBR4G3pzlnAFUQiqIob2AyKghjzLfsrxpvUBRFGYfknEktIp8RkWqx+IWIrBWRC0dDOEVRFGXsyKkggI8aY9qBC4F6rEK57xRUKkVRFGXMcaMgxP56CfBrY8zGlGPKMHhp9xFe3tM61mIoiqKkxY2CWCMiD2EpiAdFZAKQKKxY44N/vW8jX7t7/ViLoSiKkhY3hXJXA8cD240x3SJShxbKjQi7Wrrp6I3RG41TFvSPtTiKoigDcGNBnIZVHNcqIlcA3wDaCivW6NPWHeUv6w7w+ObG0Xm/niit3VHiCcOrB9pH5T0VRVG84EZB/BToFpHlwBeAbcCvc10kIreISKOIbMjxupNFJCYi70k5FheRl+3HfS5kzJveaJz3/PRZTrj+Ia777VquuW01G/YVXv/taelOfj8a76coiuIVNwoiZqyWr+8EfmyMuRGY4OK6W4GLs71ARPzAd4GHBp3qMcYcbz/e4eK98qYs6GdaTTnXnbOQ269ZSV1liC/e9QqRWGHDLKkKYt1eVRCKohQfbhREh4h8DSu99S8i4sOaD5EVY8yTQK651Z8G/og183rM+NHlJ/CFCxdxxsIGbrhsGZsOdvCjR18v6HvuthXESXNq1YJQFKUocaMg3gf0YdVDHARmAv813DcWkRnAu7BcWIMpE5HVIrJKRC4d7nt54bxjpvDuE2fyk8e3sbu5O/cFebLnSDc1FUFOX1DPlkMd9ESKsoO6oijjmJwKwlYKfwTC9qHDwD0j8N7/C3zFGJPOlzPHGLMC+ADwvyKyINMiInKtrUxWNzU1jYBY8OlzFxJPGJ7YUjjDZndLD7PrKlg2YyIJgwaqFUUpOty02vgY8AfgZ/ahGcC9I/DeK4DfichO4D3ATxxrwRizz/66HWsG9gmZFjHG3GyMWWGMWTFpUtqhSJ6ZU1/BjJpyntnaPCLrpWNPSzezaitYNnMiAOv3thbsvRRFUfLBjYvpOuAMoB3AGPM6MHm4b2yMmWeMmWuMmYulgD5pjLlXRGpFJAwgIg32e7863Pfzgohw+oJ6ntveTDyRfSRrPsQThr1HuplVV8HU6jIaqsKs36cWhKIoxYUbBdFnjIk4T0QkgNXuOysicgfwHLBIRPaKyNUi8gkR+USOS48BVovIK8BjwHeMMaOqIADOWNhAW0+UV/eP/MZ9sL2XaNwwu64CEWHZjGrW72sd8fdRFEUZDm4qqZ8Qka8D5SJyAfBJ4M+5LjLGXO5WCGPMVSnfPwssc3ttoTh9QT0Az247nHQDjRRO8Ht2XQUAy2ZM5IktTVpRrShKUeHGgvgK0ASsBz4OPIBVTf2GZnJ1GQsnV/HMtpGPQ+w5MlBBNEwIkzDQ2Rcb8fdSFEXJl6wWhF3IttEYsxj4+eiIVDycsaCeO1fvJRJLEAq40aXu2NPSjU9gWk0ZAAGftXYsPvLxDkVRlHzJuusZY+LAZhGZPUryFBWnL2ygJxrnpd1HRnTd3S3dTK8pJ+i3Pv6g3+qeHo1rk1xFUYoHNzGIWmCjiLwAdDkHC90Coxg4dX49PoGnXj/Myvn1I7bu7pbupHsJSCoKVRCKohQTbhTENwsuRZEysTzIKfPq+NvGg3zxokUjtu6elm7OP2ZK8nm/glAXk6IoxYObSuongM3ARKAaq/X3E4UWrFi4ZNk0tjZ28vqhjhFZrycS53BnhFkpFkRAXUyKohQhbiqprwFeAC7DqnheJSIfLbRgxcJFx05FBP664eCIrNfeGwWgtiKUPBZSF5OiKEWIm9ScLwEnGGOuMsZcCZyElfo6LphSXcZJs2t5YP2BEVnPacpXFuz/6B0LIlaAqm1FUZR8caMgmoFU/0qHfWzc8JZl09h0sIMdh7tyvzgHvTFLQZSnFMQlYxAFnkGhKIriBTcKYivwvIj8q4h8C1gFbBGRz4vI5wsrXnFw8dKpAPx1w/CtiH4LIlVB2DEItSAURSki3CiIbVjdW53d60/ADqypcm4my5U8M2rKOX5WDXe+uIcOO4aQL71Ry0oIp7iY1IJQFKUYcZPm+l1jTG/qARFpMMYcLpBMRcmXLlrElbe8wHW/fYlfXrkiual7JZ2LKVlJnVAFoShK8eBml3tBRE51nojIu4FnCydScXLGwgb+/V1LeXJLE//ypw15r9ObxsUUClgupojWQSiKUkS4sSA+CNwiIo8D04F64NxCClWsvO/k2Ww51Mkvn97BP569kNn1FbkvGkRWC0LTXBVFKSLcFMqtB/4d+ARwDvApY8zeQgtWrJwyrw7or2fwSk/EUgIDgtQBrYNQFKX4yGlBiMgvgQXAccDRwP0i8iNjzI2FFq4Ycbq6RvLczHuj6dJc1cWkKErx4SYGsR44xxizwxjzILASOLGwYhUvYTs4Hckz46jHVhADspjUxaQoShHixsX0v8BsETnfPhQBPltAmYqapAWRp4Loi8YRgXDKfAl1MSmKUoy46cX0MeAPwM/sQzOx6iLGJeGA5Rrqy1NB9MYSlAX8iEjyWMDnNOtTF5OiKMWDGxfTdcAZQDuAMeZ1YHIhhSpmhmtB9ETiA/owgc6DUBSlOHGjIPqMMRHniYgE6K+qzoqI3CIijSKStXBARE4WkZiIvCfl2JUi8rr9uNLN+40G/UHqeF7X90bjAwLUAH6f4BMdOaooSnHhRkE8ISJfB8pF5ALgLuDPLte/Fbg42wvsudffBR5KOVYHfAsrIH4K8C0RqXX5ngVl2BZEND4gxdUh6PepBaEoSlHhRkF8FWjCymb6OPAA8A03ixtjngRacrzs08AfgcaUYxcBDxtjWowxR4CHyaFoRgsnuJx3DCKaIJxRQagFoShK8ZCzDsIYkwB+bj9GFBGZAbwLqwDv5JRTM4A9Kc/32sfGnGFnMcXilAeH6uWgX9SCUBSlqMiv49zI8b/AV2wllBcicq2IrBaR1U1NTSMnWQac6W/5WhBWkHqoBRHw+7RZn6IoRYWbXkyFZAXwOzvlswG4RERiwD7gzSmvmwk8nm4BY8zNwM0AK1asKLiPJjTMQrneWJyJ5cG060Zi6mJSFKV4yGhBiMj/2V8/U6g3N8bMM8bMNcbMxaq1+KQx5l7gQeBCEam1g9MX2sfGHJ9PCPqlABaEqAWhKEpRkc2COElEpgMfFZFfA5J60hiTK/iMiNyBZQk0iMherMykoH39TZmuM8a0iMj1wIv2oW+7eb/RIhzw529BRBOaxaQoSkmQTUHcBDwCzAfWMFBBGPt4Vowxl7sVxBhz1aDntwC3uL1+NAkFfMOqgxhcKAdWNbVmMSmKUkxkdDEZY35ojDkGuMUYM992BzmPnMrhjYwVL8i/m2s6CyIUUAtCUZTiwk2a6z+KyHLgLPvQk8aYdYUVq7gJBfJTEMYYemOJIZXUYFkQWkmtKEox4aZZ3z8Bt2P1X5oM3C4iny60YMVMOODLK0gdjRviCZPWxRT0+/KeMaEoilII3KS5XgOsNMZ0AYjId4HngB8VUrBiJl8Lwhk3milI3R2JDVu2vlickN83oFusoihKPrhREAKkRmTjDMpoGm9YQeo8FEQkm4LIP0gdiSW4/v5XeXFnC1sOdfCRM+bxzbctyWstRVEUBzcK4lfA8yJyj/38UuCXBZOoBAj583Mx9UaHzqN2CAwjzXVbUyf/t2oXx8+qobYixI7DXXmtoyiKkoqbiXLfBz6C1XSvBfiIPWVu3BLKMwbRk2YedXLNYSgIZ871Z84/igWTq+jsG76rSlEUxVWrDWPMWmBtgWUpGaxCuUjuFw7C2cjT1kH4hVgiPxdT0jIJ+JkQDnCwvTevdRRFUVIZ62Z9JUk44CMS814o15vFggj6fUSH0d8JIBz0URkO0KUWhKIoI4AqiDzIN0jtuJjSz4MQonlaEH2OZRLwUxkO0NmXX5W3oihKKq4UhIjMEZHz7e/LRWRCYcUqbvKtpHZcQRktiLxjEE7w20dV2E9nXzSvdRRFUVJxUyj3MaxOqz+zD80E7i2gTEVPOJhvFlOWGITPl3cldV9KfUVlOEBvNEEsT2UTiyfoiagFoiiKOwviOuAMoB3AGPM6VkX1uCV/CyJLHURA8q6kTk2frQpbeQddeW7yNz62jbf96Km8rlUU5Y2FGwXRZ4xJpuyISACrm+u4Jd9K6mxprkGfL++7fkfxhAO+fgWRZ6B6z5FutjV1Ja0SRVHGL24UxBMi8nWgXEQuAO4C/lxYsYqbUMBHLGH1VfJCtkK5oN9HwuB5zcHrVg5TQTjtPhrb+/K6XlGUNw5uFMRXgCZgPfBx4AHgG4UUqtgJB6wN3qsVkXqnP5iA3+pekk+gujcWJ+gX/D5JWhAdeSsIS8bGDq2lUJTxTtZCORHxAxuNMYuBn4+OSMVPKNA/l7o8NNQayERvNE444MPnG9rKypl1HY2nnziXa90yW2kN34KwFMTBNrUgFGW8k9WCMMbEgc0iMnuU5CkJHAXR53GqXKZhQdBvQeSTydQXSyRrK4Ybg3AymA5pNbaijHvctNqoBTaKyAtAsgucMeYdBZOqyAn7+y0IL/RE42kD1GDFICBPF1PKGFNHQeRbLNdlxyBUQSiK4kZBfLPgUpQYSQvCcwwikbYGAlJcTHkEqfuiiWRcozJsKaDhWhDaz0lRFDcjR58YDUFKiXAgfwsil4spn35Mqa6ryqQFMbwYhFoQiqK4qaTuEJF2+9ErInERaXdx3S0i0igiGzKcf6eIrBORl0VktYicmXIubh9/WUTu8/YjFZ5QngoiWwzCcTHFEvllMTnrhgM+Aj4ZhoJwXEzDC1IbYzj/+0/w+xd3D2sdRVHGDjfzICYYY6qNMdVAOfBu4Ccu1r4VuDjL+UeA5caY44GPAr9IOddjjDnefhRdrCOpIDzGC/qiiSwxCMuCiMTyq4NwXFciQlVZfh1do/FEcqrdofZejMm/HrKjL8bWxk4e29SU9xqKoowtnrq5Got7gYtcvPZJrAFDmc53mv4dqJISqs4ODSNInSkGMRwLoi/Wn+YKUBkK5GVBOO6lKdVhuiPxvGspAI50WcX3G/a35b2GoihjS84YhIhclvLUB6wARsRBLSLvAm7A6u301pRTZSKyGogB37GVUqY1rgWuBZg9e3SycZ2UUq/tKLKnuQ4niylBOEXxVIUDdPZ639ydAPW8hkoOtfdxqK2X6rKg53UAWmwFsfdID63dEWoqQnmtoyjK2OHGgnh7yuMioAN450i8uTHmHrsI71Lg+pRTc4wxK4APAP8rIguyrHGzMWaFMWbFpEmTRkKsnAzHgiiMi2mQBRH2J9NVveBcM39SFTC8OMSR7v6Jexv35wxZKYpShLjJYvpIoYUwxjwpIvNFpMEYc9gYs88+vl1EHgdOALYVWg63DCfNNd2wIBhmkHrQupXhAO3DsSDqK4Hhpbo2d/YriA372jhjYUPeaymKMja4yWL6TxGpFpGgiDwiIk0icsVw31hEFoqI2N+fCISBZhGpFZGwfbwBq9X4q8N9v5Ek3zTXvgIVyvUNim1MyDNI7cQg5jZYCmI4qa6OBVFbEWSDWhCKUpK4KZS70BjzZTtesBO4DHgS+E22i0TkDuDNQIOI7AW+BQQBjDE3YWVDfVhEokAP8D5jjBGRY4CfiUgCS4F9xxhTVAoiXwsiW5A64HOa9eXhYooNjG1UhvJVENY1dZUhJpYHh6UgWrqihPw+VsytY6MGqhWlJHGjIJzXvBW4yxjTZt/4Z8UYc3mO898Fvpvm+LPAMhdyjRn5WBDReIJYwmQMUjtKx6sFEU8YonEzKAaRX5DasSAqQn6mVIc52DYMC6IrQm1lkGUzJvL31w7R2RdLtgFRFKU0cBOkvl9ENgEnAY+IyCRGKIupVMmnDqI3y7Ag6LcgvDbrczKpBmcxdUVinusYBiqIMg515B+kbu6KUFsRYumMaoyB1w6om0lRSg03hXJfBU4HVhhjolgN+0Yki6lUySeLqX+oT/Y6CK/Fd8l1U2ZMVIYDJEz/BDu39NgupvKQn6nVZRwajgXRHaG+KsSx0ycCVqBaUZTSwq3NPx04X0TKUo79ugDylAQBvw+feKuDyDaPGlKymDxaEOnWrSrr78dUEXLv1nEsiMpQgCnVZTR19hFPGPxp5lfk4khXhCXTq5k8IUxDVZgN+9SCUJRSw02h3Lewgs1LsKbJvQV4mnGsIMCaKufNgsilIPKbKJdWQSQ7usZhgvu1uiL9brApE8uIJwzNnX1Mri7LceVQWroj1FWGEBGWTK9m00FVEIpSariJQbwHOA84aNdELAcmFlSqEiAU8OXpYhrZSup0rqtK22rwGqjuicQoC1oT76ZMCAP51ULE4gnaeqLU2tXTE8uDebcfVxRl7HCjIHqMMQkgJiLVQCMwq7BiFT+hgM9TvKAnR5C6f+RonkHqQKoFkV/L7+5IPKlcnNYYHXlkQ7X2RDHGSpcF62fLJ31XUZSxxY2CWC0iNVgzqdcAa4HnCilUKRDy+zzVQfS7gjLUQSRHjuZnQQzIYirLb+xoTySenLEdTtZ6eJ9M5zTqSyqIgHiuGUlla2MHX/7DK54LExVFGR5uspg+aYxptYvbLgCuHI32G8VOOOBNQfTkiEH0F8p5VBCxoes6Q4O89mPqisSocBSErXD6ot435ZbBCsLvy6tC3OHBjYe4c/VeVu/K2BxYUZQC4KbVhojIFSLyL8aYnUCriJxSeNGKG+8xiOwKQkQI+sXzyNE+Z90RcjGV2y4mx2XVm48FkWyz4VgQ3j6rwew90gPAs1ub815DURTvuHEx/QQ4DXAqozuAGwsmUYkQzltBZP7Ig36f55GjaYPU4XyD1HEqbQuibBgWRPMgCyI4TAtiX6ulIJ7ZdjjvNRRF8Y4bBbHSGHMddvW0MeYIMO6b+3u9K3bcUZksCLDcTDGPFoSjeFK7uVYEnTRX7xZE0sUUcGZeeN/YnRhEbaU1SyIU8BFLGBIefzaHvUe6AVi3t42O3mheayiK4h03CiIqIn7siW92q41xHy0MBXyeAriOMnEK4jKt6XmMqaN4UiqpfT6xhgb1eXMPdUdiKS6m/IPULV1RqsKBpJLJt0ocrNnW+1t7WDqjmnjC8Px2jUMoymjhRkH8ELgHmCwi/45VJPcfBZWqBAgH/J42PGcjDwcyf+QBny+PLKb0sY3KsD8/CyI4MIupNw8X05HuSNJ6SF0rHwXR3BWhN5rgHcunEw741M2kKKOIm4FBt4vIGqxiOQEuNca8VnDJipyQ35uLyfHBh7JYEMGAeK4XyFSAVxn2Ppe6JxKnwq7CDvh9BHySpwURoS5lxGhy1kUe7ionQD2voYqT59ZpoFpRRhE3FgTAIeAp4Fmg3B7wM67xGoOIxBIEfIIvS1+joM97MLc3FifolyH9kqo8KghjDN3R/hgE2Km8eaa51lb2K4h8ut867LMVxIyack5fWM/mQx00DaPLrKIo7nHTi+l64CqskZ/O7a0Bzi2cWMVPPgoilMW9BPll+wyeR+3gdWhQXyxBPGEGNPcrC/rzSnNt6Ypw1JSq5PN+C8J7kHpfqxWgnlFbzhkLGoDNrNrezNuXT/e8lqIo3nDT6vO9wAJjTCTnK8cRIY+FcpF4bgUR8Ese3VwTA6qoHarKAuxp6Xa9Tk9kaCuQfC2II90DXUz9FoR3ZbP3SA8TygJMLA8mR6E2qgWhKKOCGxfTBqCmwHKUHF7rICKxRNb4A1h32t6zmOID+jA5OEOD3NJtB7srwykKIuj3nObaG43THYkPdDHZbUQi+VgQR3qYUVNuyTOMzCpFUbzjxoK4AXhJRDYAyVs3Y8w7CiZVCRAK+OjzsJm7czF5tyD6oom0xXeVYb+nQrn+YUH9fxJhj6m8MLTNBgwzBtHaw8zaiqQ8kF/xnqIo3nGjIG7Dmh29Hq1/SBK2s5iMMbiZ0d3nwsUU9JgZBXYMIk3xXWU4YM2DcInz2orgQAvCa5prOgURzLOVuTGGvUd6OHV+PWC1I/Hq2lMUJX/cuJi6jTE/NMY8Zox5wnm4WVxEbhGRRtv6SHf+nSKyTkReFpHVInJmyrkrReR1+3Gly59n1PB6V+zGxRTw+zz3YuqNpVcQZXadhtvq5dR51A75WBBOH6YBFkQeI1oB2ntidPbFki4msBSzupgUZXRwY0E8JSI3APcx0MW01sW1twI/JvP0uUeA+4wxRkSOA+4EFotIHfAtYAVWxtQaEbnPbvNRFDh+/0gskTYGMBg3LqaQX/LqxZSu+C5VgZX5csvXE7VcTBXhgS4mr7UUbT1WK4zqsv5CuWCeLqa9dgbTzNoUBRFUC0JRRgs3CuIE++upKcdcpbkaY54UkblZznemPK2kP432IuBhY0wLgIg8DFwM3OFC3lEhuQG73KzcBqljCe8uponlwSHH+wO6iaz9nxzSWRBlQT+HO70lryXnWqcEu/O1IJwiuRmpCiLg1xiEoowSbiqpzymkACLyLqxA+GTgrfbhGcCelJfttY8VDV5dTFFXaa7eJ69ZCmDoumGPCqy7L0Oaq0d3TrdtcaTWUzg/t9cYRGqRXKpM+QS7FUXxTsYdy54Bke38gtSYQb4YY+4xxiwGLgWu93q9iFxrxy9WNzU1DVcc1zh3xW7vZt3UQQT9MmKFcv3dWN1t8N0RZ2NPVRDe79addNnUdfK1IPa19lAW9A3JiHJmYCiKUliyWRD1WOmta7BGjTYBZcBC4GzgMPDVkRLEdkfNF5EGYB/w5pTTM4HHM1x3M3AzwIoVK0Zt8HEhgtR5tdqIJga0+h4in1sLIlkHkRKDyMPf390XxycDmxIG87Qg9h7pZmZtxYAssXxqMxRFyY+MO5Yx5gfAiVh+/0lYzfpOxNq8P2SMebcx5vXhvLmILBT7v9/u7xQGmoEHgQtFpFZEaoEL7WNFg1cXjqs6iEA+dRDxrC4mt5tpTySODNrYywJ+z3fr3ZE4laHAgE09XwuiuTPCpKrwgGP5uL0URcmPrDEIY0wceNh+eEZE7sCyBBpEZC9WZlLQXvsm4N3Ah0UkCvQA7zPGGKDF7gH1or3Ut52AdbEQ8rgB97lQEAGfd/96b4ZKaq8WRFef1ep74N16HhZEJEZ5aKA8SXec1zTX3ijzG6oGHMsnsyqVz/3+ZebUV/DZ84/Oew1FGS+4yWLKG2PM5TnOfxerCC/duVuAWwoh10jgOYspnj4ddfCaXiyIeMIQjZu0FoRXBdYTjQ2ooob+gHAiYbJ2oU2lKxIf4KZKlcVrAL69J0Z1+WCZ/DR7zKxyiMUTPLD+QLLwTlGU7Lht960MwmtfIFeFcj5vQWrnvdOlsabWabihOxIfkJqauq6XO/+eSGxAgBq8K1OH9t7ogHoKcKya/FxM25q66Isl6NEgt6K4QhVEnnjdgN22+44lDJaXLTfJYUFZC+XcbYZdffEBKa6QX3O8rr74EAXh9wk+8RakjsYTdEfiVA+q8QgPo9XGhn1tQP8UPkVRspNTQYjIFBH5pYj81X6+RESuLrxoxY3nLCaXaa7g3hWTadwoeG9s1xMdeuffnyrrfkO2hg4N9Vx6nbfdnqzIHupiyltB7LcUhFPMpyhKdtxYELdiZRA5E1q2AJ8tkDwlg5fMnHjCEE+YZNO6TDjn3VZTOwoi3TwIrwqsOzJ0Y8+ne2p331BFA94bEbbbnWjTWhB5WgAb97UD/bMvFEXJjhsF0WCMuRO7k6sxJgaM+/8wL0Hg5DxqF5XU4H7yWr+LaQQsiMhQ15BjmXiZKpdO0Tjy5GNBDG4jkm8vpkTCsNG2IDQGoSjucKMgukSkHrtPkoicCrQVVKoSwEvg1dnQcgWpk4N1XG6kvVmC1EkF5nKtrjTB5bwsiDTrgD1O1ZMFYbuYhlgQlovJbZzGYWdzF12ROLUVQbUgFMUlbhTE57E6uS4QkWewOrN+uqBSlQBeCuWc1+RKcw14dDE5G3c6F1PYb8cPXN4t90TiQ9Ncg96D1N2ROBXh9ArLmwVhu5gGZzHl2Rl2w37LvbRibh090bjrNuiKMp7JqSDstt5nA6cDHweONcasK7RgxU7IQ4ZPxKWLKejVxZQtzTXobSPt7IsxYVBAOOlicmlBxOIJ+mIJKoJDXUxBv7c2Iv0WRIa4iEc308Z9bYQCPo6bMTGv6xVlPOImi+k6oMoYs9EYswGoEpFPFl604ibk9xEO+JLB1Gw4FoTrLCbXFoStINJVUnsIosfiCXqjCSozBandNvxLM9c6VR5PQeo0cyWAZN8pr00EN+xvY/HUCUklqHEIRcmNGxfTx4wxrc4Te2jPxwomUYkgItRWhDjSlbuqN6kg/NnnMngdzdmbxcXk8wlBv7i6U3bGjQ7e2L2mufYkZ0qksSACPiIeKqnbe6P4fZI5LuLB7WWMYcO+do6dPjHZBsTpXqsoSmbcKAi/pDToERE/EMry+nFDbWUoOWIzG24tiIDdzsJtu41sdRDg/q69094sB7uYvG7GXX1DW4Yn1/L7iHjY1Nt7YlSXBYbM+87HxbS/rZe2nihLplcn4yz5FsvtONzF3zYcyOtaRSk13CiIvwG/F5HzROQ8rO6ufyusWKVBbUWQI93RnK9zqplzd3P1FjdwNsl0ldTgtMbOvRF22m6ywT2UvMYg0k2lcwgGxFMvpraeaIZJed5dTE0d1qTcadVlyWrxfIvlbnp8G5+/85W8rlWUUsNNs76vYAWn/9F+/jDwi4JJVELUVoZ4zc6OyYbbNNegz85iGm0Loi+9guhPc3U7dCiziynk9yUzk9zQ3hsdkuIK+WVWtdpWXm1lMKns8k113dbUSXckTjxh8LtsYKgopYqbkaMJ4Kf2Q0nBsiC8uJiybyj9rTbctsewK6kzWhDuisoc19CEwQoi6M2dk5xKlyZI7TmLqWdooz7Iz8XUliy6CwHW9/kGqbcf7kpeXxUuaDNkRRlz3GQxnSEiD4vIFhHZLiI7RGT7aAhX7NRVhGjtiRLPkVPvuFZyBakDHoPUbT1RqsKB5HWDcWtBdGW0IEbOxRQKeG+1MTjFNVUmLwrCSSSorQgmXUz5WBCt3RFa7LW6hzGTQlFKBTe3QL8EPoc1dlRzA1OoqQhhjHW3W1uZOW7vNkgdSioIdy6mTH765HouN+UOe7MbfEfsT2ZCeXMxDU6XBVtZjaQF4cECaE1p29Fhx1vysSC2NXUlv+/SamxlHOBGQbQZY/5acElKkNpKawM70h3JriBcBqkDfieLyaUF0Z1dQbhtjd2VQUFYa7jvnuq4mAZPlIN8LIj0MYgyj24vgNbuKBPKLEurP83V+wa/vakz+X2XWhDKOMCNgnhMRP4LuBvocw7aFdbjmtoKSynkikO4L5TzlsXU2hOlpmL4FkQmFxNYG7L7NNfMFoSXGERfLE5vNDGk1Tf0u+m8KYhI8nNyFEQ+aa5O/AG0ZbgyPnCjIFbaX1ekHDPAuSMvTmmRVBBd2VNdI26zmPze6iDaeqIcPaUq4/lwwJ/M4MlGR1+MkN+XVoGFA37XMYieSAwRMo5AdWtBOG6gtGmueWQxHemOJn9Xw4lBpFoQWminjAfcZDGdMxqClCJ1tlupJccm3FcoC6I7amfmpCfkwcVUleZuHbxNcOuKxKkI+ocUtzmyuI2tJNtspK2D8N5htjUlVhP0+wj6JdkWxAvbm7qYU1/BruZutSCUcYFOlBsGjtsi1126s+Hn6uZa4cH9YYyhrSeSMwbhzsU0dB61Q8jDgB6rk2t6RRO0g9Ru2nS3ZejDBPllMbV2R5IWBFh1I14tiFg8wc7mLpbazf40BqGMBwo2UU5EbhGRRhHZkOH8B0VknYisF5FnRWR5yrmd9vGXRWS1CxnHhKpwgKBfaBkhF5NTYObm7rQnGicaNzljEG420o7eWNq4AVibaa+HIHW6FFfoV45urIj+aXLpR5eC+1ngYFlaqZ9TeR4KYu+RHqJxwzJbQagFoYwHCjlR7lbg4izndwBnG2OWAdcDNw86f44x5nhjzIqhlxYHIkJNRSi3BRFLEPAJvhyVt6GA5f7odHF32mq3+KjJZUG4cFd1pWn1nbqGJwsig6IJehiGlKmTK3hPvY0nDO290QGfU0XI7znNdfthK/6wdLptQWgMQhkHFGyinDHmSaAly/ln7c6wAKuAmS5kKTrqKnI37IvEEjnnUTtUhAKuirAcBZHdxeR3tbl3RWJpM5jA6ec0fAvCS/vxTNPkkjJ5SL1t74lijFWz4lAWzENB2DUQS6ZX4xPo7lMLQnnjUywT5a4GUmstDPCQiKwRkWuzXSgi14rIahFZ3dTUNMJi5aamIpgziykaT+QMUDtUhvyuirCS7SNypbm6uGPv7M2sIMoCPtcpod1p5lo7BJMuJjcWRPppcg5W4NydTE6RXKqLqSLk3cW0ramL2oogdZUhKkMBdTEp4wI3WUxrReRsYBEgwGZjTO4Wpi4RkXOwFMSZKYfPNMbsE5HJwMMissm2SNLJdzO2e2rFihWjPkeytiLEtpT0x3REPCiIinDAVQplW49ltdRkyWJyMpCMMWkzixw6+2JD+jAl1wj6Xfv7u/viTJ4QTnvOqwUR8vvSpsuC4/ZyJ5Nj3aUGqcvzcTE1dTJ/kpVSXBH2a5qrMi4Y04lyInIcVmfYdxpjmp3jxph99tdG4B7glJF4v0LgZiZEXyyRM0DtUBnyJwvOspF0MWWzIPw+jIFYjl5RXX1ZXEye0lwzB7uTwWWXMYjq8qGzIJIyeXB7taX5nMqD3i2A3S3dzK2vBKxCQG21oYwHxmyinIjMxqrO/pAxZkvK8UoRmeB8D1wIpM2EKgacmRDZ0jcjsUTOFFeHipBbC8JFkNpFW4pEwtAViedQEO42w55IPG2bDfBqQcQyupe8ypTJgvBSSW2M4XBnH5Orw8nrtVmfMh5wU0ntFxEx9g7odqKciNwBvBloEJG9wLeAIIAx5ibgX4B64Cf2nWLMzliaAtxjHwsAvzXGFO2AorrKkJ0pE8sYMI7EPMQgwgH2t/bkfF1rT5Sgf+hIzlQGbMrpPT/JbJxMLqayoPtK6mzBbi/jVNt7okwYgR5TkD7bqzzo8xSDaO+NEY0b6u3CSMuC8KYg1uw6wvamTv5hxSxP1ynKWOJGQTgT5X5mP/84LibKGWMuz3H+GuCaNMe3A8uHXlGcONkxrd2Zi9a8xCAqXfq3W+1GfdliC6FkUVnmzbB/HvXwLIh4wtAbTWTOYvJQv9DWE03bh6lfJr/rGERrdwSRgRlRbq00h+ZOqwVZQ5WlZSvC/mTbb7f8/MntvLizRRWEUlK42bW+AjyKNVHuH4FHgC8XUqhSoi7Z0TVz3D7iIQZREQrQ6SIG0Z6j1Tf0F6dl25T7p8llKnDzE42bnDMvnKBvxiwmD21EMnVyTcrkoYFgq902PHX6mxerCKDZVgb1Vf0WhNcYxq6Wbtp6srsiFaXYcDtR7ibgJhGpA2YaYzRCZ1OTbNiX+Y7Sk4sp5NKC6IkMyO1Ph5u7dkdBZCqUK0tpjpepCA5SpsnlClK7iUH05I5BtHS5dzHVDgrkV4T8ROIJYvFExmFLqTgWRH1lOHm9lxiEMYbdzV3EEobuLPEeRSk23GQxPS4i1bZyWAP8XET+p/CilQZ1Llp+e09zjZPIccfemmMWBLgbz5ls9Z1hY3fbHM8pHMtVKOeu1UauORfus5iOdEeYOEiRJju6ugxUH+60frcNjgUR9pbF1NIVSb7eSS5QlFLAza410RjTDlwG/NoYsxI4r7BilQ5Odkw2n7QXF1NlyN3m1dYTzZrBBP137dk2084ssyDASinNtQakjhsdngXRG40TiSUyWjTgLYuprWeoBVHu8jN2aLYVhDMUqtylleewq6V7gDyKUiq42bUCIjINeC9wf4HlKTkmlAXwSX+2TDoi8USykjgXTjfUXFkybd3RrDUQkNr5NPNG2Nmb3cXUb4Vk30z7XUyZYhBWDCBXFlNju+XOyVRwB3YMwkOh3GBF6nUmxOHOPmoqgsk4SmXIisu4LSDc3awKQilN3Oxa38bq5rrNGPOiiMwHXi+sWKWDzyfUVoSyzoSIxBKEPVoQ2Xr9xOIJOvoyp9U6uLlrdxRRxlYbQacFefbN0HGhZGsbnksWgH12iu+MmvKMr/HiYrI6uQ5yMXm1ILr6kimukNp1150VsUsVhFKiuAlS3wXclfJ8O/DuQgpVatRUBLN2dPVaBwHZLQinHXYuF5ObGERnlnnUA9fIvpn2OPOogxlcTC6zmBwFMT2rgnDnYorFE3T0xoa0RPc6l/pwZ4T6qn6LxlGCXZE4NRW5r9/V0oXfJ8QTRhWEUlK4CVIfLSKPOHMdROQ4EflG4UUrHeoqQ9ljEJ6a9eWeCeEoo1xZTK7SXHtjBHySsdLb7YCe/nqK4VkQTpHg1IllGV+T2mMqG85mXJshSN3rUkE0d/YxKUVBOBZEj0sLYk9LN0dPmQD0tzJXlFLAza71c+BrQBTAGLMOeH8hhSo1rJkQI1QHYW+w2WZCJDu5joSLye7DlKngzklzzdWaojuaPUjttpJ6f2sPkyaEk66tdISDfozJnRHl1KYMtiAqPLuYIskaCEixIFy2/N7V3M2x06sRUReTUlq42bUqjDEvDDqmjWhSqCkPZv3H91YHYVsQWTafVhetvsHd3X9nXzyje2nAGjnTXLMHqb3EILK5lyyZ3Lm9kh1vM1gQblxM0XiC1u5osgbCut5dIgFYgfDGjj7m1ldQXZb970RRig03u9ZhEVlA/8Cg9wAHCipViTGxPJjRgkgkDLGEcV8HEXL821ksCBfT5CB1U86SxdQXza4gXDT8g/7NtjzDnX/AJ4i4syBm1GR2L4G72ApknrrnJUh9ZFAVNfRbEG6GBu22U1xn11dSXR5QBaGUFG5KOq/DmrewWET2YY0K/WBBpSoxaiqC9ETj9MXiyTtuByco6zVIna1SN9nJ1WUMInuhXDxj3GDgGrnTXMuD/oxjVUWEoN9HXxYFYYxhf2sv5yyanPW93MZFMrmYvKS5NiX7MA3NYnJjQSQVRF0FE3NYmopSbGRVEHbn1k8aY863W2/7jDEdoyNa6eDEAtp6okyeMHCzdTYx972Y+jNkMuHcGWdraAfuW21kK0pzm+ZqtZDIrGgAwn4f0VjmuEFrd5SeaDy3i8mxanJYAMlgfnn+aa5OkVy6LCY3Lqpdzdao0jmqIJQSJOuuZfdcOtP+vkuVQ3qcVg7pMlSczdntPIhwwIffJ1lz7Ft7IkwIB3L2EQr4BJ/kTnPNVbUMuS2Irr5Y1l5NYI0djcQzr+MmxRX6lW0uC2LvkR4qQ36qywfKVRZwv8E3dzl9mNLVQbhzMU0oC1BTEWRieVCzmJSSwo2L6SURuQ+rFqLLOWiMubtgUpUYjgWRLg7h+NyDLi0IEck5Va6tJ3cVtbNWrrnUXX2Zp8CBe3dOU2ffAD99OkI5LAg3RXLgPi6yq7mLOfWVQzK0fD6hLOhu1nY6C6IiWcyY28W0q7mb2XUViIhtQWh+h1I6uFEQZUAzcG7KMYM1DU6hPwiazn3gWBBuYxBgxSGyWRBtLhr1OYT8vpwupmzdRR0LItdm2tjexwJ7ZnMmggHJqqz2Jy2IXEFqJ7Mqu0w7m7tZMq067bmKUMBVDOJwZ4SgXwa484J+HyG/z1XDvt0t3RwzzaqBqLYtiFwzwhWlWHBTSf2R0RCklHGCoOksCK9BarDuULNZEK090SGB10xY85vTr2WMoSuHi8nnE0L+3BPcGjv6OG1BfdbXhPzZrZn9rT2UBX3UVQ4/+B6LJ9jT0s1blk5Ne7486HfnYurso74yPGRDr3Ax2Kk3GmdPSzeXLLNkmFgeJBJP0BtNZBzNqijFhJtK6pkico+INNqPP4rIzNEQrlSY6MaCcOliAqeddObNp7mzb0h1cCaybe490TgJk7kPk0M4kN0d0xuN2wH6zA32wLrzzmbN7G/tZXpNec67azdur32tPcQShrkNlWnPu51L3dwVoWHC0M+6MhTIWSj32oF2YgnDshk1QPa/E0UpRtzsWr8C7gOm248/28cUmwllQUT6C9hS6cvDxWQNpMl813+grTdnINfBmr6WfiPN1erbYUJZgI7ezAqrqcPuwFqdu34hWx3EvtaenPEH6I9BZFM2Ow5b4bK59RkURNBdy+7DtgUxmAoXLb/X72sDYNnMiYAqCKX0cLNrTTLG/MoYE7MftwKTCixXSeH3CRPCgaxZTJ5iEKHMFkRLV4S+WIJpWXoVpZItBuHcAU/IoSAm5mgl0tjRC2Rv0Q1uLIgepk90oSBcZFY5HVTnNqTvplce9LtOc00XfK9wMTRo/d426itDTLd/V14UxL0v7WNPyhwJRRkL3OxazSJyhYj47ccVWEHrrIjILbZLakOG8x8UkXUisl5EnhWR5SnnLhaRzSKyVUS+6v7HGTusfkxDG/Y5Pne3aa7QP1UuHQfarM14mouNFJwYRAYLotedBVGTIz3zUHKGQ3alFQpkVhB9MaslhRvLyI2LacfhLipD/gFN9lIpD/npyVHbYYzhcGcfDWnWqAz5czbrW7+vjWUzJyZdZm4VRGNHL5/9/ctcf/+rWV+nKIXGza71UaxhQQexWmy8B3ATuL4VuDjL+R3A2caYZcD1WNXaTnHejcBbgCXA5SKyxMX7jSmZiqD6YxDug5JWmmv6zcdtpo9D2O/L2GpjyyGrrCWXW6emIkhrT+ZutY3tltKaUp3bgsjkYjpoKz43P1f/GNRsFkT6FFeH8mDuDb4rEqcvlhhQA+GQK5GgJxLn9cZOls2YmDzmVkGs2t4CwN9fO5RM/VWUsSCngjDG7DLGvMMYM8kYM9kYc6kxZreL654EWrKcf9YYc8R+ugpwAt+nAFuNMduNMRHgd8A7c/4kY4y1iY5kmuvIWBChQOYYxFOvN1FfGWLx1AlZ18jWawrgUEcfAXtwUr6yuK2BAHd1EDubu5mXIUAN1gafy8XUbLfZqE9jQVSEsqciv3qgnXjC5KUgntvWnGwHcvuqXVlfqyiFJGeaq4j8CrtRXyrGmI+OoBxXA3+1v58B7Ek5txdYOYLvVRCqy4PsOzL0bs+pHPYWg/DTFYmlzZff39ZDyO9Le1ebjnDAx5HuoRtpImF4emszZx7VkLF/ksPEDMrPobG9j0kTwjnXCWWxILY3WUHlmbW5J/DkqqTOleIKUBby56yD2NNiz6ZIE3yvDPuzxiDW720F4LiZNcljE8rcKYjntzdz2oJ6Aj7hdy/u4Z/OOypr+3NFKRRudq37gb/Yj0eAaqBzpAQQkXOwFMRX8rz+WhFZLSKrm5qaRkosz2Rq+e1UDjszmd1QEQ5gTPpeQQdae5k6sSznZuyQye+/6WAHhzv7OHNhQ841aspDRGKJjGmhjR29OTOYkrJkUBCPbmpkVl05s+pyWxABv4+ATzIGqXOluAJUBHMriJf3WAauk4U04PpQIGsl9fp97TRUhQe43bIlMzgcau9l++EuTptfz5Wnz6WlK8Jf1mnzZGVscONi+mPK43aseMSKkXhzETkO+AXwTmOME/jeB8xKedlM+1gm+W42xqwwxqyYNGnskqscF9PgKWd9eRTKOXOp0/m4D7T1uM5ggv7pa4N5equlTM86Kvdnlq2VCFgWRK4MJrCUZLpWG519MZ7eepgLl0x1XWEcDvgyzqjY6WQwZUhxBStI3R2NZ027fWl3KwsnV6WtWq+0r8801W79vlaOSwlQO1TnaNi3arv1b3DagnpOX1DPwslV/PaFnB5dwLIKf/HUdvYe0ewnZWRwv2v1cxSQvR+zC0RkNla7jg8ZY7aknHoROEpE5olICGt63X3Dfb9CM7E8SDxhhrgdks36PASp+5vBDb1DdYrJ3JLJgnjq9cMcNbkq62hPh2SleIZAdWNHrysFkcmCeGJzE5FYgguXTMm5hkO27KydTg1EhhRXgONn1WAM/P3VQ2nPG2N4aU8rJ8yqSXvesfLSdbntjsTYOihA7ZCrYd9z25qpLgtwzLRqRIS3LJ3Ky3tas04YdHhiSxP/7y+vcefqvTlfqyhucFNJ3SEi7c5XrEK5nO4gEbkDeA5YJCJ7ReRqEfmEiHzCfsm/APXAT0TkZRFZDWCMiQGfAh4EXgPuNMZszOunG0WcltKDU13zC1KntyDiCcOh9l6PFsTQVhu90Tgv7GjhzKNyu5egv9dUOguiLxbnSHeUKS5cTEG/j2iaTf2hVw9SVxlixdw6V/KAYxmldxHlSnEFePOiycysLee253amPb+7pZuWrggnzK5Nez7TYCdjDLc+u5OEgePSuKZytfxetb2ZU+bV47ddiKfMqyOeMKzddSTjNQ43P7kd6FeQijJc3PRiyp7ikvm6y3Ocvwa4JsO5B4AH8nnfsaI6JUNlZsqeko+CyGRBHO7sI5YwTBumBbF65xH6YgnOcqkgnM6x6Ta2ZBW1Swti8MCgSCzBo5saecvSqclN0Q2ZXGeQO8UVrHjAFafO4Tt/3cTmgx0sGpTJtXa3tSGfMLsm7fUVqaNh7R6F0XiCf/nTRu54YTcXLpnCm44e6r6bWB5kW1P6EN6Bth52NndzxalzksdOnF2L3ye8uLMl7XoO6/e28dz2ZnzSX0WuKMMl464lIidme4ymkKWA44ZpG3SXHYnH8fvE0+aXtCAGuauSNRAeLIh0qaWPbmok6BdWzsveXM8hmZ6ZxoJoTLbZyK0gwnYWU6rfftX2Zjp6Y1y4JHPGUdq1Av60MQhjDOv3tQ3Z8NPxvhWzCAd8/DqNFfHS7lYqQ36OnpJ+ncpBFsSRrggf+uXz3PHCbq47ZwE3XXFS2hbv2SyIZ7f2xx+S7xMOsHR6Nc/vyJgxDsDPn9pOVTjApSfMYMfhroyxEUXxQjYL4ntZzhkGtv8e92TKcY/EEp4a9UHmsaNeayDAutOOJQyJhMHnEyKxBPe+vI/zj5mSs4LawRltmi4G4RTJ5aqiBsvFZAzEEiaZ1fXQqwepCPldu7scrB5TQ11Mmw91cLgzwuk5OssC1FaGeMfy6dy9dh9fvnjxgGD0S7tbWT6rJqNir7A/u9W7jtDaHeVrd69jf2sv//O+5bzrhMy9LCdWZFYQT2xpoqEqzDFTB7YoP2VeHbc9t4veaDxtuuuelm7+sv4AHz1jLjNqyrl77T6aOvtc/U4UJRvZdq6fGmPOAa42xpwz6KHKYRD9gdw0CsKDewlIDvDJaEG4rKKGlLGjtmvn768doqUrwvtOnpXtskHy+An4JO3G5sWCcGRxMocSCcNDGw9x9tGTPOf5V4T8tKSxaJ5+/TAAZ7hI3wW48vS59ETj3PzktuSxnkic1w60Z3QvQX/V+Dfv3cDlP19FZ1+MO649NatyAOtGoi9NynA8YXjy9SbOPnrSkBTmU+bVE4klWLe3bch6kViCz/3+ZYJ+4SNnzGOePZNjR5O6mZThk23ncnog/WE0BCl1MloQce8KIhkATWNBlAf9rocFQepwHWtTvuOF3UyfWOYqvdXBmYaWLkjd2N6H3ydpO54OxnG5ODGRV/a20tjRx4XHus9ecjhlbh3r97bS0jXQqnlm62HmT6p0nem1dMZELjtxBjc/uZ2tjVbrkQ3724glDCfMSh+gBlg8tZrHvvhmfvuxldx0xYk88JmzOGlO5tc7OLGqwZlM6/a20tod5exFQ38vJ8+11n1hx8AWaMYYvnnvBlbvOsJ/vWc502vKmW/XfuxsVgWhDJ9sO1eziDwEzBOR+wY/RkvAUqE86Cfk9w3ZRPuG4WIanCFzoK2HaTVlnqaROcqpL24Nr3l662H+YcUsTzERyFxNfai9l4aqkKv1BlszD248RMAnnLvIu4K4YMlUEsaKpzhEYgme39Hiqvgvla9fcgzlQT/fuHcDrd0Rbnt2JwDHZ7EgAOY1VHL6ggYuXjrNtTvHUe6DA8mPb27CJ3BWGtlrKqx2KM/vaMEYw4Z9bfzuhd188a51/H71Hj51zkLevnw6YM3zDvl9bPcQqE4kDL98ekfSXagoDtmc0G8FTgT+j+zxCAX7LjuNfzkaN54tiHDAh08YMhNif2uvq3bYg9cCy4K4a42VH/9eD+4lh5ryYMYgtdvNMTTIgnjo1YOcOr/e1XztwSydUc3U6jIefvUg7znJcuu8vKeV7kjctXvJoaEqzJcvXsw37t3AaTc8Sm8sztVnzkvbxXW4LJ1eTTjg4/Kfr+LipVP58kWLmdtQyRNbmlg+q4baDC1UTplXx52r93De955Ibv5lQR/vXTGTz19wdPJ1fp8wu77Ck4vppT2tXH//qzz86kF+e82prqv0lTc+GRWE3ShvlYicbowZux4WJYSVoTK4DiLu2YIQkbQzIQ609fAmD64h6FcQXZEYd764h7OOmuSqId5gaipCyZTWVA619zKz1n3jQLA6t/ZG42xv6uKq0+d6lgWsz+j8JZP545p9yeDt01sP4xM4db677KxUPnDKbB7f3EjCwJcuWsQxGWZZD5f5k6p48svncOuzO/nNql2s3vkcP73iJF7Z28pnzjsq43XnLp7Mr5/bRcOEMNe+aT6nL2hgZm152s18XkOlp1TX57ZZcZtV21u45ZkdXHPWfO8/mPKGxE0dhCoHl6Trx5RPkBrsmccpFkQ0nqCxo89TDQT0K4h71u7jYHsvN1y2zLMsYCm/120ffSpNHX0Zi8kGc/qCeiZNCPOp377EecdYxfgXeKieHswFS6bym1W7eWbrYc47ZgrPbD3McTNrPMVoHHw+4RdXnpy3LF6YUl3GVy5ezGUnzOD9N6/i/Tc/hzFW8V4m3rxoMpuuv9hVMH+ebZHEE8aV6+/Zbc0cM62ambXl/OeDm3nT0ZMypvcq44t8Wm0oGRgcyE0kDFsOdbpqZzGYwRbEnpZujPFWAwH9d+2/WbWLhZOrODtLsVU20gWp97f20NwVYcGkzD2PUplcXcavP3oK3ZEYtz+/m+UzJ3pK2R3MqfPrqAoHeHDjQe5eu5eX97R6jj+MJUdNmcDtH1tJVThAfWUobWuOVNxmes1rqCQSSySz3rLRG42zetcRzlhQzw2XLWNCOJBzUFFjRy+PvJa+RYnyxkIVxAgysWLgJrpm9xH2tfbwtuOmeV6rMhzgSErbjr9uOAi4T990cAYVdUXifOyseXn7l2sqgnT0xogn+guwnnrdfcM/h2OmVfOrj5zMhHAgGTvIl3DAz9mLJnHn6r18/s5XOGbaBD6wcvaw1hxtFk+t5s+fPpPbP7bSc+JAJpw5GG7cTGt3HSESS3D6wnoaqsJ8cOVsnt56mENZAtY/eWwbV9+2OjlqVnnjktHFJCI/Is0cCAdjzD8VRKISZnAjtj+9vI+yoI/zj/HuRjltQT2/fHoH+1ut7q33vLSPk+fWMqsu97yEVJzhOg1VId55/AzPcjhMTEnPdAKpT75+mCnVYY6eUuVprZPm1LH6m+d7js2k48OnzqGpo48rT5vLW5ZOLckAq5sZGF6Yn6IgsrXnAHhm22H8PuEUu6r+nSfM4IePbuXPr+zPGItwOs4+u7WZS0+w/qb+44HXWDi5iveu8J4AoRQv2f5DVwNrgDKsbKbX7cfxgLtpNeOMmvIQHX0xYvEE0XiCv6w7wAVLprquWE7lQ6fOwRjDb1btYuP+drY2dib/Gb3gTCb78GlzhzV0ZnAhYDxhePr1w5x11CRPabcO4YA/r+sGs3J+PXd+/DTeety0klQOhWDShDCVIb8rC+LZbc0snzmRKvtvdMGkKo6bOZF7X07fYb+1O8Kmg1Ys6im7KLGxvZefP7WdXz2zc2R+AKVoyJbFdBuAiPwjcKbdZRURuQl4anTEKy0mllsfZ3NXhFf3t3OkO8o77fx0r8yqq+CCJVO444XdtPdGCfqFty7z7qpaMq2a/3z3cck8+XwZ2K22kvX72mjribpu+KeMHiLC3IbKjE0BHTp6o6zb28Yn37xgwPF3Hj+D6+9/la2NnSycPNA6fHGn1cRwRk05T29twhjD3zYexBjYdLCd1u5IsjWLUvq4sfFrsabIOVTZx5RBLJs5ERF4z03PcuNjW5lYHsxp4mfjqtPncaQ7ym9W7eacRZPz+sfz+YT3njyL8tDwRlYO7uj61JYmRLzFH5TR46Q5tbywo2VINX4qL+xoIZ4wA5oDArx9+TR8YrlIB/P89mZCAR/Xvmk+h9r72NbUyV/WHaA86McYa03ljYMbBfEd4CURuVVEbgPWAv9RWLFKk5Pm1HHnx0/DL8LqXUe4ZNnUvFJcHU6dX8diuyvpu/JwL40kg1uJPPX6YZZOn0idy9nYyuhyybJp9MUSPLa5MeNr/vzKfiaEA5w4KE158oQyzljYwJ9e3j+kK+wLO1s4flYN5y62UnLvXruPF3a2cNUZcwkHfKzargrijYSbkaO/AlYC92BNgDvNcT8pQzl5bh0PfOYsvvX2JXzmvKNzX5AFEeHzFxzNibNrOGfxsIf4DYvUoUEdvVHW7j6i7qUi5uS5dTRUhXlgffp51i1dER5Yf5B3nzQzbWzqncfPYHdLN2t3tyaPdfRG2bCvjVPn1TGrroK59RX84qkdGGPdwJw0pzYZwE7H2t1H+N5Dm7nt2Z08uaVJW5KXAG4myglwPrDcGPMnICQipxRcshKmIhTgI2fMy6v+YTAXHjuVuz95xrACzCNBqgXx+OYmYgkzLPeZUlj8PuHipVN4dFNj2tG1d63eQySeyJgWfNGxUwgHfAPcTGt2HSFhSGY8nXlUA5F4gqMmV3H0lAmcOr+e1+w4RMxO0tja2IExhl89s4P33vQcP3p0K9+6byMfvuUFfmZPwEvFGKMztYsIN/6PnwCnAc6EuA7gxoJJpBQlAb+PqnCA1u4od7ywm5m15ZziYUSoMvpcsmwavdEEj20a2AwhkTD89oXdnDK3LmPF9ISyIOcvmcL96w4k27O/sKOFgE84cU4NAGcunJR8H4CV8+qScYj/fHAz1/12Led//0lO/ve/829/fpU3L5rMK/9yIS/+8/lcuGQK339oC1sODazOv/nJ7Zz53cfSDnFSRh83CmKlMeY6oBfAGHMETXMdl0wsD/LyniM8u62Zy0+ZrWmlRc7KefU0VIV4YMNAN9PTWw+zq7mbD56avajw0uNn0NIV4enXDxOJJXjktUaWzZyYHLf65kWTuPrMecl1ls+qIRzw8f2Ht3Dzk9t534pZ/L9Ll3Ly3Dq+fslibv7QSUysCDJpQpj/uGwZVWUBvnDnK0kF9Ny2Zr77t01Uhvxcf/+rvLKndeQ/FMUTbhREVET82EVzIjIJSD8MWHlDU1MRZO3uVgI+4R9WDK8KWik8fp9w0bFTeeS1Q/zk8a1sb+rkrtV7+Lc/b6SuMsTFS7OPeT376EnUVAS59+V9fPPeDWw+1MHVZ85Lni8L+vnm25Yku/mWBf2cOLuWTQc7OHluLddfupQrTp3DT684iWvftGDADUVDVZj/eNdS1u9r4wM/X8XPntjGp+94ibkNlTz4uTcxeUIZ1/12bdoOwvnSG43z9XvW8+QWbS/nFjcK4odYAerJIvLvwNO4yGISkVtEpFFENmQ4v1hEnhORPhH54qBzO0VkvYi8LCKrXciojAJOHOKCJVN0nGWJ8PE3LWDp9In85982c+73nuBLf1iHMXDDZcuSw6QyEQr4uGTZNO57ZX9y7sTbjsteT/O25dNYMKmSGz94Ys4MvouXTuPrlyzmSHeUG/66ia6+GDddcRIzayv48QdO4FB7L5+6Y23Swhgu3/3bJn77/G4+9uvVPJ8lmK70I24yCURkMXAeIMAjxpjXXFzzJqAT+LUxZmma85OBOcClwBFjzH+nnNsJrDDGHHb3Y1isWLHCrF6t+qRQfPL2NTyw/iC//ugpGqAuMXY3d/PopkMcO2MiK+bUuq5iX72zhffc9BwXHTuFn37wpIK5Ffe19hCPG2bX97cduXP1Hr78h3W8/+RZ3HDZsmFV3j+2uZGP/OpF/uGkmazdfYTG9j7uuPZUluZokDgeEJE1xpgV6c65yWL6JVBmjLnRGPNjY8xrIvKvua4zxjwJZEyKNsY0GmNeBEbOhlQKypJp1SydUV1SHVMVi9n1FVx1xjxOnlvnaaNdMbeOuz95Oj94/wkFjTnNqCkfoBwA3rtiFp86ZyG/e3EP33toS9KSaO+N8rMntrFm1xFXa2851MGX7nqFxVMncP2lS/nNNSupLg/ypT+sG/Gf442GmyZBFwErROR7xphf28feAfxrwaSy4h0PiYgBfmaMubmA76W45FPnHsV15ywckR5KSukwuJBuNPnChUezv62HHz+2lfte2c9bj5vG71/cQ0tXBJ/AZ88/muvOWZjshLt+bxv/+eAmAj5h6YyJbG3s5K8bDjKhLMAPLz+BsqCfaRPL+fBpc7jhr5s42NY7Iunob1TcKIhG4BzgNyKyEvgMlqupkJxpjNlnu6EeFpFNtkUyBBG5FrgWYPbs0mr1XIqoclBGExHhe/+wnLcfN53/fmgzP318G6fNr+cz5x/FHS/s5vsPb+G+V/Zz2vx6DIbfPr+busow9ZUhntjSRGU4wD+du5CPnDFvwDjXsxdN4oa/buLJLU15jeAdL7hREGKMaQPebruWHgcK6rgzxuyzvzaKyD3AKUBaBWFbFzeDFYMopFyKoow+IsI5iydz9tGTONTRy9TqMkSElfPqOGfRZO5as4d7XtpHZ1+MD6yczVcuXszE8iA9kTgi6QctLZoygSnVYZ54XRVENtwoiPucb4wx/yoia4DPFUogEakEfMaYDvv7C4FvF+r9FEUpDXw+GTCBUES49IQZXHrCDOIJQ2dvLNlUEsjaoFJEeNNRk3jo1UPE4gkCIzCb5I2Im15M3xr0/M/GmHNzXScidwDPAYtEZK+IXC0inxCRT9jnp4rIXuDzwDfs11QDU4CnReQV4AXgL8aYv3n/0RRFGS/4fTJAObjhTUdPoq0nyit72wYc39bUyWsH2l2v8+LOFl7d305fLJ77xSVGtolyTxtjzhSRDgZOlhPAGGOqM1wK1gsuz3H+IJCu2qodWJ7tWkVRlOFy5sIGfAJPbmnipDlWIP65bc1cfduLdEfiXHr8dL508WJm1GSem/7HNXv5wl2vABDwCVefNY+vveWYUZF/NMhoQRhjzrS/TjDGVKc8JuRSDoqiKMVObWWI5bNqeHxLEx29Uf624QBX/eoFZtSU8/Gz5/PXDQc573uP83+rdqXtPLu1sYNv3LuBU+bW8cPLT+DcxZP52RPbWbe3dchrb39+F1+86xVueOA17ntlf17y9kbj/N9zO7n85lWs3e0uxXe4ZCyUE5GsndiMMUXX+F0L5RRF8cL/PLyFHzzyevL50hnV/PqjK6mrDLGvtYev3W215jh38WS+eOEijpk2ARGhrTvKP/zsWZo7IzzwmbOYUl1GR2+Uc/77cebWV3LXJ05LZvz9+ZX9fPqOl6itCNLVFycST/Dv71rKB1fOcS3nqu3NfPqOl2jq6EuOEb7pQydx9ggUrGYrlMsWpF6D5VpKl9dogPQTzRVFUUqEK06dQ8IYJpQFmDyhjPOXTEnO555RU86tV53Mr5/byQ1/3cSjmxpZOLmKqnCA9fvaSBjDbR85hSnVVh3FhLIgX7xwEV+9ez33rzvA25dPZ3tTJ1/94zpOnF3D7z9+Gj4RPnrri3zrTxtZOKmKlfPrs4mX5MbHtuIX4bcfW8nCyVVcecuLXHPbi/zg/Scku+kWAletNkoFtSAURSkEzZ19PLDhIPe/sp9oPMHpCxq4YMkUls+qGfC6eMLw9h89zZ4j3SyfWcOuli46e2P85Z/OYrody2jrifKuG5+htSfKnz99ZtYYB0BnX4wTvv0QHzljHl+/5JjkGlff+iIv72nlJx88kQuPzd54MRvZLAi3vZhqgaOAZMlhpsK1sUQVhKIoY82WQx384O+vs6+1h66+GN96+7GcOWj64ramTt7542dYOLmKOz9+GqGAjz+u2ctz25v5f5cuHVC78bcNB/jEb9byu2tP5dQUi6OjN8oVv3yB1/a3c/OHT+LNi/KbOjksBSEi12BVT88EXgZOBZ5zk+o62qiCUBSlVHhg/QE+eftaPnrGPGbUlnP9/a8CcNkJM/jee5cnYxhfuusVHtx4kDXfvIDgoHqNtu4ol/98FQfaenjqK+cm3WNeyDcG4fAZ4GRglTHmHLuza85234qiKEpmLlk2jatOn8stz+wA4C1Lp3LU5Cp++OhWFk2dwMfPXkAiYXhscxNnL5o8RDkATKwI8ptrVrKzuSsv5ZALNyv2GmN6RQQRCRtjNonIohGXRFEUZZzxtUsWs6u5i2k15Xz7Hcfi9wnbDnfxnb9toqEqzMLJVRzu7OPcxZmzleoqQ9RVFmbIpxsFsVdEaoB7sRrnHQF2FUQaRVGUcUQ44OdXHzllwLH/fs9yWrsjfOGuV1g6oxqfwNlH5xdfGC45FYQx5l32t/8qIo9hNerT1heKoigFoDzk55arTuazv3uZv244yIo5tQWzEHLhymllZzHNAjrsx1JgbQHlUhRFGbeEA35+/IET+flT2zlhUCrtaJJTQYjI9cBVwHbAGQ5rgKLLYlIURXmj4PcJnzh7wZjK4MaCeC+wwBgTKbQwiqIoSvHgpgn6BqCmwHIoiqIoRYYbC+IG4CUR2QD0OQeNMe8omFSKoijKmONGQdwGfBdYT38MQlEURXmD40ZBdBtjflhwSRRFUZSiwo2CeEpEbsCaTZ3qYtI0V0VRlDcwbhTECfbXU1OOaZqroijKG5ysCkJE/MB9xpj/GSV5FEVRlCLBTbvvF4wxp2R9UZEgIk3k3yeqATg8guKMFqUqN5Su7KUqN5Su7KUqNxS/7HOMMWm7AbpREP8DBIHfA13O8TdaDEJEVmfqiV7MlKrcULqyl6rcULqyl6rcUNqyu4lBHG9//XbKMY1BKIqivMFx0831nNEQRFEURSkucrbaEJGJIvJ9EVltP74nIhNHQ7hR5uaxFiBPSlVuKF3ZS1VuKF3ZS1VuKGHZ3cQg/ojVj+k2+9CHgOXGmMsKLJuiKIoyhrhREC8bY47PdUxRFEV5Y+Gmm2uPiJzpPBGRM4Cewok0uojIxSKyWUS2ishXx1qebIjILBF5TEReFZGNIvIZ+3idiDwsIq/bX2vHWtZ0iIhfRF4Skfvt5/NE5Hn7s/+9iIzN2KwciEiNiPxBRDaJyGsiclopfOYi8jn772SDiNwhImXF+pmLyC0i0mg3BXWOpf2MxeKH9s+wTkROLDK5/8v+W1knIvfYI5udc1+z5d4sIheNidAecKMgPgHcKCI7RWQX8GP7WMljFwLeCLwFWAJcLiJLxlaqrMSALxhjlmBVtl9ny/tV4BFjzFHAI/bzYuQzwGspz78L/I8xZiFwBLh6TKTKzQ+AvxljFgPLsX6Gov7MRWQG8E/ACmPMUsAPvJ/i/cxvBS4edCzTZ/wW4Cj7cS3w01GSMR23MlTuh4GlxpjjgC3A1wDs/9X3A8fa1/zE3oOKF2OMqwdQDVS7fX0pPIDTgAdTnn8N+NpYy+VB/j8BFwCbgWn2sWnA5rGWLY2sM7H+yc8F7gcEq3gokO53USwPrBnsO7DdsSnHi/ozB2YAe4A6rGzF+4GLivkzB+YCG3J9xsDPgMvTva4Y5B507l3A7fb3A/YX4EHgtLH+3LM93IwcDQPvtj+EgIg4iuXbWS4rFZx/Ioe9wMoxksUTIjIXq0/W88AUY8wB+9RBYMpYyZWF/wW+DEywn9cDrcaYmP18L9bvo9iYBzQBvxKR5cAaLEuoqD9zY8w+EflvYDeWS/ghLNlL4TN3yPQZp/u/nQEcoPj4KFaRMVgyrko5V+yfvysX05+Ad2K5N7pSHsoYISJVwB+Bzxpj2lPPGevWJHvmwSgjIm8DGo0xa8ZaljwIACcCPzXGnID1tz/AnVSkn3kt1v/tPGA6UMlQV0jJUIyfcS5E5J+x9s3bx1qWfHFTST3TGFOyf1g52AfMSnk+0z5WtIhIEEs53G6Muds+fEhEphljDojINKBx7CRMyxnAO0TkEqAMy135A6BGRAL2HW2xfvZ7gb3GmOft53/AUhDF/pmfD+wwxjQBiMjdWL+HUvjMHTJ9xkX/fysiVwFvA86zlRuUgNyDcWNBPCsiywouydjwInCUndkRwgog3TfGMmVELP/eL4HXjDHfTzl1H3Cl/f2VWFZf0WCM+ZoxZqYxZi7WZ/yoMeaDwGPAe+yXFZ3cAMaYg8AeEVlkHzoPeJUi/8yxXEunikiF/XfjyF30n3kKmT7j+4AP29lMpwJtKa6oMUdELsZyp77DGNOdcuo+4P0iEhaReVhB9hfGQkbXuAjAvApEsAJB67BGj64b6+DJSD2AS7AyDbYB/zzW8uSQ9UwsM3sd8LL9uATLn/8I8Drwd6BurGXN8jO8Gbjf/n4+1j/IVuAuIDzW8mWQ+Xhgtf253wvUlsJnDvwbsAmr0PX/gHCxfubAHVgxhCiW1XZ1ps8YK8HhRvt/dj1WplYxyb0VK0bi/I/elPL6f7bl3gy8Zaw/91wPN4Vyc9IdN8bk21ZbURRFKQFyKghFURRlfOImBqEoiqKMQ1RBKIqiKGlRBaEoiqKkRRWEoiiKkhZVEIqiKEpaVEEo4xK7hfcnU55PF5E/FOi9LhWRf/F4zQOpbaJdvP5tIvJG6I+mFBGa5qqMS+xmh/cbqxV2od/rWayq2sMFfA8B1gJnmIHVu4qSN2pBKOOV7wALRORle8DLXGfoi4hcJSL32kNqdorIp0Tk82INO1olInX26xaIyN9EZI2IPCUiiwe/iYgcDfQ5ykFEbhWRn9rrbBeRN9tDZ14TkVtTrtspIg22XK+JyM/FGv7zkIiUD34fY93pPY7V/0dRRgRVEMp45avANmPM8caYL6U5vxS4DDgZ+Heg21jdXJ8DPmy/5mbg08aYk4AvAj9Js84ZWHf2qdRizWL4HFZ/nv/BGiKzTESOT7PGUcCNxphjgVas9vvpWA2cleGconjGTTdXRRmPPGaM6QA6RKQN+LN9fD1wnN1y/XTgLmdGClavo8FMw5onkcqfjTFGRNYDh4wx6wFEZCPW3JWXB71+hzHGObbGfk06GrFaeyvKiKAKQlHS05fyfSLleQLr/8aHNXzn+Bzr9GBNpUu3duq6qWtnkyUODHEx2ZTxBpoXr4w96mJSxisd9E+384yxBjXtEJF/ACtIbE+cG8xrwMJ838cjR2N1blWUEUEVhDIuMcY0A8+IyAYR+a88l/kgcLWIvAJsxJrgNpgngRMkxQ81UojIJ0TkEymHzgH+MtLvo4xfNM1VUQqMiPwAK+7w9wK+xxTgt8aY8wr1Hsr4Qy0IRSk8/wFUFPg9ZgNfKPB7KOMMtSAURVGUtKgFoSiKoqRFFYSiKIqSFlUQiqIoSlpUQSiKoihpUQWhKIqipOX/Axyw4K6fvwpnAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#The structure of .mat in Python\n",
    "\n",
    "#Here, you'll discover what is in the MATLAB dictionary that \n",
    "#you loaded in the previous exercise.\n",
    "\n",
    "#The file 'albeck_gene_expression.mat' is already loaded \n",
    "#into the variable mat. The following libraries have already \n",
    "#been imported as follows:\n",
    "\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#Once again, this file contains gene expression data from \n",
    "#the Albeck Lab at UCDavis. You can find the data and \n",
    "#some great documentation here.\n",
    "\n",
    "#Instructions\n",
    "\n",
    "#Use the method .keys() on the dictionary mat to print \n",
    "#the keys. Most of these keys (in fact the ones that do \n",
    "#NOT begin and end with '__') are variables from the \n",
    "#corresponding MATLAB environment.\n",
    "\n",
    "#Print the type of the value corresponding to the key \n",
    "#'CYratioCyt' in mat. Recall that mat['CYratioCyt'] \n",
    "#accesses the value.\n",
    "\n",
    "#Print the shape of the value corresponding to the key \n",
    "#'CYratioCyt' using the numpy function shape().\n",
    "\n",
    "#Execute the entire script to see some oscillatory gene \n",
    "#expression data!\n",
    "\n",
    "# Print the keys of the MATLAB dictionary\n",
    "print(mat.keys())\n",
    "\n",
    "# Print the type of the value corresponding to the key 'CYratioCyt'\n",
    "print(type(mat['CYratioCyt']))\n",
    "\n",
    "# Print the shape of the value corresponding to the key 'CYratioCyt'\n",
    "print(mat['CYratioCyt'].shape)\n",
    "\n",
    "# Subset the array and plot it\n",
    "data = mat['CYratioCyt'][25, 5:]\n",
    "fig = plt.figure()\n",
    "plt.plot(data)\n",
    "plt.xlabel('time (min.)')\n",
    "plt.ylabel('normalized fluorescence (measure of expression)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b2c68a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Album', 'Artist', 'Customer', 'Employee', 'Genre', 'Invoice', 'InvoiceLine', 'MediaType', 'Playlist', 'PlaylistTrack', 'Track']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LUISHE~1\\AppData\\Local\\Temp/ipykernel_12468/4129524908.py:25: SADeprecationWarning: The Engine.table_names() method is deprecated and will be removed in a future release.  Please refer to Inspector.get_table_names(). (deprecated since: 1.4)\n",
      "  table_names=engine.table_names()\n"
     ]
    }
   ],
   "source": [
    "#What are the tables in the database?\n",
    "\n",
    "\n",
    "#In this exercise, you'll once again create an engine to connect to 'Chinook.sqlite'. Before you can get any data out of the database, however, you'll need to know what tables it contains!\n",
    "\n",
    "#To this end, you'll save the table names to a list using the method table_names() on the engine and then you will print the list.\n",
    "\n",
    "#Instructions\n",
    "\n",
    "#Import the function create_engine from the module sqlalchemy.\n",
    "\n",
    "#Create an engine to connect to the SQLite database 'Chinook.sqlite' and assign it to engine.\n",
    "\n",
    "#Using the method table_names() on the engine engine, assign the table names of 'Chinook.sqlite' to the variable table_names.\n",
    "\n",
    "#Print the object table_names to the shell.\n",
    "\n",
    "# Import necessary module\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Create engine: engine\n",
    "engine=create_engine('sqlite:///Chinook.sqlite')\n",
    "\n",
    "# Save the table names to a list: table_names\n",
    "table_names=engine.table_names()\n",
    "\n",
    "# Print the table names to the shell\n",
    "print(table_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5f59a801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0                                      1  2\n",
      "0  1  For Those About To Rock We Salute You  1\n",
      "1  2                      Balls to the Wall  2\n",
      "2  3                      Restless and Wild  2\n",
      "3  4                      Let There Be Rock  1\n",
      "4  5                               Big Ones  3\n"
     ]
    }
   ],
   "source": [
    "#The Hello World of SQL Queries!\n",
    "\n",
    "#Now, it's time for liftoff! In this exercise, you'll perform the \n",
    "#Hello World of SQL queries, SELECT, in order to retrieve all \n",
    "#columns of the table Album in the Chinook database. \n",
    "#Recall that the query SELECT * selects all columns.\n",
    "\n",
    "#Open the engine connection as con using the method connect() on the engine.\n",
    "\n",
    "#Execute the query that selects ALL columns from the Album table. Store the results in rs.\n",
    "\n",
    "#Store all of your query results in the DataFrame df by applying the fetchall() method to the results rs.\n",
    "\n",
    "#Close the connection!\n",
    "\n",
    "# Import packages\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "# Create engine: engine\n",
    "engine = create_engine('sqlite:///Chinook.sqlite')\n",
    "\n",
    "# Open engine connection: con\n",
    "con=engine.connect()\n",
    "\n",
    "\n",
    "# Perform query: rs\n",
    "rs = con.execute('SELECT * FROM Album')\n",
    "\n",
    "# Save results of the query to DataFrame: df\n",
    "df = pd.DataFrame(rs.fetchall())\n",
    "\n",
    "# Close connection\n",
    "con.close()\n",
    "\n",
    "\n",
    "# Print head of DataFrame df\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ac721f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "  LastName                Title\n",
      "0    Adams      General Manager\n",
      "1  Edwards        Sales Manager\n",
      "2  Peacock  Sales Support Agent\n"
     ]
    }
   ],
   "source": [
    "#Customizing the Hello World of SQL Queries\n",
    "\n",
    "#Congratulations on executing your first SQL query! Now \n",
    "#you're going to figure out how to customize your query in \n",
    "#order to:\n",
    "\n",
    "#Select specified columns from a table;\n",
    "#Select a specified number of rows;\n",
    "#Import column names from the database table.\n",
    "\n",
    "#Recall that Hugo performed a very similar query \n",
    "#customization in the video:\n",
    "\n",
    "#engine = create_engine('sqlite:///Northwind.sqlite')\n",
    "\n",
    "#with engine.connect() as con:\n",
    "    #rs = con.execute(\"SELECT OrderID, OrderDate, ShipName FROM Orders\")\n",
    "    #df = pd.DataFrame(rs.fetchmany(size=5))\n",
    "    #df.columns = rs.keys()\n",
    "\n",
    "#Packages have already been imported as follows:\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "#The engine has also already been created:\n",
    "\n",
    "#engine = create_engine('sqlite:///Chinook.sqlite')\n",
    "\n",
    "#The engine connection is already open with the statement\n",
    "\n",
    "#with engine.connect() as con:\n",
    "\n",
    "#All the code you need to complete is within this context.\n",
    "\n",
    "#Instructions\n",
    "\n",
    "#Execute the SQL query that selects the columns \n",
    "#LastName and Title from the Employee table. Store \n",
    "#the results in the variable rs.\n",
    "\n",
    "#Apply the method fetchmany() to rs in order to \n",
    "#retrieve 3 of the records. Store them in the DataFrame \n",
    "#df.\n",
    "\n",
    "#Using the rs object, set the DataFrame's column names \n",
    "#to the corresponding names of the table columns.\n",
    "\n",
    "# Open engine in context manager\n",
    "# Perform query and save results to DataFrame: df\n",
    "with engine.connect() as con:\n",
    "    rs = con.execute('SELECT LastName, Title FROM Employee')\n",
    "    df = pd.DataFrame(rs.fetchmany(size=3))\n",
    "    df.columns = rs.keys()\n",
    "\n",
    "# Print the length of the DataFrame df\n",
    "print(len(df))\n",
    "\n",
    "# Print the head of the DataFrame df\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bdd75f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   EmployeeId  LastName FirstName       Title  ReportsTo            BirthDate  \\\n",
      "0           6  Mitchell   Michael  IT Manager          1  1973-07-01 00:00:00   \n",
      "1           7      King    Robert    IT Staff          6  1970-05-29 00:00:00   \n",
      "2           8  Callahan     Laura    IT Staff          6  1968-01-09 00:00:00   \n",
      "\n",
      "              HireDate                      Address        City State Country  \\\n",
      "0  2003-10-17 00:00:00         5827 Bowness Road NW     Calgary    AB  Canada   \n",
      "1  2004-01-02 00:00:00  590 Columbia Boulevard West  Lethbridge    AB  Canada   \n",
      "2  2004-03-04 00:00:00                  923 7 ST NW  Lethbridge    AB  Canada   \n",
      "\n",
      "  PostalCode              Phone                Fax                    Email  \n",
      "0    T3B 0C5  +1 (403) 246-9887  +1 (403) 246-9899  michael@chinookcorp.com  \n",
      "1    T1K 5N8  +1 (403) 456-9986  +1 (403) 456-8485   robert@chinookcorp.com  \n",
      "2    T1H 1Y8  +1 (403) 467-3351  +1 (403) 467-8772    laura@chinookcorp.com  \n"
     ]
    }
   ],
   "source": [
    "#Filtering your database records using \n",
    "#SQL's WHERE\n",
    "\n",
    "#You can now execute a basic SQL query to select records \n",
    "#from any table in your database and you can also perform \n",
    "#simple query customizations to select particular \n",
    "#columns and numbers of rows.\n",
    "\n",
    "#There are a couple more standard SQL query chops that \n",
    "#will aid you in your journey to becoming an SQL ninja.\n",
    "\n",
    "#Let's say, for example that you wanted to get all records \n",
    "#from the Customer table of the Chinook database for \n",
    "#which the Country is 'Canada'. You can do this very \n",
    "#easily in SQL using a SELECT statement followed \n",
    "#by a WHERE clause as follows:\n",
    "\n",
    "#SELECT * FROM Customer WHERE Country = 'Canada'\n",
    "\n",
    "#In fact, you can filter any SELECT statement by any \n",
    "#condition using a WHERE clause. This is called filtering your \n",
    "#records.\n",
    "\n",
    "#In this interactive exercise, you'll select all records of the \n",
    "#Employee table for which 'EmployeeId' is greater than or \n",
    "#equal to 6.\n",
    "\n",
    "#Packages are already imported as follows:\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "#Query away!\n",
    "\n",
    "#Instructions\n",
    "\n",
    "\n",
    "#Complete the argument of create_engine() so that the \n",
    "#engine for the SQLite database 'Chinook.sqlite' is \n",
    "#created.\n",
    "\n",
    "#Execute the query that selects all records from the \n",
    "#Employee table where 'EmployeeId' is greater than or \n",
    "#equal to 6. Use the >= operator and assign the results\n",
    " #to rs.\n",
    "\n",
    "#Apply the method fetchall() to rs in order to fetch all \n",
    "#records in rs. Store them in the DataFrame df.\n",
    "\n",
    "#Using the rs object, set the DataFrame's column names \n",
    "#to the corresponding names of the table columns.\n",
    "\n",
    "# Create engine: engine\n",
    "engine = create_engine('sqlite:///Chinook.sqlite')\n",
    "\n",
    "# Open engine in context manager\n",
    "# Perform query and save results to DataFrame: df\n",
    "with engine.connect() as con:\n",
    "    rs = con.execute('SELECT * FROM Employee WHERE EmployeeId >= 6')\n",
    "    df = pd.DataFrame(rs.fetchall())\n",
    "    df.columns = rs.keys()\n",
    "\n",
    "# Print the head of the DataFrame df\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a6f65e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   EmployeeId  LastName FirstName                Title  ReportsTo  \\\n",
      "0           4      Park  Margaret  Sales Support Agent        2.0   \n",
      "1           2   Edwards     Nancy        Sales Manager        1.0   \n",
      "2           1     Adams    Andrew      General Manager        NaN   \n",
      "3           5   Johnson     Steve  Sales Support Agent        2.0   \n",
      "4           8  Callahan     Laura             IT Staff        6.0   \n",
      "\n",
      "             BirthDate             HireDate              Address        City  \\\n",
      "0  1947-09-19 00:00:00  2003-05-03 00:00:00     683 10 Street SW     Calgary   \n",
      "1  1958-12-08 00:00:00  2002-05-01 00:00:00         825 8 Ave SW     Calgary   \n",
      "2  1962-02-18 00:00:00  2002-08-14 00:00:00  11120 Jasper Ave NW    Edmonton   \n",
      "3  1965-03-03 00:00:00  2003-10-17 00:00:00         7727B 41 Ave     Calgary   \n",
      "4  1968-01-09 00:00:00  2004-03-04 00:00:00          923 7 ST NW  Lethbridge   \n",
      "\n",
      "  State Country PostalCode              Phone                Fax  \\\n",
      "0    AB  Canada    T2P 5G3  +1 (403) 263-4423  +1 (403) 263-4289   \n",
      "1    AB  Canada    T2P 2T3  +1 (403) 262-3443  +1 (403) 262-3322   \n",
      "2    AB  Canada    T5K 2N1  +1 (780) 428-9482  +1 (780) 428-3457   \n",
      "3    AB  Canada    T3B 1Y7   1 (780) 836-9987   1 (780) 836-9543   \n",
      "4    AB  Canada    T1H 1Y8  +1 (403) 467-3351  +1 (403) 467-8772   \n",
      "\n",
      "                      Email  \n",
      "0  margaret@chinookcorp.com  \n",
      "1     nancy@chinookcorp.com  \n",
      "2    andrew@chinookcorp.com  \n",
      "3     steve@chinookcorp.com  \n",
      "4     laura@chinookcorp.com  \n"
     ]
    }
   ],
   "source": [
    "#Ordering your SQL records with ORDER BY\n",
    "\n",
    "#You can also order your SQL query results. For example, if \n",
    "#you wanted to get all records from the Customer table of \n",
    "#the Chinook database and order them in increasing order \n",
    "#by the column SupportRepId, you could do so with the \n",
    "#following query:\n",
    "\n",
    "#\"SELECT * FROM Customer ORDER BY SupportRepId\"\n",
    "\n",
    "#In fact, you can order any SELECT statement by any column.\n",
    "\n",
    "#In this interactive exercise, you'll select all records of the \n",
    "#Employee table and order them in increasing order by the \n",
    "#column BirthDate.\n",
    "\n",
    "#Packages are already imported as follows:\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "#Get querying!\n",
    "\n",
    "#Instructions\n",
    "\n",
    "#Using the function create_engine(), create an engine \n",
    "#for the SQLite database Chinook.sqlite and assign it to \n",
    "#the variable engine.\n",
    "\n",
    "#In the context manager, execute the query that selects \n",
    "#all records from the Employee table and orders them in \n",
    "#increasing order by the column BirthDate. Assign the \n",
    "#result to rs.\n",
    "\n",
    "#In a call to pd.DataFrame(), apply the method \n",
    "#fetchall() to rs in order to fetch all records in rs. \n",
    "#Store them in the DataFrame df.\n",
    "\n",
    "#Set the DataFrame's column names to the corresponding \n",
    "#names of the table columns.\n",
    "\n",
    "# Create engine: engine\n",
    "engine=create_engine('sqlite:///Chinook.sqlite')\n",
    "\n",
    "\n",
    "# Open engine in context manager\n",
    "with engine.connect() as con:\n",
    "    rs = con.execute('SELECT * FROM Employee ORDER BY BirthDate')\n",
    "    df = pd.DataFrame(rs.fetchall())\n",
    "\n",
    "    # Set the DataFrame's column names\n",
    "    df.columns=rs.keys()\n",
    "\n",
    "\n",
    "# Print head of DataFrame\n",
    "print(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "905e0723",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid decimal literal (Temp/ipykernel_12468/2680507467.py, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\LUISHE~1\\AppData\\Local\\Temp/ipykernel_12468/2680507467.py\"\u001b[1;36m, line \u001b[1;32m13\u001b[0m\n\u001b[1;33m    3the Northwind database, Hugo executed the following\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid decimal literal\n"
     ]
    }
   ],
   "source": [
    "#Pandas and The Hello World of SQL\n",
    " #Queries!\n",
    "\n",
    "#Here, you'll take advantage of the power of pandas to \n",
    "#write the results of your SQL query to a DataFrame in one \n",
    "#swift line of Python code!\n",
    "\n",
    "#You'll first import pandas and create the SQLite \n",
    "#'Chinook.sqlite' engine. Then you'll query the database \n",
    "#to select all records from the Album table.\n",
    "\n",
    "#Recall that to select all records from the Orders table in \n",
    "3the Northwind database, Hugo executed the following \n",
    "#command:\n",
    "\n",
    "#df = pd.read_sql_query(\"SELECT * FROM Orders\", engine)\n",
    "\n",
    "#Instructions\n",
    "\n",
    "#Import the pandas package using the alias pd.\n",
    "\n",
    "#Using the function create_engine(), create an engine \n",
    "#for the SQLite database Chinook.sqlite and assign it to \n",
    "#the variable engine.\n",
    "\n",
    "#Use the pandas function read_sql_query() to assign to \n",
    "#the variable df the DataFrame of results from the \n",
    "#following query: select all records from the table Album.\n",
    "\n",
    "#The remainder of the code is included to confirm that the \n",
    "#DataFrame created by this method is equal to that \n",
    "#created by the previous method that you learned.\n",
    "\n",
    "# Import packages\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "# Create engine: engine\n",
    "engine=create_engine('sqlite:///Chinook.sqlite')\n",
    "\n",
    "# Execute query and store records in DataFrame: df\n",
    "df = pd.read_sql_query('SELECT * FROM Album', engine)\n",
    "\n",
    "# Print head of DataFrame\n",
    "print(df.head())\n",
    "\n",
    "# Open engine in context manager and store query result in df1\n",
    "with engine.connect() as con:\n",
    "    rs = con.execute(\"SELECT * FROM Album\")\n",
    "    df1 = pd.DataFrame(rs.fetchall())\n",
    "    df1.columns = rs.keys()\n",
    "\n",
    "# Confirm that both methods yield the same result\n",
    "print(df.equals(df1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "37f7c73d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   EmployeeId  LastName FirstName       Title  ReportsTo            BirthDate  \\\n",
      "0           8  Callahan     Laura    IT Staff          6  1968-01-09 00:00:00   \n",
      "1           7      King    Robert    IT Staff          6  1970-05-29 00:00:00   \n",
      "2           6  Mitchell   Michael  IT Manager          1  1973-07-01 00:00:00   \n",
      "\n",
      "              HireDate                      Address        City State Country  \\\n",
      "0  2004-03-04 00:00:00                  923 7 ST NW  Lethbridge    AB  Canada   \n",
      "1  2004-01-02 00:00:00  590 Columbia Boulevard West  Lethbridge    AB  Canada   \n",
      "2  2003-10-17 00:00:00         5827 Bowness Road NW     Calgary    AB  Canada   \n",
      "\n",
      "  PostalCode              Phone                Fax                    Email  \n",
      "0    T1H 1Y8  +1 (403) 467-3351  +1 (403) 467-8772    laura@chinookcorp.com  \n",
      "1    T1K 5N8  +1 (403) 456-9986  +1 (403) 456-8485   robert@chinookcorp.com  \n",
      "2    T3B 0C5  +1 (403) 246-9887  +1 (403) 246-9899  michael@chinookcorp.com  \n"
     ]
    }
   ],
   "source": [
    "#Pandas for more complex querying\n",
    "\n",
    "#Here, you'll become more familiar with the pandas function \n",
    "#read_sql_query() by using it to execute a more complex \n",
    "#query: a SELECT statement followed by both a WHERE \n",
    "#clause AND an ORDER BY clause.\n",
    "\n",
    "#You'll build a DataFrame that contains the rows of the \n",
    "#Employee table for which the EmployeeId is greater than \n",
    "#or equal to 6 and you'll order these entries by BirthDate\n",
    "\n",
    "#Instructions\n",
    "\n",
    "#Using the function create_engine(), create an engine \n",
    "#for the SQLite database Chinook.sqlite and assign it to \n",
    "#the variable engine.\n",
    "\n",
    "#Use the pandas function read_sql_query() to assign to \n",
    "#the variable df the DataFrame of results from the \n",
    "#following query: select all records from the Employee \n",
    "#table where the EmployeeId is greater than or equal to \n",
    "#6 and ordered by BirthDate (make sure to use WHERE \n",
    "#and ORDER BY in this precise order).\n",
    "\n",
    "# Import packages\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "# Create engine: engine\n",
    "engine=create_engine('sqlite:///Chinook.sqlite')\n",
    "\n",
    "\n",
    "# Execute query and store records in DataFrame: df\n",
    "df=pd.read_sql_query('SELECT * FROM Employee WHERE EmployeeId >= 6 ORDER BY BirthDate', engine)\n",
    "\n",
    "# Print head of DataFrame\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0fc7336b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   Title       Name\n",
      "0  For Those About To Rock We Salute You      AC/DC\n",
      "1                      Balls to the Wall     Accept\n",
      "2                      Restless and Wild     Accept\n",
      "3                      Let There Be Rock      AC/DC\n",
      "4                               Big Ones  Aerosmith\n"
     ]
    }
   ],
   "source": [
    "#The power of SQL lies in relationships \n",
    "#between tables: INNER JOIN\n",
    "\n",
    "#Here, you'll perform your first INNER JOIN! You'll be \n",
    "#working with your favourite SQLite database, \n",
    "#Chinook.sqlite. For each record in the Album table,\n",
    "# you'll extract the Title along with the Name of the \n",
    "#Artist. The latter will come from the Artist table and \n",
    "#so you will need to INNER JOIN these two tables on the \n",
    "#ArtistID column of both.\n",
    "\n",
    "#Recall that to INNER JOIN the Orders and Customers \n",
    "#tables from the Northwind database, Hugo executed the \n",
    "#following SQL query:\n",
    "\n",
    "#\"SELECT OrderID, CompanyName FROM Orders INNER JOIN Customers on Orders.CustomerID = Customers.CustomerID\"\n",
    "\n",
    "#The following code has already been executed to import \n",
    "#the necessary packages and to create the engine:\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('sqlite:///Chinook.sqlite')\n",
    "\n",
    "#Assign to rs the results from the following query: select \n",
    "#all the records, extracting the Title of the record and \n",
    "#Name of the artist of each record from the Album table \n",
    "#and the Artist table, respectively. To do so,\n",
    "# INNER JOIN these two tables on the ArtistID column \n",
    "#of both.\n",
    "\n",
    "#In a call to pd.DataFrame(), apply the method \n",
    "#fetchall() to rs in order to fetch all records in rs. \n",
    "#Store them in the DataFrame df.\n",
    "\n",
    "#Set the DataFrame's column names to the corresponding \n",
    "#names of the table columns.\n",
    "\n",
    "# Open engine in context manager\n",
    "# Perform query and save results to DataFrame: df\n",
    "with engine.connect() as con:\n",
    "    rs=con.execute('SELECT Title, Name FROM Album INNER JOIN Artist on Album.ArtistId=Artist.ArtistId')\n",
    "    df=pd.DataFrame(rs.fetchall())\n",
    "    df.columns=rs.keys()\n",
    "\n",
    "# Print head of DataFrame df\n",
    "print(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a673d9f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PlaylistId  TrackId  TrackId              Name  AlbumId  MediaTypeId  \\\n",
      "0           1     3390     3390  One and the Same      271            2   \n",
      "1           1     3392     3392     Until We Fall      271            2   \n",
      "2           1     3393     3393     Original Fire      271            2   \n",
      "3           1     3394     3394       Broken City      271            2   \n",
      "4           1     3395     3395          Somedays      271            2   \n",
      "\n",
      "   GenreId Composer  Milliseconds    Bytes  UnitPrice  \n",
      "0       23     None        217732  3559040       0.99  \n",
      "1       23     None        230758  3766605       0.99  \n",
      "2       23     None        218916  3577821       0.99  \n",
      "3       23     None        228366  3728955       0.99  \n",
      "4       23     None        213831  3497176       0.99  \n"
     ]
    }
   ],
   "source": [
    "#Filtering your INNER JOIN\n",
    "\n",
    "#Congrats on performing your first INNER JOIN! You're now \n",
    "#going to finish this chapter with one final exercise in which \n",
    "#you perform an INNER JOIN and filter the result using a \n",
    "#WHERE clause.\n",
    "\n",
    "#Recall that to INNER JOIN the Orders and Customers \n",
    "#tables from the Northwind database, Hugo executed the \n",
    "#following SQL query:\n",
    "\n",
    "#\"SELECT OrderID, CompanyName FROM Orders INNER JOIN Customers on Orders.CustomerID = Customers.CustomerID\"\n",
    "\n",
    "#The following code has already been executed to import \n",
    "#the necessary packages and to create the engine:\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('sqlite:///Chinook.sqlite')\n",
    "\n",
    "#Use the pandas function read_sql_query() to assign to \n",
    "#the variable df the DataFrame of results from the \n",
    "#following query: select all records from \n",
    "#PlaylistTrack INNER JOIN Track on \n",
    "#PlaylistTrack.TrackId = Track.TrackId \n",
    "#that satisfy the condition Milliseconds < 250000.\n",
    "\n",
    "# Execute query and store records in DataFrame: df\n",
    "df=pd.read_sql_query('SELECT * FROM PlaylistTrack INNER JOIN Track on PlaylistTrack.TrackId=Track.TrackId WHERE Milliseconds<250000',engine)\n",
    "\n",
    "# Print head of DataFrame\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c8d1c30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
      "0            7.4              0.70         0.00             1.9      0.076   \n",
      "1            7.8              0.88         0.00             2.6      0.098   \n",
      "2            7.8              0.76         0.04             2.3      0.092   \n",
      "3           11.2              0.28         0.56             1.9      0.075   \n",
      "4            7.4              0.70         0.00             1.9      0.076   \n",
      "\n",
      "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
      "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
      "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
      "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
      "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
      "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
      "\n",
      "   alcohol  quality  \n",
      "0      9.4        5  \n",
      "1      9.8        5  \n",
      "2      9.8        5  \n",
      "3      9.8        6  \n",
      "4      9.4        5  \n"
     ]
    }
   ],
   "source": [
    "#Importing flat files from the web: your turn!\n",
    "\n",
    "#You are about to import your first file from the web! The \n",
    "#flat file you will import will be 'winequality-red.csv' \n",
    "#from the University of California, Irvine's Machine Learning \n",
    "#repository. The flat file contains tabular data of \n",
    "#physiochemical properties of red wine, such as pH, alcohol \n",
    "#content and citric acid content, along with wine quality \n",
    "#rating.\n",
    "\n",
    "#The URL of the file is\n",
    "\n",
    "#'https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'\n",
    "\n",
    "#After you import it, you'll check your working directory to \n",
    "#confirm that it is there and then you'll load it into a \n",
    "#pandas DataFrame.\n",
    "\n",
    "#Instructions\n",
    "\n",
    "#Import the function urlretrieve from the subpackage \n",
    "#urllib.request.\n",
    "\n",
    "#Assign the URL of the file to the variable url.\n",
    "\n",
    "#Use the function urlretrieve() to save the file locally \n",
    "#as 'winequality-red.csv'.\n",
    "\n",
    "#Execute the remaining code to load \n",
    "#'winequality-red.csv' in a pandas DataFrame and to \n",
    "#print its head to the shell.\n",
    "\n",
    "# Import package\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Assign url of file: url\n",
    "url='https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'\n",
    "\n",
    "# Save file locally\n",
    "urlretrieve(url, 'winequality-red.csv')\n",
    "\n",
    "# Read file into a DataFrame and print its head\n",
    "df = pd.read_csv('winequality-red.csv', sep=';')\n",
    "print(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "86c89acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luis hernandez\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3457: FutureWarning: In a future version of pandas all arguments of read_csv except for the argument 'filepath_or_buffer' will be keyword-only\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
      "0            7.4              0.70         0.00             1.9      0.076   \n",
      "1            7.8              0.88         0.00             2.6      0.098   \n",
      "2            7.8              0.76         0.04             2.3      0.092   \n",
      "3           11.2              0.28         0.56             1.9      0.075   \n",
      "4            7.4              0.70         0.00             1.9      0.076   \n",
      "\n",
      "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
      "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
      "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
      "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
      "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
      "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
      "\n",
      "   alcohol  quality  \n",
      "0      9.4        5  \n",
      "1      9.8        5  \n",
      "2      9.8        5  \n",
      "3      9.8        6  \n",
      "4      9.4        5  \n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'ix'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\LUISHE~1\\AppData\\Local\\Temp/ipykernel_12468/103622731.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;31m# Plot first column of df\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'fixed acidity (g(tartaric acid)/dm$^3$)'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'count'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5485\u001b[0m         ):\n\u001b[0;32m   5486\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5487\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5488\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5489\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'ix'"
     ]
    }
   ],
   "source": [
    "#Opening and reading flat files from the web\n",
    "\n",
    "#You have just imported a file from the web, saved it locally \n",
    "#and loaded it into a DataFrame. If you just wanted to load \n",
    "#a file from the web into a DataFrame without first saving it \n",
    "#locally, you can do that easily using pandas. In particular,\n",
    "# you can use the function pd.read_csv() with the URL as \n",
    "#the first argument and the separator sep as the second \n",
    "#argument.\n",
    "\n",
    "#The URL of the file, once again, is\n",
    "\n",
    "#'https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'\n",
    "\n",
    "#Instructions\n",
    "\n",
    "#Assign the URL of the file to the variable url.\n",
    "\n",
    "#Read file into a DataFrame df using pd.read_csv(), \n",
    "#recalling that the separator in the file is ';'.\n",
    "\n",
    "#Print the head of the DataFrame df.\n",
    "\n",
    "#Execute the rest of the code to plot histogram of the first \n",
    "#feature in the DataFrame df.\n",
    "\n",
    "# Import packages\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Assign url of file: url\n",
    "\n",
    "url='https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/datasets/winequality-red.csv'\n",
    "\n",
    "\n",
    "# Read file into a DataFrame: df\n",
    "df=pd.read_csv(url,';')\n",
    "\n",
    "# Print the head of the DataFrame\n",
    "print(df.head())\n",
    "\n",
    "# Plot first column of df\n",
    "pd.DataFrame.hist(df.ix[:, 0:1])\n",
    "plt.xlabel('fixed acidity (g(tartaric acid)/dm$^3$)')\n",
    "plt.ylabel('count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f47d71c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Missing optional dependency 'xlrd'. Install xlrd >= 1.0.0 for Excel support Use pip or conda to install xlrd.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\LUISHE~1\\AppData\\Local\\Temp/ipykernel_12468/1127605311.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;31m# Read in all sheets of Excel file: xls\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m \u001b[0mxls\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msheet_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;31m# Print the sheetnames to the shell\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001b[0m\n\u001b[0;32m    362\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 364\u001b[1;33m         \u001b[0mio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    365\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m         raise ValueError(\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[0;32m   1231\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstorage_options\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1233\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engines\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_io\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1234\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1235\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__fspath__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\excel\\_xlrd.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, filepath_or_buffer, storage_options)\u001b[0m\n\u001b[0;32m     22\u001b[0m         \"\"\"\n\u001b[0;32m     23\u001b[0m         \u001b[0merr_msg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Install xlrd >= 1.0.0 for Excel support\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0mimport_optional_dependency\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"xlrd\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextra\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\compat\\_optional.py\u001b[0m in \u001b[0;36mimport_optional_dependency\u001b[1;34m(name, extra, errors, min_version)\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"raise\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: Missing optional dependency 'xlrd'. Install xlrd >= 1.0.0 for Excel support Use pip or conda to install xlrd."
     ]
    }
   ],
   "source": [
    "#Importing non-flat files from the web\n",
    "\n",
    "#Congrats! You've just loaded a flat file from the web into a \n",
    "#DataFrame without first saving it locally using the pandas \n",
    "#function pd.read_csv(). This function is super cool \n",
    "#because it has close relatives that allow you to load all \n",
    "#types of files, not only flat ones. In this interactive exercise, \n",
    "#you'll use pd.read_excel() to import an Excel spreadsheet.\n",
    "\n",
    "#The URL of the spreadsheet is\n",
    "\n",
    "#'http://s3.amazonaws.com/assets.datacamp.com/course/importing_data_into_r/latitude.xls'\n",
    "\n",
    "#Your job is to use pd.read_excel() to read in all of its \n",
    "#sheets, print the sheet names and then print the head of \n",
    "#the first sheet using its name, not its index.\n",
    "\n",
    "#Note that the output of pd.read_excel() is a Python \n",
    "#dictionary with sheet names as keys and corresponding \n",
    "#DataFrames as corresponding values.\n",
    "\n",
    "#Instructions\n",
    "\n",
    "#Assign the URL of the file to the variable url.\n",
    "\n",
    "#Read the file in url into a dictionary xls using \n",
    "#pd.read_excel() recalling that, in order to import all \n",
    "#sheets you need to pass None to the argument \n",
    "#sheet_name.\n",
    "\n",
    "#Print the names of the sheets in the Excel spreadsheet; \n",
    "#these will be the keys of the dictionary xls.\n",
    "\n",
    "#Print the head of the first sheet using the sheet name, not \n",
    "#the index of the sheet! The sheet name is '1700'\n",
    "\n",
    "# Import package\n",
    "import pandas as pd\n",
    "\n",
    "# Assign url of file: url\n",
    "url='http://s3.amazonaws.com/assets.datacamp.com/course/importing_data_into_r/latitude.xls'\n",
    "\n",
    "# Read in all sheets of Excel file: xls\n",
    "xls=pd.read_excel(url,sheet_name=None)\n",
    "\n",
    "# Print the sheetnames to the shell\n",
    "print(xls.keys())\n",
    "\n",
    "# Print the head of the first sheet (using its name, NOT its index)\n",
    "print(xls['1700'].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "600044f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'http.client.HTTPResponse'>\n"
     ]
    }
   ],
   "source": [
    "#Performing HTTP requests in Python using urllib\n",
    "\n",
    "#Now that you know the basics behind HTTP GET requests,\n",
    " #it's time to perform some of your own. In this interactive \n",
    "#exercise, you will ping our very own DataCamp servers to \n",
    "#perform a GET request to extract information from the first \n",
    "#coding exercise of this course,\n",
    " #\"https://campus.datacamp.com/courses/1606/4135?ex=2\".\n",
    "\n",
    "#In the next exercise, you'll extract the HTML itself. Right \n",
    "#now, however, you are going to package and send the \n",
    "#request and then catch the response.\n",
    "\n",
    "#Instructions\n",
    "\n",
    "#Import the functions urlopen and Request from the \n",
    "#subpackage urllib.request.\n",
    "\n",
    "#Package the request to the url \n",
    "#\"https://campus.datacamp.com/courses/1606/4135?ex=2\" \n",
    "#using the function Request() and assign it to request.\n",
    "\n",
    "#Send the request and catch the response in the variable \n",
    "#response with the function urlopen().\n",
    "\n",
    "#Run the rest of the code to see the datatype of \n",
    "#response and to close the connection!\n",
    "\n",
    "# Import packages\n",
    "from urllib.request import urlopen, Request\n",
    "\n",
    "# Specify the url\n",
    "url = \"https://campus.datacamp.com/courses/1606/4135?ex=2\"\n",
    "\n",
    "# This packages the request: request\n",
    "request=Request(url)\n",
    "\n",
    "# Sends the request and catches the response: response\n",
    "response=urlopen(request)\n",
    "\n",
    "# Print the datatype of response\n",
    "print(type(response))\n",
    "\n",
    "# Be polite and close the response!\n",
    "response.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8bd8f696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<!doctype html><html lang=\"en\"><head><link rel=\"apple-touch-icon-precomposed\" sizes=\"57x57\" href=\"/campus/apple-touch-icon-57x57.png\"><link rel=\"apple-touch-icon-precomposed\" sizes=\"114x114\" href=\"/campus/apple-touch-icon-114x114.png\"><link rel=\"apple-touch-icon-precomposed\" sizes=\"72x72\" href=\"/campus/apple-touch-icon-72x72.png\"><link rel=\"apple-touch-icon-precomposed\" sizes=\"144x144\" href=\"/campus/apple-touch-icon-144x144.png\"><link rel=\"apple-touch-icon-precomposed\" sizes=\"60x60\" href=\"/campus/apple-touch-icon-60x60.png\"><link rel=\"apple-touch-icon-precomposed\" sizes=\"120x120\" href=\"/campus/apple-touch-icon-120x120.png\"><link rel=\"apple-touch-icon-precomposed\" sizes=\"76x76\" href=\"/campus/apple-touch-icon-76x76.png\"><link rel=\"apple-touch-icon-precomposed\" sizes=\"152x152\" href=\"/campus/apple-touch-icon-152x152.png\"><link rel=\"icon\" type=\"image/png\" href=\"/campus/favicon.ico\"><link rel=\"icon\" type=\"image/png\" href=\"/campus/favicon-196x196.png\" sizes=\"196x196\"><link rel=\"icon\" type=\"image/png\" href=\"/campus/favicon-96x96.png\" sizes=\"96x96\"><link rel=\"icon\" type=\"image/png\" href=\"/campus/favicon-32x32.png\" sizes=\"32x32\"><link rel=\"icon\" type=\"image/png\" href=\"/campus/favicon-16x16.png\" sizes=\"16x16\"><link rel=\"icon\" type=\"image/png\" href=\"/campus/favicon-128.png\" sizes=\"128x128\"><meta name=\"application-name\" content=\"DataCamp\"><meta name=\"msapplication-TileColor\" content=\"#FFFFFF\"><meta name=\"msapplication-TileImage\" content=\"/campus/mstile-144x144.png\"><meta name=\"msapplication-square70x70logo\" content=\"/campus/mstile-70x70.png\"><meta name=\"msapplication-square150x150logo\" content=\"/campus/mstile-150x150.png\"><meta name=\"msapplication-wide310x150logo\" content=\"/campus/mstile-310x150.png\"><meta name=\"msapplication-square310x310logo\" content=\"/campus/mstile-310x310.png\"><link href=\"/campus/static/css/17.279a2d9e.chunk.css\" rel=\"stylesheet\"><link href=\"/campus/static/css/main.88385ec5.chunk.css\" rel=\"stylesheet\"><title data-react-helmet=\"true\">Importing flat files from the web: your turn! | Python</title><link data-react-helmet=\"true\" rel=\"canonical\" href=\"https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=2\"><meta data-react-helmet=\"true\" charset=\"utf-8\"><meta data-react-helmet=\"true\" http-equiv=\"X-UA-Compatible\" content=\"IE=edge,chrome=1\"><meta data-react-helmet=\"true\" name=\"viewport\" content=\"width=device-width, initial-scale=1, maximum-scale=1\"><meta data-react-helmet=\"true\" name=\"fragment\" content=\"!\"><meta data-react-helmet=\"true\" name=\"keywords\" content=\"R, Python, Data analysis, interactive, learning\"><meta data-react-helmet=\"true\" name=\"description\" content=\"Here is an example of Importing flat files from the web: your turn!: You are about to import your first file from the web! The flat file you will import will be &apos;winequality-red.\"><meta data-react-helmet=\"true\" name=\"twitter:card\" content=\"summary\"><meta data-react-helmet=\"true\" name=\"twitter:site\" content=\"@DataCamp\"><meta data-react-helmet=\"true\" name=\"twitter:title\" content=\"Importing flat files from the web: your turn! | Python\"><meta data-react-helmet=\"true\" name=\"twitter:description\" content=\"Here is an example of Importing flat files from the web: your turn!: You are about to import your first file from the web! The flat file you will import will be &apos;winequality-red.\"><meta data-react-helmet=\"true\" name=\"twitter:creator\" content=\"@DataCamp\"><meta data-react-helmet=\"true\" name=\"twitter:image:src\" content=\"/public/assets/images/var/twitter_share.png\"><meta data-react-helmet=\"true\" name=\"twitter:domain\" content=\"www.datacamp.com\"><meta data-react-helmet=\"true\" property=\"og:title\" content=\"Importing flat files from the web: your turn! | Python\"><meta data-react-helmet=\"true\" property=\"og:image\" content=\"/public/assets/images/var/linkedin_share.png\"><meta data-react-helmet=\"true\" name=\"google-signin-clientid\" content=\"892114885437-01a7plbsu1b2vobuhvnckmmanhb58h3a.apps.googleusercontent.com\"><meta data-react-helmet=\"true\" name=\"google-signin-scope\" content=\"email profile\"><meta data-react-helmet=\"true\" name=\"google-signin-cookiepolicy\" content=\"single_host_origin\"><script data-react-helmet=\"true\" async=\"true\" src=\"https://compliance.datacamp.com/base.js\"></script><script async src=\\'/cdn-cgi/bm/cv/669835187/api.js\\'></script></head><body><script>window.PRELOADED_STATE = \"[&quot;~#iR&quot;,[&quot;^ &quot;,&quot;n&quot;,&quot;StateRecord&quot;,&quot;v&quot;,[&quot;^ &quot;,&quot;backendSession&quot;,[&quot;~#iOM&quot;,[&quot;status&quot;,[&quot;^2&quot;,[&quot;code&quot;,&quot;none&quot;,&quot;text&quot;,&quot;&quot;]],&quot;isInitSession&quot;,false,&quot;message&quot;,null]],&quot;boot&quot;,[&quot;^0&quot;,[&quot;^ &quot;,&quot;n&quot;,&quot;BootStateRecord&quot;,&quot;v&quot;,[&quot;^ &quot;,&quot;bootState&quot;,&quot;PRE_BOOTED&quot;,&quot;error&quot;,null]]],&quot;chapter&quot;,[&quot;^2&quot;,[&quot;current&quot;,[&quot;^2&quot;,[&quot;badge_uncompleted_url&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing_unc.png&quot;,&quot;number&quot;,1,&quot;number_of_videos&quot;,3,&quot;slug&quot;,&quot;importing-data-from-the-internet-1&quot;,&quot;last_updated_on&quot;,&quot;09/12/2021&quot;,&quot;title_meta&quot;,null,&quot;nb_exercises&quot;,12,&quot;free_preview&quot;,true,&quot;slides_link&quot;,&quot;https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/slides/chapter1.pdf&quot;,&quot;title&quot;,&quot;Importing data from the Internet&quot;,&quot;xp&quot;,1050,&quot;id&quot;,4135,&quot;exercises&quot;,[&quot;~#iL&quot;,[[&quot;^2&quot;,[&quot;type&quot;,&quot;VideoExercise&quot;,&quot;title&quot;,&quot;Importing flat files from the web&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,1,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=1&quot;]],[&quot;^2&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Importing flat files from the web: your turn!&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,2,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=2&quot;]],[&quot;^2&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Opening and reading flat files from the web&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,3,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=3&quot;]],[&quot;^2&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Importing non-flat files from the web&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,4,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=4&quot;]],[&quot;^2&quot;,[&quot;type&quot;,&quot;VideoExercise&quot;,&quot;title&quot;,&quot;HTTP requests to import files from the web&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,5,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=5&quot;]],[&quot;^2&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Performing HTTP requests in Python using urllib&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,6,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=6&quot;]],[&quot;^2&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Printing HTTP request results in Python using urllib&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,7,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=7&quot;]],[&quot;^2&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Performing HTTP requests in Python using requests&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,8,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=8&quot;]],[&quot;^2&quot;,[&quot;type&quot;,&quot;VideoExercise&quot;,&quot;title&quot;,&quot;Scraping the web in Python&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,9,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=9&quot;]],[&quot;^2&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Parsing HTML with BeautifulSoup&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,10,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=10&quot;]],[&quot;^2&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Turning a webpage into data using BeautifulSoup: getting the text&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,11,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=11&quot;]],[&quot;^2&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Turning a webpage into data using BeautifulSoup: getting the hyperlinks&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,12,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=12&quot;]]]],&quot;description&quot;,&quot;The web is a rich source of data from which you can extract various types of insights and findings. In this chapter, you will learn how to get data from the web, whether it is stored in files or in HTML. You&#39;ll also learn the basics of scraping and parsing web data.&quot;,&quot;badge_completed_url&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing.png&quot;]]]],&quot;contentAuthorization&quot;,[&quot;^ &quot;],&quot;course&quot;,[&quot;^2&quot;,[&quot;difficulty_level&quot;,1,&quot;reduced_outline&quot;,null,&quot;marketing_video&quot;,&quot;&quot;,&quot;active_image&quot;,&quot;course-1606-master:d5b250978170fe950a6ca0a26b6049af-20211209132516524&quot;,&quot;author_field&quot;,null,&quot;chapters&quot;,[&quot;^7&quot;,[[&quot;^2&quot;,[&quot;badge_uncompleted_url&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing_unc.png&quot;,&quot;number&quot;,1,&quot;number_of_videos&quot;,3,&quot;slug&quot;,&quot;importing-data-from-the-internet-1&quot;,&quot;last_updated_on&quot;,&quot;09/12/2021&quot;,&quot;title_meta&quot;,null,&quot;nb_exercises&quot;,12,&quot;free_preview&quot;,true,&quot;slides_link&quot;,&quot;https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/slides/chapter1.pdf&quot;,&quot;title&quot;,&quot;Importing data from the Internet&quot;,&quot;xp&quot;,1050,&quot;id&quot;,4135,&quot;exercises&quot;,[&quot;^7&quot;,[[&quot;^2&quot;,[&quot;type&quot;,&quot;VideoExercise&quot;,&quot;title&quot;,&quot;Importing flat files from the web&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,1,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=1&quot;]],[&quot;^2&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Importing flat files from the web: your turn!&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,2,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=2&quot;]],[&quot;^2&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Opening and reading flat files from the web&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,3,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=3&quot;]],[&quot;^2&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Importing non-flat files from the web&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,4,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=4&quot;]],[&quot;^2&quot;,[&quot;type&quot;,&quot;VideoExercise&quot;,&quot;title&quot;,&quot;HTTP requests to import files from the web&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,5,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=5&quot;]],[&quot;^2&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Performing HTTP requests in Python using urllib&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,6,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=6&quot;]],[&quot;^2&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Printing HTTP request results in Python using urllib&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,7,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=7&quot;]],[&quot;^2&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Performing HTTP requests in Python using requests&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,8,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=8&quot;]],[&quot;^2&quot;,[&quot;type&quot;,&quot;VideoExercise&quot;,&quot;title&quot;,&quot;Scraping the web in Python&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,9,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=9&quot;]],[&quot;^2&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Parsing HTML with BeautifulSoup&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,10,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=10&quot;]],[&quot;^2&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Turning a webpage into data using BeautifulSoup: getting the text&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,11,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=11&quot;]],[&quot;^2&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Turning a webpage into data using BeautifulSoup: getting the hyperlinks&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,12,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=12&quot;]]]],&quot;description&quot;,&quot;The web is a rich source of data from which you can extract various types of insights and findings. In this chapter, you will learn how to get data from the web, whether it is stored in files or in HTML. You&#39;ll also learn the basics of scraping and parsing web data.&quot;,&quot;badge_completed_url&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing.png&quot;]],[&quot;^2&quot;,[&quot;badge_uncompleted_url&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing_unc.png&quot;,&quot;number&quot;,2,&quot;number_of_videos&quot;,2,&quot;slug&quot;,&quot;interacting-with-apis-to-import-data-from-the-web-2&quot;,&quot;last_updated_on&quot;,&quot;09/12/2021&quot;,&quot;title_meta&quot;,null,&quot;nb_exercises&quot;,9,&quot;free_preview&quot;,null,&quot;slides_link&quot;,&quot;https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/slides/chapter2.pdf&quot;,&quot;title&quot;,&quot;Interacting with APIs to import data from the web&quot;,&quot;xp&quot;,650,&quot;id&quot;,4136,&quot;exercises&quot;,[&quot;^7&quot;,[[&quot;^2&quot;,[&quot;type&quot;,&quot;VideoExercise&quot;,&quot;title&quot;,&quot;Introduction to APIs and JSONs&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,1,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/interacting-with-apis-to-import-data-from-the-web-2?ex=1&quot;]],[&quot;^2&quot;,[&quot;type&quot;,&quot;PureMultipleChoiceExercise&quot;,&quot;title&quot;,&quot;Pop quiz: What exactly is a JSON?&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,2,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/interacting-with-apis-to-import-data-from-the-web-2?ex=2&quot;]],[&quot;^2&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Loading and exploring a JSON&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,3,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/interacting-with-apis-to-import-data-from-the-web-2?ex=3&quot;]],[&quot;^2&quot;,[&quot;type&quot;,&quot;MultipleChoiceExercise&quot;,&quot;title&quot;,&quot;Pop quiz: Exploring your JSON&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,4,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/interacting-with-apis-to-import-data-from-the-web-2?ex=4&quot;]],[&quot;^2&quot;,[&quot;type&quot;,&quot;VideoExercise&quot;,&quot;title&quot;,&quot;APIs and interacting with the world wide web&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,5,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/interacting-with-apis-to-import-data-from-the-web-2?ex=5&quot;]],[&quot;^2&quot;,[&quot;type&quot;,&quot;PureMultipleChoiceExercise&quot;,&quot;title&quot;,&quot;Pop quiz: What&#39;s an API?&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,6,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/interacting-with-apis-to-import-data-from-the-web-2?ex=6&quot;]],[&quot;^2&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;API requests&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,7,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/interacting-with-apis-to-import-data-from-the-web-2?ex=7&quot;]],[&quot;^2&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;JSON\\xe2\\x80\\x93from the web to Python&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,8,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/interacting-with-apis-to-import-data-from-the-web-2?ex=8&quot;]],[&quot;^2&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Checking out the Wikipedia API&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,9,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/interacting-with-apis-to-import-data-from-the-web-2?ex=9&quot;]]]],&quot;description&quot;,&quot;In this chapter, you will gain a deeper understanding of how to import data from the web. You will learn the basics of extracting data from APIs, gain insight on the importance of APIs, and practice extracting data by diving into the OMDB and Library of Congress APIs.&quot;,&quot;badge_completed_url&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing.png&quot;]],[&quot;^2&quot;,[&quot;badge_uncompleted_url&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing_unc.png&quot;,&quot;number&quot;,3,&quot;number_of_videos&quot;,2,&quot;slug&quot;,&quot;diving-deep-into-the-twitter-api&quot;,&quot;last_updated_on&quot;,&quot;09/12/2021&quot;,&quot;title_meta&quot;,null,&quot;nb_exercises&quot;,8,&quot;free_preview&quot;,null,&quot;slides_link&quot;,&quot;https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/slides/chapter3.pdf&quot;,&quot;title&quot;,&quot;Diving  deep into the Twitter API&quot;,&quot;xp&quot;,700,&quot;id&quot;,4140,&quot;exercises&quot;,[&quot;^7&quot;,[[&quot;^2&quot;,[&quot;type&quot;,&quot;VideoExercise&quot;,&quot;title&quot;,&quot;The Twitter API and Authentication&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,1,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/diving-deep-into-the-twitter-api?ex=1&quot;]],[&quot;^2&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;API Authentication&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,2,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/diving-deep-into-the-twitter-api?ex=2&quot;]],[&quot;^2&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Streaming tweets&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,3,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/diving-deep-into-the-twitter-api?ex=3&quot;]],[&quot;^2&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Load and explore your Twitter data&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,4,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/diving-deep-into-the-twitter-api?ex=4&quot;]],[&quot;^2&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Twitter data to DataFrame&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,5,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/diving-deep-into-the-twitter-api?ex=5&quot;]],[&quot;^2&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;A little bit of Twitter text analysis&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,6,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/diving-deep-into-the-twitter-api?ex=6&quot;]],[&quot;^2&quot;,[&quot;type&quot;,&quot;NormalExercise&quot;,&quot;title&quot;,&quot;Plotting your Twitter data&quot;,&quot;aggregate_xp&quot;,100,&quot;number&quot;,7,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/diving-deep-into-the-twitter-api?ex=7&quot;]],[&quot;^2&quot;,[&quot;type&quot;,&quot;VideoExercise&quot;,&quot;title&quot;,&quot;Final Thoughts&quot;,&quot;aggregate_xp&quot;,50,&quot;number&quot;,8,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/diving-deep-into-the-twitter-api?ex=8&quot;]]]],&quot;description&quot;,&quot;In this chapter, you will consolidate your knowledge of interacting with APIs in a deep dive into the Twitter streaming API. You&#39;ll learn how to stream real-time Twitter data, and how to analyze and visualize it.&quot;,&quot;badge_completed_url&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing.png&quot;]]]],&quot;time_needed&quot;,null,&quot;author_image&quot;,&quot;https://assets.datacamp.com/production/course_1606/author_images/author_image_course_1606_20200310-1-lgdj4c?1583853939&quot;,&quot;tracks&quot;,[&quot;^7&quot;,[[&quot;^2&quot;,[&quot;path&quot;,&quot;/tracks/data-analyst-with-python&quot;,&quot;title_with_subtitle&quot;,&quot;Data Analyst  with Python&quot;]],[&quot;^2&quot;,[&quot;path&quot;,&quot;/tracks/data-scientist-with-python&quot;,&quot;title_with_subtitle&quot;,&quot;Data Scientist  with Python&quot;]],[&quot;^2&quot;,[&quot;path&quot;,&quot;/tracks/importing-cleaning-data-with-python&quot;,&quot;title_with_subtitle&quot;,&quot;Importing &amp; Cleaning Data  with Python&quot;]]]],&quot;runtime_config&quot;,null,&quot;lti_only&quot;,false,&quot;image_url&quot;,&quot;https://assets.datacamp.com/production/course_1606/shields/thumb/shield_image_course_1606_20200310-1-17hkmhz?1583853940&quot;,&quot;topic_id&quot;,8,&quot;slug&quot;,&quot;intermediate-importing-data-in-python&quot;,&quot;last_updated_on&quot;,&quot;07/01/2022&quot;,&quot;paid&quot;,true,&quot;collaborators&quot;,[&quot;^7&quot;,[[&quot;^2&quot;,[&quot;avatar_url&quot;,&quot;https://assets.datacamp.com/users/avatars/000/382/294/square/francis-photo.jpg?1471980001&quot;,&quot;full_name&quot;,&quot;Francisco Castro&quot;]]]],&quot;time_needed_in_hours&quot;,2,&quot;technology_id&quot;,2,&quot;university&quot;,null,&quot;archived_at&quot;,null,&quot;state&quot;,&quot;live&quot;,&quot;author_bio&quot;,null,&quot;should_cache&quot;,true,&quot;sharing_links&quot;,[&quot;^2&quot;,[&quot;twitter&quot;,&quot;http://bit.ly/1eWTMJh&quot;,&quot;facebook&quot;,&quot;http://bit.ly/1iS42Do&quot;]],&quot;instructors&quot;,[&quot;^7&quot;,[[&quot;^2&quot;,[&quot;id&quot;,301837,&quot;marketing_biography&quot;,&quot;Data Scientist at DataCamp&quot;,&quot;biography&quot;,&quot;Hugo is a data scientist, educator, writer and podcaster at DataCamp. His main interests are promoting data &amp; AI literacy, helping to spread data skills through organizations and society and doing amateur stand up comedy in NYC. If you want to know what he likes to talk about, definitely check out DataFramed, the DataCamp podcast, which he hosts and produces: https://www.datacamp.com/community/podcast&quot;,&quot;avatar_url&quot;,&quot;https://assets.datacamp.com/users/avatars/000/301/837/square/hugoaboutpic.jpg?1493154678&quot;,&quot;full_name&quot;,&quot;Hugo Bowne-Anderson&quot;,&quot;instructor_path&quot;,&quot;/instructors/hugobowne&quot;]]]],&quot;seo_title&quot;,&quot;Intermediate Importing Data in Python&quot;,&quot;title&quot;,&quot;Intermediate Importing Data in Python&quot;,&quot;xp&quot;,2400,&quot;image_thumbnail_url&quot;,&quot;https://assets.datacamp.com/production/course_1606/shields/thumb_home/shield_image_course_1606_20200310-1-17hkmhz?1583853940&quot;,&quot;short_description&quot;,&quot;Improve your Python data importing skills and learn to work with web and API data.&quot;,&quot;nb_of_subscriptions&quot;,127374,&quot;seo_description&quot;,&quot;Learn how to import data into Python from sources like the web and by pulling data from APIs, such as the Twitter streaming API to stream real-time tweets.&quot;,&quot;type&quot;,&quot;datacamp&quot;,&quot;link&quot;,&quot;https://www.datacamp.com/courses/intermediate-importing-data-in-python&quot;,&quot;id&quot;,1606,&quot;datasets&quot;,[&quot;^7&quot;,[[&quot;^2&quot;,[&quot;asset_url&quot;,&quot;https://assets.datacamp.com/production/repositories/488/datasets/b422ace2fceada7b569e0ba3e8d833fddc684c4d/latitude.xls&quot;,&quot;name&quot;,&quot;Latitudes (XLS)&quot;]],[&quot;^2&quot;,[&quot;asset_url&quot;,&quot;https://assets.datacamp.com/production/repositories/488/datasets/3ef452f83a91556ea4284624b969392c0506fb33/tweets3.txt&quot;,&quot;name&quot;,&quot;Tweets&quot;]],[&quot;^2&quot;,[&quot;asset_url&quot;,&quot;https://assets.datacamp.com/production/repositories/488/datasets/013936d2700e2d00207ec42100d448c23692eb6f/winequality-red.csv&quot;,&quot;name&quot;,&quot;Red wine quality&quot;]]]],&quot;description&quot;,&quot;As a data scientist, you will need to clean data, wrangle and munge it, visualize it, build predictive models and interpret these models. Before you can do so, however, you will need to know how to get data into Python. In the prequel to this course, you learned many ways to import data into Python: from flat files such as .txt and .csv; from files native to other software such as Excel spreadsheets, Stata, SAS, and MATLAB files; and from relational databases such as SQLite and PostgreSQL. In this course, you&#39;ll extend this knowledge base by learning to import data from the web and by pulling data from Application Programming Interfaces\\xe2\\x80\\x94 APIs\\xe2\\x80\\x94such as the Twitter streaming API, which allows us to stream real-time tweets.&quot;,&quot;prerequisites&quot;,[&quot;^7&quot;,[[&quot;^2&quot;,[&quot;path&quot;,&quot;/courses/introduction-to-importing-data-in-python&quot;,&quot;title&quot;,&quot;Introduction to Importing Data in Python&quot;]]]],&quot;original_image_url&quot;,&quot;https://assets.datacamp.com/production/course_1606/shields/original/shield_image_course_1606_20200310-1-17hkmhz?1583853940&quot;,&quot;programming_language&quot;,&quot;python&quot;,&quot;external_slug&quot;,&quot;intermediate-importing-data-in-python&quot;]],&quot;exercises&quot;,[&quot;^2&quot;,[&quot;current&quot;,1,&quot;all&quot;,[&quot;^7&quot;,[[&quot;^2&quot;,[&quot;sample_code&quot;,&quot;&quot;,&quot;sct&quot;,&quot;&quot;,&quot;aspect_ratio&quot;,56.25,&quot;instructions&quot;,null,&quot;externalId&quot;,990668,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,null,&quot;possible_answers&quot;,[&quot;^7&quot;,[]],&quot;runtime_config&quot;,null,&quot;number&quot;,1,&quot;video_hls&quot;,null,&quot;randomNumber&quot;,0.7355159797845514,&quot;chapter_id&quot;,4135,&quot;assignment&quot;,null,&quot;feedbacks&quot;,[&quot;^7&quot;,[]],&quot;attachments&quot;,null,&quot;version&quot;,&quot;v0&quot;,&quot;title&quot;,&quot;Importing flat files from the web&quot;,&quot;xp&quot;,50,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;&quot;,&quot;solution&quot;,&quot;&quot;,&quot;type&quot;,&quot;VideoExercise&quot;,&quot;id&quot;,990668,&quot;projector_key&quot;,&quot;course_1606_59604c018a6e132016cd26144a12fee0&quot;,&quot;video_link&quot;,null,&quot;key&quot;,&quot;e36457c7ed&quot;,&quot;course_id&quot;,1606]],[&quot;^2&quot;,[&quot;sample_code&quot;,&quot;# Import package\\\\\\\\nfrom ____ import ____\\\\\\\\n\\\\\\\\n# Import pandas\\\\\\\\nimport pandas as pd\\\\\\\\n\\\\\\\\n# Assign url of file: url\\\\\\\\n\\\\\\\\n\\\\\\\\n# Save file locally\\\\\\\\n\\\\\\\\n\\\\\\\\n# Read file into a DataFrame and print its head\\\\\\\\ndf = pd.read_csv(&#39;winequality-red.csv&#39;, sep=&#39;;&#39;)\\\\\\\\nprint(df.head())&quot;,&quot;sct&quot;,&quot;Ex().has_import(\\\\\\\\&quot;urllib.request.urlretrieve\\\\\\\\&quot;)\\\\\\\\nEx().has_import(\\\\\\\\&quot;pandas\\\\\\\\&quot;)\\\\\\\\nEx().check_object(\\\\\\\\&quot;url\\\\\\\\&quot;).has_equal_value()\\\\\\\\nEx().check_function(\\\\\\\\&quot;urllib.request.urlretrieve\\\\\\\\&quot;).multi(\\\\\\\\n  check_args(0).has_equal_value(),\\\\\\\\n  check_args(1).has_equal_value()\\\\\\\\n)\\\\\\\\nEx().check_correct(\\\\\\\\n  check_object(\\\\\\\\&quot;df\\\\\\\\&quot;).has_equal_value(),\\\\\\\\n  check_function(\\\\\\\\&quot;pandas.read_csv\\\\\\\\&quot;).multi(\\\\\\\\n    check_args(0).has_equal_value(),\\\\\\\\n    check_args(1).has_equal_value()\\\\\\\\n  )\\\\\\\\n)\\\\\\\\nEx().has_printout(0)\\\\\\\\nsuccess_msg(\\\\\\\\&quot;Awesome!\\\\\\\\&quot;)\\\\\\\\n&quot;,&quot;instructions&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Import the function &lt;code&gt;urlretrieve&lt;/code&gt; from the subpackage &lt;code&gt;urllib.request&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Assign the URL of the file to the variable &lt;code&gt;url&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Use the function &lt;code&gt;urlretrieve()&lt;/code&gt; to save the file locally as &lt;code&gt;&#39;winequality-red.csv&#39;&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Execute the remaining code to load &lt;code&gt;&#39;winequality-red.csv&#39;&lt;/code&gt; in a pandas DataFrame and to print its head to the shell.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;externalId&quot;,42707,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;To import a function &lt;code&gt;y&lt;/code&gt; from a subpackage &lt;code&gt;x&lt;/code&gt;, execute &lt;code&gt;from x import y&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;This one&#39;s a long URL. Make sure you typed it in correctly!&lt;/li&gt;\\\\\\\\n&lt;li&gt;Pass the &lt;em&gt;url&lt;/em&gt; to import (in the &lt;code&gt;url&lt;/code&gt; object you defined) as the first argument and the &lt;em&gt;filename&lt;/em&gt; for saving the file locally as the second argument to &lt;code&gt;urlretrieve()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You don&#39;t have to change the code for loading &lt;code&gt;&#39;winequality-red.csv&#39;&lt;/code&gt; and printing its head.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;possible_answers&quot;,[&quot;^7&quot;,[]],&quot;number&quot;,2,&quot;user&quot;,[&quot;^2&quot;,[&quot;isHintShown&quot;,false,&quot;editorTabs&quot;,[&quot;^2&quot;,[&quot;files/script.py&quot;,[&quot;^2&quot;,[&quot;title&quot;,&quot;script.py&quot;,&quot;isSolution&quot;,false,&quot;props&quot;,[&quot;^2&quot;,[&quot;active&quot;,true,&quot;isClosable&quot;,false,&quot;code&quot;,null,&quot;extra&quot;,[&quot;^2&quot;,[]]]]]]]],&quot;outputMarkdownTabs&quot;,[&quot;^2&quot;,[]],&quot;markdown&quot;,[&quot;^2&quot;,[&quot;titles&quot;,[&quot;^7&quot;,[&quot;Knit PDF&quot;,&quot;Knit HTML&quot;]],&quot;activeTitle&quot;,&quot;Knit HTML&quot;]],&quot;currentXp&quot;,100,&quot;graphicalTabs&quot;,[&quot;^2&quot;,[&quot;plot&quot;,[&quot;^2&quot;,[&quot;extraClass&quot;,&quot;animation--flash&quot;,&quot;title&quot;,&quot;Plots&quot;,&quot;props&quot;,[&quot;^2&quot;,[&quot;sources&quot;,[&quot;^7&quot;,[]],&quot;currentIndex&quot;,0]],&quot;dimension&quot;,[&quot;^2&quot;,[&quot;isRealSize&quot;,false,&quot;width&quot;,1,&quot;height&quot;,1]]]],&quot;html&quot;,[&quot;^2&quot;,[&quot;extraClass&quot;,&quot;animation--flash&quot;,&quot;title&quot;,&quot;HTML Viewer&quot;,&quot;props&quot;,[&quot;^2&quot;,[&quot;sources&quot;,[&quot;^7&quot;,[]],&quot;currentIndex&quot;,0]]]]]],&quot;feedbackMessages&quot;,[&quot;^7&quot;,[]],&quot;lastSubmittedCode&quot;,null,&quot;ltiStatus&quot;,[&quot;^2&quot;,[]],&quot;lastSubmitActiveEditorTab&quot;,null,&quot;consoleSqlTabs&quot;,[&quot;^2&quot;,[&quot;query_result&quot;,[&quot;^2&quot;,[&quot;extraClass&quot;,&quot;&quot;,&quot;title&quot;,&quot;query result&quot;,&quot;props&quot;,[&quot;^2&quot;,[&quot;active&quot;,true,&quot;isNotView&quot;,true,&quot;message&quot;,&quot;No query executed yet...&quot;]]]]]],&quot;consoleTabs&quot;,[&quot;^2&quot;,[&quot;console&quot;,[&quot;^2&quot;,[&quot;title&quot;,&quot;IPython Shell&quot;,&quot;props&quot;,[&quot;^2&quot;,[&quot;active&quot;,true]],&quot;dimension&quot;,[&quot;^2&quot;,[&quot;cols&quot;,400]]]],&quot;slides&quot;,[&quot;^2&quot;,[&quot;title&quot;,&quot;Slides&quot;,&quot;props&quot;,[&quot;^2&quot;,[&quot;active&quot;,false]]]]]],&quot;inputMarkdownTabs&quot;,[&quot;^2&quot;,[]],&quot;consoleObjectViewTabs&quot;,[&quot;^2&quot;,[]]]],&quot;randomNumber&quot;,0.7979484705551529,&quot;assignment&quot;,&quot;&lt;p&gt;You are about to import your first file from the web! The flat file you will import will be &lt;code&gt;&#39;winequality-red.csv&#39;&lt;/code&gt; from the University of California, Irvine&#39;s &lt;a href=\\\\\\\\&quot;http://archive.ics.uci.edu/ml/index.html\\\\\\\\&quot;&gt;Machine Learning repository&lt;/a&gt;. The flat file contains tabular data of physiochemical properties of red wine, such as pH, alcohol content and citric acid content, along with wine quality rating.&lt;/p&gt;\\\\\\\\n&lt;p&gt;The URL of the file is&lt;/p&gt;\\\\\\\\n&lt;pre&gt;&lt;code&gt;&#39;https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/datasets/winequality-red.csv&#39;\\\\\\\\n&lt;/code&gt;&lt;/pre&gt;\\\\\\\\n&lt;p&gt;After you import it, you&#39;ll check your working directory to confirm that it is there and then you&#39;ll load it into a &lt;code&gt;pandas&lt;/code&gt; DataFrame.&lt;/p&gt;&quot;,&quot;feedbacks&quot;,[&quot;^7&quot;,[]],&quot;attachments&quot;,null,&quot;title&quot;,&quot;Importing flat files from the web: your turn!&quot;,&quot;xp&quot;,100,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;&quot;,&quot;solution&quot;,&quot;# Import package\\\\\\\\nfrom urllib.request import urlretrieve\\\\\\\\n\\\\\\\\n# Import pandas\\\\\\\\nimport pandas as pd\\\\\\\\n\\\\\\\\n# Assign url of file: url\\\\\\\\nurl = &#39;https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/datasets/winequality-red.csv&#39;\\\\\\\\n\\\\\\\\n# Save file locally\\\\\\\\nurlretrieve(url, &#39;winequality-red.csv&#39;)\\\\\\\\n\\\\\\\\n# Read file into a DataFrame and print its head\\\\\\\\ndf = pd.read_csv(&#39;winequality-red.csv&#39;, sep=&#39;;&#39;)\\\\\\\\nprint(df.head())&quot;,&quot;type&quot;,&quot;NormalExercise&quot;,&quot;id&quot;,42707]],[&quot;^2&quot;,[&quot;sample_code&quot;,&quot;# Import packages\\\\\\\\nimport matplotlib.pyplot as plt\\\\\\\\nimport pandas as pd\\\\\\\\n\\\\\\\\n# Assign url of file: url\\\\\\\\n\\\\\\\\n\\\\\\\\n# Read file into a DataFrame: df\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print the head of the DataFrame\\\\\\\\nprint(____)\\\\\\\\n\\\\\\\\n# Plot first column of df\\\\\\\\npd.DataFrame.hist(df.ix[:, 0:1])\\\\\\\\nplt.xlabel(&#39;fixed acidity (g(tartaric acid)/dm$^3$)&#39;)\\\\\\\\nplt.ylabel(&#39;count&#39;)\\\\\\\\nplt.show()\\\\\\\\n&quot;,&quot;sct&quot;,&quot;Ex().has_import(\\\\\\\\&quot;matplotlib.pyplot\\\\\\\\&quot;)\\\\\\\\nEx().has_import(\\\\\\\\&quot;pandas\\\\\\\\&quot;)\\\\\\\\nEx().check_object(\\\\\\\\&quot;url\\\\\\\\&quot;).has_equal_value()\\\\\\\\nEx().check_correct(\\\\\\\\n  check_object(\\\\\\\\&quot;df\\\\\\\\&quot;).has_equal_value(),\\\\\\\\n  check_function(\\\\\\\\&quot;pandas.read_csv\\\\\\\\&quot;).multi(\\\\\\\\n    check_args(0).has_equal_value(),\\\\\\\\n    check_args(1).has_equal_value()\\\\\\\\n  )\\\\\\\\n)\\\\\\\\nEx().has_printout(0)\\\\\\\\nEx().check_function(\\\\\\\\&quot;pandas.DataFrame.hist\\\\\\\\&quot;).check_args(0).has_equal_value()\\\\\\\\nEx().check_function(\\\\\\\\&quot;matplotlib.pyplot.show\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\nsuccess_msg(\\\\\\\\&quot;Awesome!\\\\\\\\&quot;)\\\\\\\\n&quot;,&quot;instructions&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Assign the URL of the file to the variable &lt;code&gt;url&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Read file into a DataFrame &lt;code&gt;df&lt;/code&gt; using &lt;code&gt;pd.read_csv()&lt;/code&gt;, recalling that the separator in the file is &lt;code&gt;&#39;;&#39;&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Print the head of the DataFrame &lt;code&gt;df&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Execute the rest of the code to plot histogram of the first feature in the DataFrame &lt;code&gt;df&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;externalId&quot;,42708,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Make sure you typed the URL correctly!&lt;/li&gt;\\\\\\\\n&lt;li&gt;Pass the &lt;em&gt;url&lt;/em&gt; (the &lt;code&gt;url&lt;/code&gt; object you defined) as the first argument and the &lt;em&gt;separator&lt;/em&gt; as the second argument to &lt;code&gt;pd.read_csv()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;The &lt;em&gt;head&lt;/em&gt; of a DataFrame can be accessed by using &lt;code&gt;head()&lt;/code&gt; on the DataFrame.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You don&#39;t have to change any of the code for plotting the histograms.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;possible_answers&quot;,[&quot;^7&quot;,[]],&quot;number&quot;,3,&quot;randomNumber&quot;,0.3820707880727505,&quot;assignment&quot;,&quot;&lt;p&gt;You have just imported a file from the web, saved it locally and loaded it into a DataFrame. If you just wanted to load a file from the web into a DataFrame without first saving it locally, you can do that easily using &lt;code&gt;pandas&lt;/code&gt;. In particular, you can use the function &lt;code&gt;pd.read_csv()&lt;/code&gt; with the URL as the first argument and the separator &lt;code&gt;sep&lt;/code&gt; as the second argument.&lt;/p&gt;\\\\\\\\n&lt;p&gt;The URL of the file, once again, is&lt;/p&gt;\\\\\\\\n&lt;pre&gt;&lt;code&gt;&#39;https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/datasets/winequality-red.csv&#39;\\\\\\\\n&lt;/code&gt;&lt;/pre&gt;&quot;,&quot;feedbacks&quot;,[&quot;^7&quot;,[]],&quot;attachments&quot;,null,&quot;title&quot;,&quot;Opening and reading flat files from the web&quot;,&quot;xp&quot;,100,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;&quot;,&quot;solution&quot;,&quot;# Import packages\\\\\\\\nimport matplotlib.pyplot as plt\\\\\\\\nimport pandas as pd\\\\\\\\n\\\\\\\\n# Assign url of file: url\\\\\\\\nurl = &#39;https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/datasets/winequality-red.csv&#39;\\\\\\\\n\\\\\\\\n# Read file into a DataFrame: df\\\\\\\\ndf = pd.read_csv(url, sep=&#39;;&#39;)\\\\\\\\n\\\\\\\\n# Print the head of the DataFrame\\\\\\\\nprint(df.head())\\\\\\\\n\\\\\\\\n# Plot first column of df\\\\\\\\npd.DataFrame.hist(df.ix[:, 0:1])\\\\\\\\nplt.xlabel(&#39;fixed acidity (g(tartaric acid)/dm$^3$)&#39;)\\\\\\\\nplt.ylabel(&#39;count&#39;)\\\\\\\\nplt.show()\\\\\\\\n&quot;,&quot;type&quot;,&quot;NormalExercise&quot;,&quot;id&quot;,42708]],[&quot;^2&quot;,[&quot;sample_code&quot;,&quot;# Import package\\\\\\\\nimport pandas as pd\\\\\\\\n\\\\\\\\n# Assign url of file: url\\\\\\\\n\\\\\\\\n\\\\\\\\n# Read in all sheets of Excel file: xls\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print the sheetnames to the shell\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print the head of the first sheet (using its name, NOT its index)\\\\\\\\n\\\\\\\\n&quot;,&quot;sct&quot;,&quot;Ex().has_import(&#39;pandas&#39;)\\\\\\\\nEx().check_correct(\\\\\\\\n    has_printout(0),\\\\\\\\n    multi(\\\\\\\\n        check_correct(\\\\\\\\n            check_object(&#39;xls&#39;).is_instance(dict),\\\\\\\\n            check_correct(\\\\\\\\n                check_function(&#39;pandas.read_excel&#39;).multi(\\\\\\\\n                    check_args(0).has_equal_value(),\\\\\\\\n                    check_args(&#39;sheet_name&#39;).has_equal_value()\\\\\\\\n                ),\\\\\\\\n                check_object(&#39;url&#39;).has_equal_value()\\\\\\\\n            )\\\\\\\\n        )\\\\\\\\n    )\\\\\\\\n)\\\\\\\\nEx().has_printout(1)\\\\\\\\nsuccess_msg(\\\\\\\\&quot;Awesome!\\\\\\\\&quot;)&quot;,&quot;instructions&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Assign the URL of the file to the variable &lt;code&gt;url&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Read the file in &lt;code&gt;url&lt;/code&gt; into a dictionary &lt;code&gt;xls&lt;/code&gt; using &lt;code&gt;pd.read_excel()&lt;/code&gt; recalling that, in order to import all sheets you need to pass &lt;code&gt;None&lt;/code&gt; to the argument &lt;code&gt;sheet_name&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Print the names of the sheets in the Excel spreadsheet; these will be the keys of the dictionary &lt;code&gt;xls&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Print the head of the first sheet &lt;em&gt;using the sheet name, not the index of the sheet&lt;/em&gt;! The sheet name is &lt;code&gt;&#39;1700&#39;&lt;/code&gt;&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;externalId&quot;,42709,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Make sure you typed in the URL correctly!&lt;/li&gt;\\\\\\\\n&lt;li&gt;Pass the &lt;em&gt;url&lt;/em&gt; (the &lt;code&gt;url&lt;/code&gt; object you defined) as the first argument and &lt;code&gt;sheet_name&lt;/code&gt; with its corresponding value as the second argument to &lt;code&gt;pd.read_excel()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;The &lt;em&gt;keys&lt;/em&gt; of a dictionary can be accessed by using &lt;code&gt;keys()&lt;/code&gt; on the dictionary.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You can access a sheet using the format: &lt;em&gt;dictionary&lt;/em&gt;&lt;strong&gt;[&lt;/strong&gt;&lt;em&gt;sheet name or index&lt;/em&gt;&lt;strong&gt;]&lt;/strong&gt;.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;possible_answers&quot;,[&quot;^7&quot;,[]],&quot;number&quot;,4,&quot;randomNumber&quot;,0.520461459043172,&quot;assignment&quot;,&quot;&lt;p&gt;Congrats! You&#39;ve just loaded a flat file from the web into a DataFrame without first saving it locally using the &lt;code&gt;pandas&lt;/code&gt; function &lt;code&gt;pd.read_csv()&lt;/code&gt;. This function is super cool because it has close relatives that allow you to load all types of files, not only flat ones. In this interactive exercise, you&#39;ll use &lt;code&gt;pd.read_excel()&lt;/code&gt; to import an Excel spreadsheet.&lt;/p&gt;\\\\\\\\n&lt;p&gt;The URL of the spreadsheet is&lt;/p&gt;\\\\\\\\n&lt;pre&gt;&lt;code&gt;&#39;http://s3.amazonaws.com/assets.datacamp.com/course/importing_data_into_r/latitude.xls&#39;\\\\\\\\n&lt;/code&gt;&lt;/pre&gt;\\\\\\\\n&lt;p&gt;Your job is to use &lt;code&gt;pd.read_excel()&lt;/code&gt; to read in all of its sheets, print the sheet names and then print the head of the first sheet &lt;em&gt;using its name, not its index&lt;/em&gt;.&lt;/p&gt;\\\\\\\\n&lt;p&gt;Note that the output of &lt;code&gt;pd.read_excel()&lt;/code&gt; is a Python dictionary with sheet names as keys and corresponding DataFrames as corresponding values.&lt;/p&gt;&quot;,&quot;feedbacks&quot;,[&quot;^7&quot;,[]],&quot;attachments&quot;,null,&quot;title&quot;,&quot;Importing non-flat files from the web&quot;,&quot;xp&quot;,100,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;&quot;,&quot;solution&quot;,&quot;# Import package\\\\\\\\nimport pandas as pd\\\\\\\\n\\\\\\\\n# Assign url of file: url\\\\\\\\nurl = &#39;http://s3.amazonaws.com/assets.datacamp.com/course/importing_data_into_r/latitude.xls&#39;\\\\\\\\n\\\\\\\\n# Read in all sheets of Excel file: xls\\\\\\\\nxls = pd.read_excel(url, sheet_name=None)\\\\\\\\n\\\\\\\\n# Print the sheetnames to the shell\\\\\\\\nprint(xls.keys())\\\\\\\\n\\\\\\\\n# Print the head of the first sheet (using its name, NOT its index)\\\\\\\\nprint(xls[&#39;1700&#39;].head())&quot;,&quot;type&quot;,&quot;NormalExercise&quot;,&quot;id&quot;,42709]],[&quot;^2&quot;,[&quot;sample_code&quot;,&quot;&quot;,&quot;sct&quot;,&quot;&quot;,&quot;aspect_ratio&quot;,56.25,&quot;instructions&quot;,null,&quot;externalId&quot;,990669,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,null,&quot;possible_answers&quot;,[&quot;^7&quot;,[]],&quot;runtime_config&quot;,null,&quot;number&quot;,5,&quot;video_hls&quot;,null,&quot;randomNumber&quot;,0.7252146440228739,&quot;chapter_id&quot;,4135,&quot;assignment&quot;,null,&quot;feedbacks&quot;,[&quot;^7&quot;,[]],&quot;attachments&quot;,null,&quot;version&quot;,&quot;v0&quot;,&quot;title&quot;,&quot;HTTP requests to import files from the web&quot;,&quot;xp&quot;,50,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;&quot;,&quot;solution&quot;,&quot;&quot;,&quot;type&quot;,&quot;VideoExercise&quot;,&quot;id&quot;,990669,&quot;projector_key&quot;,&quot;course_1606_9d15ae176be1800b996f7869a82b8087&quot;,&quot;video_link&quot;,null,&quot;key&quot;,&quot;e480d1fdcf&quot;,&quot;course_id&quot;,1606]],[&quot;^2&quot;,[&quot;sample_code&quot;,&quot;# Import packages\\\\\\\\n\\\\\\\\n\\\\\\\\n# Specify the url\\\\\\\\nurl = \\\\\\\\&quot;https://campus.datacamp.com/courses/1606/4135?ex=2\\\\\\\\&quot;\\\\\\\\n\\\\\\\\n# This packages the request: request\\\\\\\\n\\\\\\\\n\\\\\\\\n# Sends the request and catches the response: response\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print the datatype of response\\\\\\\\nprint(type(response))\\\\\\\\n\\\\\\\\n# Be polite and close the response!\\\\\\\\nresponse.close()\\\\\\\\n&quot;,&quot;sct&quot;,&quot;\\\\\\\\n# Test: import urlopen, Request\\\\\\\\nimport_msg = \\\\\\\\&quot;Did you correctly import the required packages?\\\\\\\\&quot;\\\\\\\\nEx().has_import(\\\\\\\\n    \\\\\\\\&quot;urllib.request.urlopen\\\\\\\\&quot;,\\\\\\\\n    not_imported_msg=import_msg\\\\\\\\n)\\\\\\\\nEx().has_import(\\\\\\\\n    \\\\\\\\&quot;urllib.request.Request\\\\\\\\&quot;,\\\\\\\\n    not_imported_msg=import_msg\\\\\\\\n)\\\\\\\\n\\\\\\\\n# Test: Predefined code\\\\\\\\npredef_msg = \\\\\\\\&quot;You don&#39;t have to change any of the predefined code.\\\\\\\\&quot;\\\\\\\\nEx().check_object(\\\\\\\\&quot;url\\\\\\\\&quot;, missing_msg=predef_msg).has_equal_value(incorrect_msg = predef_msg)\\\\\\\\n\\\\\\\\n# Test: call to Request() and &#39;request&#39; variable\\\\\\\\nEx().check_function(\\\\\\\\&quot;urllib.request.Request\\\\\\\\&quot;).check_args(0).has_equal_value()\\\\\\\\nEx().check_object(\\\\\\\\&quot;request\\\\\\\\&quot;)\\\\\\\\n  \\\\\\\\n# Test: call to urlopen() and &#39;response&#39; variable\\\\\\\\nEx().check_function(\\\\\\\\&quot;urllib.request.urlopen\\\\\\\\&quot;).check_args(0).has_equal_ast()\\\\\\\\nEx().check_object(\\\\\\\\&quot;response\\\\\\\\&quot;),\\\\\\\\n\\\\\\\\n# Test: Predefined code\\\\\\\\nEx().has_printout(0)\\\\\\\\nEx().check_function(\\\\\\\\&quot;response.close\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\nsuccess_msg(\\\\\\\\&quot;Awesome!\\\\\\\\&quot;)\\\\\\\\n&quot;,&quot;instructions&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Import the functions &lt;code&gt;urlopen&lt;/code&gt; and &lt;code&gt;Request&lt;/code&gt; from the subpackage &lt;code&gt;urllib.request&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Package the request to the url &lt;code&gt;\\\\\\\\&quot;https://campus.datacamp.com/courses/1606/4135?ex=2\\\\\\\\&quot;&lt;/code&gt; using the function &lt;code&gt;Request()&lt;/code&gt; and assign it to &lt;code&gt;request&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Send the request and catch the response in the variable &lt;code&gt;response&lt;/code&gt; with  the function &lt;code&gt;urlopen()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Run the rest of the code to see the datatype of &lt;code&gt;response&lt;/code&gt; and to close the connection!&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;externalId&quot;,42711,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;To import two functions in one line, import the first function as usual and add a comma &lt;code&gt;,&lt;/code&gt; followed by the second function.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Pass the &lt;em&gt;url&lt;/em&gt; (already in the &lt;code&gt;url&lt;/code&gt; object defined) as an argument to &lt;code&gt;Request()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Pass &lt;code&gt;request&lt;/code&gt; as an argument to &lt;code&gt;urlopen()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You don&#39;t have to modify the code for printing the datatype of &lt;code&gt;response&lt;/code&gt; and closing the connection.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;possible_answers&quot;,[&quot;^7&quot;,[]],&quot;number&quot;,6,&quot;randomNumber&quot;,0.1300355334847203,&quot;assignment&quot;,&quot;&lt;p&gt;Now that you know the basics behind HTTP GET requests, it&#39;s time to perform some of your own. In this interactive exercise, you will ping our very own DataCamp servers to perform a GET request to extract information from the first coding exercise of this course, &lt;code&gt;\\\\\\\\&quot;https://campus.datacamp.com/courses/1606/4135?ex=2\\\\\\\\&quot;&lt;/code&gt;.&lt;/p&gt;\\\\\\\\n&lt;p&gt;In the next exercise, you&#39;ll extract the HTML itself. Right now, however, you are going to package and send the request and then catch the response.&lt;/p&gt;&quot;,&quot;feedbacks&quot;,[&quot;^7&quot;,[]],&quot;attachments&quot;,null,&quot;title&quot;,&quot;Performing HTTP requests in Python using urllib&quot;,&quot;xp&quot;,100,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;&quot;,&quot;solution&quot;,&quot;# Import packages\\\\\\\\nfrom urllib.request import urlopen, Request\\\\\\\\n\\\\\\\\n# Specify the url\\\\\\\\nurl = \\\\\\\\&quot;https://campus.datacamp.com/courses/1606/4135?ex=2\\\\\\\\&quot;\\\\\\\\n\\\\\\\\n# This packages the request: request\\\\\\\\nrequest = Request(url)\\\\\\\\n\\\\\\\\n# Sends the request and catches the response: response\\\\\\\\nresponse = urlopen(request)\\\\\\\\n\\\\\\\\n# Print the datatype of response\\\\\\\\nprint(type(response))\\\\\\\\n\\\\\\\\n# Be polite and close the response!\\\\\\\\nresponse.close()\\\\\\\\n&quot;,&quot;type&quot;,&quot;NormalExercise&quot;,&quot;id&quot;,42711]],[&quot;^2&quot;,[&quot;sample_code&quot;,&quot;# Import packages\\\\\\\\nfrom urllib.request import urlopen, Request\\\\\\\\n\\\\\\\\n# Specify the url\\\\\\\\nurl = \\\\\\\\&quot;https://campus.datacamp.com/courses/1606/4135?ex=2\\\\\\\\&quot;\\\\\\\\n\\\\\\\\n# This packages the request\\\\\\\\nrequest = Request(url)\\\\\\\\n\\\\\\\\n# Sends the request and catches the response: response\\\\\\\\n\\\\\\\\n\\\\\\\\n# Extract the response: html\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print the html\\\\\\\\n\\\\\\\\n\\\\\\\\n# Be polite and close the response!\\\\\\\\nresponse.close()&quot;,&quot;sct&quot;,&quot;\\\\\\\\n# Test: Predefined code\\\\\\\\npredef_msg = \\\\\\\\&quot;You don&#39;t have to change any of the predefined code.\\\\\\\\&quot;\\\\\\\\nEx().has_import(\\\\\\\\n    \\\\\\\\&quot;urllib.request.urlopen\\\\\\\\&quot;,\\\\\\\\n    not_imported_msg=predef_msg\\\\\\\\n)\\\\\\\\n\\\\\\\\nEx().has_import(\\\\\\\\n    \\\\\\\\&quot;urllib.request.Request\\\\\\\\&quot;,\\\\\\\\n    not_imported_msg=predef_msg\\\\\\\\n)\\\\\\\\n\\\\\\\\nEx().check_object(\\\\\\\\&quot;url\\\\\\\\&quot;).has_equal_value()\\\\\\\\n\\\\\\\\n# Test: call to Request() and &#39;request&#39; variable\\\\\\\\nEx().check_function(\\\\\\\\&quot;urllib.request.Request\\\\\\\\&quot;).check_args(0).has_equal_value()\\\\\\\\nEx().check_object(\\\\\\\\&quot;request\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\n# Test: call to urlopen() and &#39;response&#39; variable\\\\\\\\nEx().check_function(\\\\\\\\&quot;urllib.request.urlopen\\\\\\\\&quot;).check_args(0).has_equal_ast()\\\\\\\\nEx().check_object(\\\\\\\\&quot;response\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\n# Test: call to urlopen() and &#39;response&#39; variable\\\\\\\\nEx().check_function(\\\\\\\\&quot;response.read\\\\\\\\&quot;)\\\\\\\\nEx().check_object(\\\\\\\\&quot;html\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\n# Test: call to print()\\\\\\\\nEx().check_function(&#39;print&#39;).check_args(0).has_equal_ast()\\\\\\\\n\\\\\\\\n# Test: Predefined code\\\\\\\\nEx().check_function(\\\\\\\\&quot;response.close\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\nsuccess_msg(\\\\\\\\&quot;Awesome!\\\\\\\\&quot;)\\\\\\\\n&quot;,&quot;instructions&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Send the request and catch the response in the variable &lt;code&gt;response&lt;/code&gt; with the function &lt;code&gt;urlopen()&lt;/code&gt;, as in the previous exercise.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Extract the response using the &lt;code&gt;read()&lt;/code&gt; method and store the result in the variable &lt;code&gt;html&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Print the string &lt;code&gt;html&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Hit submit to perform all of the above and to close the response: be tidy!&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;externalId&quot;,42712,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Pass &lt;code&gt;request&lt;/code&gt; as an argument to &lt;code&gt;urlopen()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Apply the method &lt;code&gt;read()&lt;/code&gt; to the response object &lt;code&gt;response&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Simply pass &lt;code&gt;html&lt;/code&gt; to the &lt;code&gt;print()&lt;/code&gt; function.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You don&#39;t have to modify the code for closing the response.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;possible_answers&quot;,[&quot;^7&quot;,[]],&quot;number&quot;,7,&quot;randomNumber&quot;,0.126809206142678,&quot;assignment&quot;,&quot;&lt;p&gt;You have just packaged and sent a GET request to &lt;code&gt;\\\\\\\\&quot;https://campus.datacamp.com/courses/1606/4135?ex=2\\\\\\\\&quot;&lt;/code&gt; and then caught the response. You saw that such a response is a &lt;code&gt;http.client.HTTPResponse&lt;/code&gt; object. The question remains: what can you do with this response?&lt;/p&gt;\\\\\\\\n&lt;p&gt;Well, as it came from an HTML page, you could &lt;em&gt;read&lt;/em&gt; it to extract the HTML and, in fact, such a &lt;code&gt;http.client.HTTPResponse&lt;/code&gt; object has an associated &lt;code&gt;read()&lt;/code&gt; method. In this exercise, you&#39;ll build on your previous great work to extract the response and print the HTML.&lt;/p&gt;&quot;,&quot;feedbacks&quot;,[&quot;^7&quot;,[]],&quot;attachments&quot;,null,&quot;title&quot;,&quot;Printing HTTP request results in Python using urllib&quot;,&quot;xp&quot;,100,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;&quot;,&quot;solution&quot;,&quot;# Import packages\\\\\\\\nfrom urllib.request import urlopen, Request\\\\\\\\n\\\\\\\\n# Specify the url\\\\\\\\nurl = \\\\\\\\&quot;https://campus.datacamp.com/courses/1606/4135?ex=2\\\\\\\\&quot;\\\\\\\\n\\\\\\\\n# This packages the request\\\\\\\\nrequest = Request(url)\\\\\\\\n\\\\\\\\n# Sends the request and catches the response: response\\\\\\\\nresponse = urlopen(request)\\\\\\\\n\\\\\\\\n# Extract the response: html\\\\\\\\nhtml = response.read()\\\\\\\\n\\\\\\\\n# Print the html\\\\\\\\nprint(html)\\\\\\\\n\\\\\\\\n# Be polite and close the response!\\\\\\\\nresponse.close()&quot;,&quot;type&quot;,&quot;NormalExercise&quot;,&quot;id&quot;,42712]],[&quot;^2&quot;,[&quot;sample_code&quot;,&quot;# Import package\\\\\\\\n\\\\\\\\n\\\\\\\\n# Specify the url: url\\\\\\\\n\\\\\\\\n\\\\\\\\n# Packages the request, send the request and catch the response: r\\\\\\\\n\\\\\\\\n\\\\\\\\n# Extract the response: text\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print the html\\\\\\\\nprint(text)&quot;,&quot;sct&quot;,&quot;\\\\\\\\n# Test: import requests\\\\\\\\nEx().has_import(\\\\\\\\&quot;requests\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\n# Test: &#39;url&#39; variable\\\\\\\\nEx().check_object(\\\\\\\\&quot;url\\\\\\\\&quot;).has_equal_value()\\\\\\\\n\\\\\\\\n# Test: call to requests.get() and &#39;r&#39; variable\\\\\\\\nEx().check_function(\\\\\\\\&quot;requests.get\\\\\\\\&quot;).check_args(0).has_equal_value()\\\\\\\\nEx().check_object(\\\\\\\\&quot;r\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\n# Test: &#39;text&#39; variable\\\\\\\\nEx().has_code(\\\\\\\\&quot;r.text\\\\\\\\&quot;, pattern = False, not_typed_msg=\\\\\\\\&quot;Have you used `r.text` to create `text`?\\\\\\\\&quot;)\\\\\\\\nEx().check_object(\\\\\\\\&quot;text\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\n# Test: Predefined code\\\\\\\\nEx().check_function(&#39;print&#39;).check_args(0).has_equal_ast()\\\\\\\\n\\\\\\\\nsuccess_msg(\\\\\\\\&quot;Awesome!\\\\\\\\&quot;)\\\\\\\\n&quot;,&quot;instructions&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Import the package &lt;code&gt;requests&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Assign the URL of interest to the variable &lt;code&gt;url&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Package the request to the URL, send the request and catch the response with a single function &lt;code&gt;requests.get()&lt;/code&gt;, assigning the response to the variable &lt;code&gt;r&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Use the &lt;code&gt;text&lt;/code&gt; attribute of the object &lt;code&gt;r&lt;/code&gt; to return the HTML of the webpage as a string; store the result in a variable &lt;code&gt;text&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Hit submit to print the HTML of the webpage.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;externalId&quot;,42713,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;To import a package &lt;code&gt;x&lt;/code&gt;, execute &lt;code&gt;import x&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Did you type in the URL correctly?&lt;/li&gt;\\\\\\\\n&lt;li&gt;Pass the &lt;em&gt;url&lt;/em&gt; (the &lt;code&gt;url&lt;/code&gt; object you defined) as an argument to &lt;code&gt;requests.get()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You can access the &lt;code&gt;text&lt;/code&gt; attribute of the object &lt;code&gt;r&lt;/code&gt; by executing &lt;code&gt;r.text&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You don&#39;t have to modify the code for printing the HTML of the webpage.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;possible_answers&quot;,[&quot;^7&quot;,[]],&quot;number&quot;,8,&quot;randomNumber&quot;,0.9953768252991049,&quot;assignment&quot;,&quot;&lt;p&gt;Now that you&#39;ve got your head and hands around making HTTP requests using the urllib package, you&#39;re going to figure out how to do the same using the higher-level requests library. You&#39;ll once again be pinging DataCamp servers for their &lt;code&gt;\\\\\\\\&quot;http://www.datacamp.com/teach/documentation\\\\\\\\&quot;&lt;/code&gt; page.&lt;/p&gt;\\\\\\\\n&lt;p&gt;Note that unlike in the previous exercises using urllib, you don&#39;t have to close the connection when using requests!&lt;/p&gt;&quot;,&quot;feedbacks&quot;,[&quot;^7&quot;,[]],&quot;attachments&quot;,null,&quot;title&quot;,&quot;Performing HTTP requests in Python using requests&quot;,&quot;xp&quot;,100,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;&quot;,&quot;solution&quot;,&quot;# Import package\\\\\\\\nimport requests\\\\\\\\n\\\\\\\\n# Specify the url: url\\\\\\\\nurl = \\\\\\\\&quot;http://www.datacamp.com/teach/documentation\\\\\\\\&quot;\\\\\\\\n\\\\\\\\n# Packages the request, send the request and catch the response: r\\\\\\\\nr = requests.get(url)\\\\\\\\n\\\\\\\\n# Extract the response: text\\\\\\\\ntext = r.text\\\\\\\\n\\\\\\\\n# Print the html\\\\\\\\nprint(text)&quot;,&quot;type&quot;,&quot;NormalExercise&quot;,&quot;id&quot;,42713]],[&quot;^2&quot;,[&quot;sample_code&quot;,&quot;&quot;,&quot;sct&quot;,&quot;&quot;,&quot;aspect_ratio&quot;,56.25,&quot;instructions&quot;,null,&quot;externalId&quot;,990670,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,null,&quot;possible_answers&quot;,[&quot;^7&quot;,[]],&quot;runtime_config&quot;,null,&quot;number&quot;,9,&quot;video_hls&quot;,null,&quot;randomNumber&quot;,0.57087711082384,&quot;chapter_id&quot;,4135,&quot;assignment&quot;,null,&quot;feedbacks&quot;,[&quot;^7&quot;,[]],&quot;attachments&quot;,null,&quot;version&quot;,&quot;v0&quot;,&quot;title&quot;,&quot;Scraping the web in Python&quot;,&quot;xp&quot;,50,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;&quot;,&quot;solution&quot;,&quot;&quot;,&quot;type&quot;,&quot;VideoExercise&quot;,&quot;id&quot;,990670,&quot;projector_key&quot;,&quot;course_1606_9d1f8a331d1200c7e1bdbfcaf3a7a491&quot;,&quot;video_link&quot;,null,&quot;key&quot;,&quot;da43858012&quot;,&quot;course_id&quot;,1606]],[&quot;^2&quot;,[&quot;sample_code&quot;,&quot;# Import packages\\\\\\\\nimport requests\\\\\\\\nfrom ____ import ____\\\\\\\\n\\\\\\\\n# Specify url: url\\\\\\\\n\\\\\\\\n\\\\\\\\n# Package the request, send the request and catch the response: r\\\\\\\\n\\\\\\\\n\\\\\\\\n# Extracts the response as html: html_doc\\\\\\\\n\\\\\\\\n\\\\\\\\n# Create a BeautifulSoup object from the HTML: soup\\\\\\\\n\\\\\\\\n\\\\\\\\n# Prettify the BeautifulSoup object: pretty_soup\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print the response\\\\\\\\nprint(pretty_soup)&quot;,&quot;sct&quot;,&quot;# Test: Predefined code\\\\\\\\npredef_msg = \\\\\\\\&quot;You don&#39;t have to change any of the predefined code.\\\\\\\\&quot;\\\\\\\\nEx().has_import(\\\\\\\\n    \\\\\\\\&quot;requests\\\\\\\\&quot;,\\\\\\\\n    not_imported_msg=predef_msg\\\\\\\\n)\\\\\\\\n\\\\\\\\n# Test: import BeautifulSoup\\\\\\\\nimport_msg = \\\\\\\\&quot;Did you correctly import the required packages?\\\\\\\\&quot;\\\\\\\\nEx().has_import(\\\\\\\\n    \\\\\\\\&quot;bs4.BeautifulSoup\\\\\\\\&quot;,\\\\\\\\n    not_imported_msg=import_msg\\\\\\\\n)\\\\\\\\n\\\\\\\\n# Test: &#39;url&#39; variable\\\\\\\\nEx().check_object(\\\\\\\\&quot;url\\\\\\\\&quot;).has_equal_value()\\\\\\\\n\\\\\\\\n# Test: call to requests.get() and &#39;r&#39; variable\\\\\\\\nEx().check_function(\\\\\\\\&quot;requests.get\\\\\\\\&quot;).check_args(0).has_equal_value()\\\\\\\\nEx().check_object(\\\\\\\\&quot;r\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\n\\\\\\\\n# Test: &#39;html_doc&#39; variable\\\\\\\\nEx().check_correct(\\\\\\\\n  check_object(\\\\\\\\&quot;html_doc\\\\\\\\&quot;).has_equal_value(),\\\\\\\\n  has_code(\\\\\\\\&quot;r.text\\\\\\\\&quot;, pattern = False, not_typed_msg=\\\\\\\\&quot;Have you used `r.text` to create `html_doc`?\\\\\\\\&quot;)\\\\\\\\n)\\\\\\\\n\\\\\\\\n# Test: call to BeautifulSoup() and &#39;soup&#39; variable\\\\\\\\nEx().check_correct(\\\\\\\\n  check_object(\\\\\\\\&quot;soup\\\\\\\\&quot;).has_equal_value(),\\\\\\\\n  check_function(\\\\\\\\&quot;bs4.BeautifulSoup\\\\\\\\&quot;).check_args(0).has_equal_value()\\\\\\\\n  )\\\\\\\\n\\\\\\\\n# Test: call to prettify() and &#39;pretty_soup&#39; variable\\\\\\\\nEx().check_correct(\\\\\\\\n  check_object(\\\\\\\\&quot;pretty_soup\\\\\\\\&quot;).has_equal_value(),\\\\\\\\n  check_function(\\\\\\\\&quot;soup.prettify\\\\\\\\&quot;)\\\\\\\\n  )\\\\\\\\n\\\\\\\\n# Test: Predefined code\\\\\\\\nEx().has_printout(0)\\\\\\\\n\\\\\\\\nsuccess_msg(\\\\\\\\&quot;Awesome!\\\\\\\\&quot;)\\\\\\\\n&quot;,&quot;instructions&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Import the function &lt;code&gt;BeautifulSoup&lt;/code&gt; from the package &lt;code&gt;bs4&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Assign the URL of interest to the variable &lt;code&gt;url&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Package the request to the URL, send the request and catch the response with a single function &lt;code&gt;requests.get()&lt;/code&gt;, assigning the response to the variable &lt;code&gt;r&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Use the &lt;code&gt;text&lt;/code&gt; attribute of the object &lt;code&gt;r&lt;/code&gt; to return the HTML of the webpage as a string; store the result in a variable &lt;code&gt;html_doc&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Create a BeautifulSoup object &lt;code&gt;soup&lt;/code&gt; from the resulting HTML using the function &lt;code&gt;BeautifulSoup()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Use the method &lt;code&gt;prettify()&lt;/code&gt; on &lt;code&gt;soup&lt;/code&gt; and assign the result to &lt;code&gt;pretty_soup&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Hit submit to print to prettified HTML to your shell!&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;externalId&quot;,42715,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;To import a function &lt;code&gt;y&lt;/code&gt; from a package &lt;code&gt;x&lt;/code&gt;, execute &lt;code&gt;from x import y&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Check the URL to make sure that you typed it in correctly.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Pass the &lt;em&gt;url&lt;/em&gt; (the &lt;code&gt;url&lt;/code&gt; object you defined) as an argument to &lt;code&gt;requests.get()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You can access the &lt;code&gt;text&lt;/code&gt; attribute of the object &lt;code&gt;r&lt;/code&gt; by executing &lt;code&gt;r.text&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Pass the extracted &lt;em&gt;HTML&lt;/em&gt; as an argument to &lt;code&gt;BeautifulSoup()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;To use the &lt;code&gt;prettify()&lt;/code&gt; method on the BeautifulSoup object &lt;code&gt;soup&lt;/code&gt;, execute &lt;code&gt;soup.prettify()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You don&#39;t have to modify the code to print the prettified HTML.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;possible_answers&quot;,[&quot;^7&quot;,[]],&quot;number&quot;,10,&quot;randomNumber&quot;,0.36228885515222475,&quot;assignment&quot;,&quot;&lt;p&gt;In this interactive exercise, you&#39;ll learn how to use the BeautifulSoup package to &lt;em&gt;parse&lt;/em&gt;, &lt;em&gt;prettify&lt;/em&gt; and &lt;em&gt;extract&lt;/em&gt; information from HTML. You&#39;ll scrape the data from the webpage of Guido van Rossum, Python&#39;s very own &lt;a href=\\\\\\\\&quot;https://en.wikipedia.org/wiki/Benevolent_dictator_for_life\\\\\\\\&quot;&gt;Benevolent Dictator for Life&lt;/a&gt;. In the following exercises, you&#39;ll prettify the HTML and then extract the text and the hyperlinks.&lt;/p&gt;\\\\\\\\n&lt;p&gt;The URL of interest is &lt;code&gt;url = &#39;https://www.python.org/~guido/&#39;&lt;/code&gt;.&lt;/p&gt;&quot;,&quot;feedbacks&quot;,[&quot;^7&quot;,[]],&quot;attachments&quot;,null,&quot;title&quot;,&quot;Parsing HTML with BeautifulSoup&quot;,&quot;xp&quot;,100,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;&quot;,&quot;solution&quot;,&quot;# Import packages\\\\\\\\nimport requests\\\\\\\\nfrom bs4 import BeautifulSoup\\\\\\\\n\\\\\\\\n# Specify url: url\\\\\\\\nurl = &#39;https://www.python.org/~guido/&#39;\\\\\\\\n\\\\\\\\n# Package the request, send the request and catch the response: r\\\\\\\\nr = requests.get(url)\\\\\\\\n\\\\\\\\n# Extracts the response as html: html_doc\\\\\\\\nhtml_doc = r.text\\\\\\\\n\\\\\\\\n# Create a BeautifulSoup object from the HTML: soup\\\\\\\\nsoup = BeautifulSoup(html_doc)\\\\\\\\n\\\\\\\\n# Prettify the BeautifulSoup object: pretty_soup\\\\\\\\npretty_soup = soup.prettify()\\\\\\\\n\\\\\\\\n# Print the response\\\\\\\\nprint(pretty_soup)&quot;,&quot;type&quot;,&quot;NormalExercise&quot;,&quot;id&quot;,42715]],[&quot;^2&quot;,[&quot;sample_code&quot;,&quot;# Import packages\\\\\\\\nimport requests\\\\\\\\nfrom bs4 import BeautifulSoup\\\\\\\\n\\\\\\\\n# Specify url: url\\\\\\\\nurl = &#39;https://www.python.org/~guido/&#39;\\\\\\\\n\\\\\\\\n# Package the request, send the request and catch the response: r\\\\\\\\nr = requests.get(url)\\\\\\\\n\\\\\\\\n# Extract the response as html: html_doc\\\\\\\\nhtml_doc = r.text\\\\\\\\n\\\\\\\\n# Create a BeautifulSoup object from the HTML: soup\\\\\\\\n\\\\\\\\n\\\\\\\\n# Get the title of Guido&#39;s webpage: guido_title\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print the title of Guido&#39;s webpage to the shell\\\\\\\\n\\\\\\\\n\\\\\\\\n# Get Guido&#39;s text: guido_text\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print Guido&#39;s text to the shell\\\\\\\\nprint(guido_text)&quot;,&quot;sct&quot;,&quot;# Test: Predefined code\\\\\\\\npredef_msg = \\\\\\\\&quot;You don&#39;t have to change any of the predefined code.\\\\\\\\&quot;\\\\\\\\nEx().has_import(\\\\\\\\n    \\\\\\\\&quot;requests\\\\\\\\&quot;,\\\\\\\\n    not_imported_msg=predef_msg\\\\\\\\n)\\\\\\\\n\\\\\\\\n# Test: import BeautifulSoup\\\\\\\\nEx().has_import(\\\\\\\\n    \\\\\\\\&quot;bs4.BeautifulSoup\\\\\\\\&quot;,\\\\\\\\n    not_imported_msg=predef_msg\\\\\\\\n)\\\\\\\\n\\\\\\\\n# Test: &#39;url&#39; variable\\\\\\\\nEx().check_object(\\\\\\\\&quot;url\\\\\\\\&quot;).has_equal_value()\\\\\\\\n\\\\\\\\n# Test: call to requests.get() and &#39;r&#39; variable\\\\\\\\nEx().check_function(\\\\\\\\&quot;requests.get\\\\\\\\&quot;).check_args(0).has_equal_value()\\\\\\\\nEx().check_object(\\\\\\\\&quot;r\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\n\\\\\\\\n# Test: &#39;html_doc&#39; variable\\\\\\\\nEx().check_correct(\\\\\\\\n  check_object(\\\\\\\\&quot;html_doc\\\\\\\\&quot;).has_equal_value(),\\\\\\\\n  has_code(\\\\\\\\&quot;r.text\\\\\\\\&quot;, pattern = False, not_typed_msg=\\\\\\\\&quot;Have you used `r.text` to create `html_doc`?\\\\\\\\&quot;)\\\\\\\\n)\\\\\\\\n\\\\\\\\n# Test: call to BeautifulSoup() and &#39;soup&#39; variable\\\\\\\\nEx().check_correct(\\\\\\\\n  check_object(\\\\\\\\&quot;soup\\\\\\\\&quot;).has_equal_value(),\\\\\\\\n  check_function(\\\\\\\\&quot;bs4.BeautifulSoup\\\\\\\\&quot;).check_args(0).has_equal_value()\\\\\\\\n  )\\\\\\\\n\\\\\\\\n# Test: &#39;guido_title&#39; variable\\\\\\\\nEx().check_correct(\\\\\\\\n  check_object(\\\\\\\\&quot;guido_title\\\\\\\\&quot;).has_equal_value(),\\\\\\\\n  has_code(\\\\\\\\&quot;soup.title\\\\\\\\&quot;, pattern = False, not_typed_msg=\\\\\\\\&quot;Have you used `soup.title` to create `guido_title`?\\\\\\\\&quot;)\\\\\\\\n)\\\\\\\\n\\\\\\\\n# Test: call to print()\\\\\\\\nEx().has_printout(0)\\\\\\\\n\\\\\\\\n# Test: call to soup.get_text() and &#39;guido_text&#39; variable\\\\\\\\nEx().check_correct(\\\\\\\\n  check_object(\\\\\\\\&quot;guido_text\\\\\\\\&quot;).has_equal_value(),\\\\\\\\n  check_function(\\\\\\\\&quot;soup.get_text\\\\\\\\&quot;)\\\\\\\\n  )\\\\\\\\n\\\\\\\\n# Test: Predefined code\\\\\\\\nEx().has_printout(1)\\\\\\\\n\\\\\\\\nsuccess_msg(\\\\\\\\&quot;Awesome!\\\\\\\\&quot;)\\\\\\\\n&quot;,&quot;instructions&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;In the sample code, the HTML response object &lt;code&gt;html_doc&lt;/code&gt; has already been created: your first task is to Soupify it using the function &lt;code&gt;BeautifulSoup()&lt;/code&gt; and to assign the resulting soup to the variable &lt;code&gt;soup&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Extract the title from the HTML soup &lt;code&gt;soup&lt;/code&gt; using the attribute &lt;code&gt;title&lt;/code&gt; and assign the result to &lt;code&gt;guido_title&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Print the title of Guido&#39;s webpage to the shell using the &lt;code&gt;print()&lt;/code&gt; function.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Extract the text from the HTML soup &lt;code&gt;soup&lt;/code&gt; using the method &lt;code&gt;get_text()&lt;/code&gt; and assign to &lt;code&gt;guido_text&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Hit submit to print the text from Guido&#39;s webpage to the shell.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;externalId&quot;,42716,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Pass the &lt;em&gt;HTML response object&lt;/em&gt; as an argument to &lt;code&gt;BeautifulSoup()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You can access the &lt;code&gt;title&lt;/code&gt; attribute of the object &lt;code&gt;soup&lt;/code&gt; by executing &lt;code&gt;soup.title&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;The object that contains the title of Guido&#39;s webpage is &lt;code&gt;guido_title&lt;/code&gt;; pass this as an argument to &lt;code&gt;print()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Use the method &lt;code&gt;get_text()&lt;/code&gt; on the HTML soup &lt;code&gt;soup&lt;/code&gt; by executing &lt;code&gt;soup.get_text()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You don&#39;t have to modify the code to print the text from Guido&#39;s webpage.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;possible_answers&quot;,[&quot;^7&quot;,[]],&quot;number&quot;,11,&quot;randomNumber&quot;,0.025243375882895602,&quot;assignment&quot;,&quot;&lt;p&gt;As promised, in the following exercises, you&#39;ll learn the basics of extracting information from HTML soup. In this exercise, you&#39;ll figure out how to extract the text from the BDFL&#39;s webpage, along with printing the webpage&#39;s title.&lt;/p&gt;&quot;,&quot;feedbacks&quot;,[&quot;^7&quot;,[]],&quot;attachments&quot;,null,&quot;title&quot;,&quot;Turning a webpage into data using BeautifulSoup: getting the text&quot;,&quot;xp&quot;,100,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;&quot;,&quot;solution&quot;,&quot;# Import packages\\\\\\\\nimport requests\\\\\\\\nfrom bs4 import BeautifulSoup\\\\\\\\n\\\\\\\\n# Specify url: url\\\\\\\\nurl = &#39;https://www.python.org/~guido/&#39;\\\\\\\\n\\\\\\\\n# Package the request, send the request and catch the response: r\\\\\\\\nr = requests.get(url)\\\\\\\\n\\\\\\\\n# Extract the response as html: html_doc\\\\\\\\nhtml_doc = r.text\\\\\\\\n\\\\\\\\n# Create a BeautifulSoup object from the HTML: soup\\\\\\\\nsoup = BeautifulSoup(html_doc)\\\\\\\\n\\\\\\\\n# Get the title of Guido&#39;s webpage: guido_title\\\\\\\\nguido_title = soup.title\\\\\\\\n\\\\\\\\n# Print the title of Guido&#39;s webpage to the shell\\\\\\\\nprint(guido_title)\\\\\\\\n\\\\\\\\n# Get Guido&#39;s text: guido_text\\\\\\\\nguido_text = soup.get_text()\\\\\\\\n\\\\\\\\n# Print Guido&#39;s text to the shell\\\\\\\\nprint(guido_text)&quot;,&quot;type&quot;,&quot;NormalExercise&quot;,&quot;id&quot;,42716]],[&quot;^2&quot;,[&quot;sample_code&quot;,&quot;# Import packages\\\\\\\\nimport requests\\\\\\\\nfrom bs4 import BeautifulSoup\\\\\\\\n\\\\\\\\n# Specify url\\\\\\\\nurl = &#39;https://www.python.org/~guido/&#39;\\\\\\\\n\\\\\\\\n# Package the request, send the request and catch the response: r\\\\\\\\nr = requests.get(url)\\\\\\\\n\\\\\\\\n# Extracts the response as html: html_doc\\\\\\\\nhtml_doc = r.text\\\\\\\\n\\\\\\\\n# create a BeautifulSoup object from the HTML: soup\\\\\\\\nsoup = BeautifulSoup(html_doc)\\\\\\\\n\\\\\\\\n# Print the title of Guido&#39;s webpage\\\\\\\\nprint(soup.title)\\\\\\\\n\\\\\\\\n# Find all &#39;a&#39; tags (which define hyperlinks): a_tags\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print the URLs to the shell\\\\\\\\nfor ____ in ____:\\\\\\\\n    ____&quot;,&quot;sct&quot;,&quot;predef_msg = \\\\\\\\&quot;You don&#39;t have to change any of the predefined code.\\\\\\\\&quot;\\\\\\\\nEx().has_import(\\\\\\\\&quot;requests\\\\\\\\&quot;)\\\\\\\\nEx().has_import(\\\\\\\\&quot;bs4.BeautifulSoup\\\\\\\\&quot;)\\\\\\\\nEx().check_object(\\\\\\\\&quot;url\\\\\\\\&quot;).has_equal_value(incorrect_msg = predef_msg)\\\\\\\\nEx().check_function(\\\\\\\\&quot;requests.get\\\\\\\\&quot;).check_args(0).has_equal_ast()\\\\\\\\nEx().check_object(\\\\\\\\&quot;html_doc\\\\\\\\&quot;).has_equal_value(incorrect_msg = predef_msg)\\\\\\\\nEx().check_object(\\\\\\\\&quot;soup\\\\\\\\&quot;).has_equal_value(incorrect_msg = predef_msg)\\\\\\\\nEx().has_printout(0)\\\\\\\\n\\\\\\\\nEx().check_correct(\\\\\\\\n    check_object(\\\\\\\\&quot;a_tags\\\\\\\\&quot;),\\\\\\\\n    check_function(\\\\\\\\&quot;soup.find_all\\\\\\\\&quot;).check_args(0).has_equal_value()\\\\\\\\n)\\\\\\\\nEx().check_for_loop().multi(\\\\\\\\n        check_iter().has_equal_value(incorrect_msg = \\\\\\\\&quot;You have to iterate over `a_tags`\\\\\\\\&quot;),\\\\\\\\n        check_body().set_context(&#39;&lt;a href=\\\\\\\\&quot;pics.html\\\\\\\\&quot;&gt;&lt;img border=\\\\\\\\&quot;0\\\\\\\\&quot; src=\\\\\\\\&quot;images/IMG_2192.jpg\\\\\\\\&quot;/&gt;&lt;/a&gt;&#39;).check_function(\\\\\\\\&quot;print\\\\\\\\&quot;).check_args(0).check_function(\\\\\\\\&quot;link.get\\\\\\\\&quot;).check_args(0).has_equal_value()\\\\\\\\n    )\\\\\\\\n\\\\\\\\nsuccess_msg(\\\\\\\\&quot;Awesome!\\\\\\\\&quot;)&quot;,&quot;instructions&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Use the method &lt;code&gt;find_all()&lt;/code&gt; to find all hyperlinks in &lt;code&gt;soup&lt;/code&gt;, remembering that hyperlinks are defined by the HTML tag &lt;code&gt;&amp;lt;a&amp;gt;&lt;/code&gt; but passed to &lt;code&gt;find_all()&lt;/code&gt; without angle brackets; store the result in the variable &lt;code&gt;a_tags&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;The variable &lt;code&gt;a_tags&lt;/code&gt; is a results set: your job now is to enumerate over it, using a &lt;code&gt;for&lt;/code&gt; loop and to print the actual URLs of the hyperlinks; to do this, for every element &lt;code&gt;link&lt;/code&gt; in &lt;code&gt;a_tags&lt;/code&gt;, you want to &lt;code&gt;print()&lt;/code&gt; &lt;code&gt;link.get(&#39;href&#39;)&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;externalId&quot;,42717,&quot;question&quot;,&quot;&quot;,&quot;hint&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Pass the &lt;em&gt;HTML tag&lt;/em&gt; to find (without the angle brackets &lt;code&gt;&amp;lt;&amp;gt;&lt;/code&gt;) as a string argument to &lt;code&gt;find_all()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Recall that the &lt;code&gt;for&lt;/code&gt; loop recipe is: &lt;code&gt;for&lt;/code&gt; &lt;em&gt;loop variable&lt;/em&gt; &lt;code&gt;in&lt;/code&gt; &lt;em&gt;results set&lt;/em&gt;&lt;code&gt;:&lt;/code&gt;. Don&#39;t forget to pass &lt;code&gt;link.get(&#39;href&#39;)&lt;/code&gt; as an argument to &lt;code&gt;print()&lt;/code&gt; inside the &lt;code&gt;for&lt;/code&gt; loop body.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;possible_answers&quot;,[&quot;^7&quot;,[]],&quot;number&quot;,12,&quot;randomNumber&quot;,0.6835808699100132,&quot;assignment&quot;,&quot;&lt;p&gt;In this exercise, you&#39;ll figure out how to extract the URLs of the hyperlinks from the BDFL&#39;s webpage. In the process, you&#39;ll become close friends with the soup method &lt;code&gt;find_all()&lt;/code&gt;.&lt;/p&gt;&quot;,&quot;feedbacks&quot;,[&quot;^7&quot;,[]],&quot;attachments&quot;,null,&quot;title&quot;,&quot;Turning a webpage into data using BeautifulSoup: getting the hyperlinks&quot;,&quot;xp&quot;,100,&quot;language&quot;,&quot;python&quot;,&quot;pre_exercise_code&quot;,&quot;&quot;,&quot;solution&quot;,&quot;# Import packages\\\\\\\\nimport requests\\\\\\\\nfrom bs4 import BeautifulSoup\\\\\\\\n\\\\\\\\n# Specify url\\\\\\\\nurl = &#39;https://www.python.org/~guido/&#39;\\\\\\\\n\\\\\\\\n# Package the request, send the request and catch the response: r\\\\\\\\nr = requests.get(url)\\\\\\\\n\\\\\\\\n# Extracts the response as html: html_doc\\\\\\\\nhtml_doc = r.text\\\\\\\\n\\\\\\\\n# create a BeautifulSoup object from the HTML: soup\\\\\\\\nsoup = BeautifulSoup(html_doc)\\\\\\\\n\\\\\\\\n# Print the title of Guido&#39;s webpage\\\\\\\\nprint(soup.title)\\\\\\\\n\\\\\\\\n# Find all &#39;a&#39; tags (which define hyperlinks): a_tags\\\\\\\\na_tags = soup.find_all(&#39;a&#39;)\\\\\\\\n\\\\\\\\n# Print the URLs to the shell\\\\\\\\nfor link in a_tags:\\\\\\\\n    print(link.get(&#39;href&#39;))&quot;,&quot;type&quot;,&quot;NormalExercise&quot;,&quot;id&quot;,42717]]]],&quot;canRateChapter&quot;,false,&quot;isChapterCompleted&quot;,false]],&quot;learningMode&quot;,&quot;course&quot;,&quot;location&quot;,[&quot;^2&quot;,[&quot;current&quot;,[&quot;^2&quot;,[&quot;pathname&quot;,&quot;/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1&quot;,&quot;query&quot;,[&quot;^2&quot;,[&quot;ex&quot;,&quot;2&quot;]]]],&quot;canonical&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=2&quot;,&quot;before&quot;,[&quot;^2&quot;,[&quot;pathname&quot;,&quot;/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1&quot;,&quot;query&quot;,[&quot;^2&quot;,[&quot;ex&quot;,&quot;2&quot;]]]]]],&quot;mobilePopup&quot;,[&quot;^2&quot;,[]],&quot;onboardingMilestones&quot;,[&quot;^ &quot;,&quot;isStarted&quot;,false,&quot;isActive&quot;,true,&quot;step&quot;,0],&quot;preFetchedData&quot;,[&quot;^0&quot;,[&quot;^ &quot;,&quot;n&quot;,&quot;PreFetchedDataStateRecord&quot;,&quot;v&quot;,[&quot;^ &quot;,&quot;^9&quot;,[&quot;^0&quot;,[&quot;^ &quot;,&quot;n&quot;,&quot;PreFetchedRequestRecord&quot;,&quot;v&quot;,[&quot;^ &quot;,&quot;status&quot;,&quot;SUCCESS&quot;,&quot;data&quot;,[&quot;^ &quot;,&quot;id&quot;,1606,&quot;title&quot;,&quot;Intermediate Importing Data in Python&quot;,&quot;description&quot;,&quot;As a data scientist, you will need to clean data, wrangle and munge it, visualize it, build predictive models and interpret these models. Before you can do so, however, you will need to know how to get data into Python. In the prequel to this course, you learned many ways to import data into Python: from flat files such as .txt and .csv; from files native to other software such as Excel spreadsheets, Stata, SAS, and MATLAB files; and from relational databases such as SQLite and PostgreSQL. In this course, you&#39;ll extend this knowledge base by learning to import data from the web and by pulling data from Application Programming Interfaces\\xe2\\x80\\x94 APIs\\xe2\\x80\\x94such as the Twitter streaming API, which allows us to stream real-time tweets.&quot;,&quot;short_description&quot;,&quot;Improve your Python data importing skills and learn to work with web and API data.&quot;,&quot;author_field&quot;,null,&quot;author_bio&quot;,null,&quot;author_image&quot;,&quot;https://assets.datacamp.com/production/course_1606/author_images/author_image_course_1606_20200310-1-lgdj4c?1583853939&quot;,&quot;nb_of_subscriptions&quot;,127374,&quot;slug&quot;,&quot;intermediate-importing-data-in-python&quot;,&quot;image_url&quot;,&quot;https://assets.datacamp.com/production/course_1606/shields/thumb/shield_image_course_1606_20200310-1-17hkmhz?1583853940&quot;,&quot;image_thumbnail_url&quot;,&quot;https://assets.datacamp.com/production/course_1606/shields/thumb_home/shield_image_course_1606_20200310-1-17hkmhz?1583853940&quot;,&quot;last_updated_on&quot;,&quot;07/01/2022&quot;,&quot;link&quot;,&quot;https://www.datacamp.com/courses/intermediate-importing-data-in-python&quot;,&quot;should_cache&quot;,true,&quot;type&quot;,&quot;datacamp&quot;,&quot;difficulty_level&quot;,1,&quot;state&quot;,&quot;live&quot;,&quot;university&quot;,null,&quot;sharing_links&quot;,[&quot;^ &quot;,&quot;twitter&quot;,&quot;http://bit.ly/1eWTMJh&quot;,&quot;facebook&quot;,&quot;http://bit.ly/1iS42Do&quot;],&quot;marketing_video&quot;,&quot;&quot;,&quot;programming_language&quot;,&quot;python&quot;,&quot;paid&quot;,true,&quot;time_needed&quot;,null,&quot;xp&quot;,2400,&quot;topic_id&quot;,8,&quot;technology_id&quot;,2,&quot;reduced_outline&quot;,null,&quot;runtime_config&quot;,null,&quot;lti_only&quot;,false,&quot;instructors&quot;,[[&quot;^ &quot;,&quot;id&quot;,301837,&quot;marketing_biography&quot;,&quot;Data Scientist at DataCamp&quot;,&quot;biography&quot;,&quot;Hugo is a data scientist, educator, writer and podcaster at DataCamp. His main interests are promoting data &amp; AI literacy, helping to spread data skills through organizations and society and doing amateur stand up comedy in NYC. If you want to know what he likes to talk about, definitely check out DataFramed, the DataCamp podcast, which he hosts and produces: https://www.datacamp.com/community/podcast&quot;,&quot;avatar_url&quot;,&quot;https://assets.datacamp.com/users/avatars/000/301/837/square/hugoaboutpic.jpg?1493154678&quot;,&quot;full_name&quot;,&quot;Hugo Bowne-Anderson&quot;,&quot;instructor_path&quot;,&quot;/instructors/hugobowne&quot;]],&quot;collaborators&quot;,[[&quot;^ &quot;,&quot;^19&quot;,&quot;https://assets.datacamp.com/users/avatars/000/382/294/square/francis-photo.jpg?1471980001&quot;,&quot;^1:&quot;,&quot;Francisco Castro&quot;]],&quot;datasets&quot;,[[&quot;^ &quot;,&quot;asset_url&quot;,&quot;https://assets.datacamp.com/production/repositories/488/datasets/b422ace2fceada7b569e0ba3e8d833fddc684c4d/latitude.xls&quot;,&quot;name&quot;,&quot;Latitudes (XLS)&quot;],[&quot;^ &quot;,&quot;^1&gt;&quot;,&quot;https://assets.datacamp.com/production/repositories/488/datasets/3ef452f83a91556ea4284624b969392c0506fb33/tweets3.txt&quot;,&quot;^1?&quot;,&quot;Tweets&quot;],[&quot;^ &quot;,&quot;^1&gt;&quot;,&quot;https://assets.datacamp.com/production/repositories/488/datasets/013936d2700e2d00207ec42100d448c23692eb6f/winequality-red.csv&quot;,&quot;^1?&quot;,&quot;Red wine quality&quot;]],&quot;tracks&quot;,[[&quot;^ &quot;,&quot;path&quot;,&quot;/tracks/data-analyst-with-python&quot;,&quot;title_with_subtitle&quot;,&quot;Data Analyst  with Python&quot;],[&quot;^ &quot;,&quot;^1A&quot;,&quot;/tracks/data-scientist-with-python&quot;,&quot;^1B&quot;,&quot;Data Scientist  with Python&quot;],[&quot;^ &quot;,&quot;^1A&quot;,&quot;/tracks/importing-cleaning-data-with-python&quot;,&quot;^1B&quot;,&quot;Importing &amp; Cleaning Data  with Python&quot;]],&quot;prerequisites&quot;,[[&quot;^ &quot;,&quot;^1A&quot;,&quot;/courses/introduction-to-importing-data-in-python&quot;,&quot;^E&quot;,&quot;Introduction to Importing Data in Python&quot;]],&quot;time_needed_in_hours&quot;,2,&quot;seo_title&quot;,&quot;Intermediate Importing Data in Python&quot;,&quot;seo_description&quot;,&quot;Learn how to import data into Python from sources like the web and by pulling data from APIs, such as the Twitter streaming API to stream real-time tweets.&quot;,&quot;archived_at&quot;,null,&quot;original_image_url&quot;,&quot;https://assets.datacamp.com/production/course_1606/shields/original/shield_image_course_1606_20200310-1-17hkmhz?1583853940&quot;,&quot;external_slug&quot;,&quot;intermediate-importing-data-in-python&quot;,&quot;chapters&quot;,[[&quot;^ &quot;,&quot;id&quot;,4135,&quot;title_meta&quot;,null,&quot;^E&quot;,&quot;Importing data from the Internet&quot;,&quot;^F&quot;,&quot;The web is a rich source of data from which you can extract various types of insights and findings. In this chapter, you will learn how to get data from the web, whether it is stored in files or in HTML. You&#39;ll also learn the basics of scraping and parsing web data.&quot;,&quot;number&quot;,1,&quot;^L&quot;,&quot;importing-data-from-the-internet-1&quot;,&quot;nb_exercises&quot;,12,&quot;badge_completed_url&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing.png&quot;,&quot;badge_uncompleted_url&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing_unc.png&quot;,&quot;^O&quot;,&quot;09/12/2021&quot;,&quot;slides_link&quot;,&quot;https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/slides/chapter1.pdf&quot;,&quot;free_preview&quot;,true,&quot;xp&quot;,1050,&quot;number_of_videos&quot;,3,&quot;^:&quot;,[[&quot;^ &quot;,&quot;^R&quot;,&quot;VideoExercise&quot;,&quot;^E&quot;,&quot;Importing flat files from the web&quot;,&quot;aggregate_xp&quot;,50,&quot;^1L&quot;,1,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=1&quot;],[&quot;^ &quot;,&quot;^R&quot;,&quot;NormalExercise&quot;,&quot;^E&quot;,&quot;Importing flat files from the web: your turn!&quot;,&quot;^1S&quot;,100,&quot;^1L&quot;,2,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=2&quot;],[&quot;^ &quot;,&quot;^R&quot;,&quot;NormalExercise&quot;,&quot;^E&quot;,&quot;Opening and reading flat files from the web&quot;,&quot;^1S&quot;,100,&quot;^1L&quot;,3,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=3&quot;],[&quot;^ &quot;,&quot;^R&quot;,&quot;NormalExercise&quot;,&quot;^E&quot;,&quot;Importing non-flat files from the web&quot;,&quot;^1S&quot;,100,&quot;^1L&quot;,4,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=4&quot;],[&quot;^ &quot;,&quot;^R&quot;,&quot;VideoExercise&quot;,&quot;^E&quot;,&quot;HTTP requests to import files from the web&quot;,&quot;^1S&quot;,50,&quot;^1L&quot;,5,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=5&quot;],[&quot;^ &quot;,&quot;^R&quot;,&quot;NormalExercise&quot;,&quot;^E&quot;,&quot;Performing HTTP requests in Python using urllib&quot;,&quot;^1S&quot;,100,&quot;^1L&quot;,6,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=6&quot;],[&quot;^ &quot;,&quot;^R&quot;,&quot;NormalExercise&quot;,&quot;^E&quot;,&quot;Printing HTTP request results in Python using urllib&quot;,&quot;^1S&quot;,100,&quot;^1L&quot;,7,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=7&quot;],[&quot;^ &quot;,&quot;^R&quot;,&quot;NormalExercise&quot;,&quot;^E&quot;,&quot;Performing HTTP requests in Python using requests&quot;,&quot;^1S&quot;,100,&quot;^1L&quot;,8,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=8&quot;],[&quot;^ &quot;,&quot;^R&quot;,&quot;VideoExercise&quot;,&quot;^E&quot;,&quot;Scraping the web in Python&quot;,&quot;^1S&quot;,50,&quot;^1L&quot;,9,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=9&quot;],[&quot;^ &quot;,&quot;^R&quot;,&quot;NormalExercise&quot;,&quot;^E&quot;,&quot;Parsing HTML with BeautifulSoup&quot;,&quot;^1S&quot;,100,&quot;^1L&quot;,10,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=10&quot;],[&quot;^ &quot;,&quot;^R&quot;,&quot;NormalExercise&quot;,&quot;^E&quot;,&quot;Turning a webpage into data using BeautifulSoup: getting the text&quot;,&quot;^1S&quot;,100,&quot;^1L&quot;,11,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=11&quot;],[&quot;^ &quot;,&quot;^R&quot;,&quot;NormalExercise&quot;,&quot;^E&quot;,&quot;Turning a webpage into data using BeautifulSoup: getting the hyperlinks&quot;,&quot;^1S&quot;,100,&quot;^1L&quot;,12,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=12&quot;]]],[&quot;^ &quot;,&quot;id&quot;,4136,&quot;^1K&quot;,null,&quot;^E&quot;,&quot;Interacting with APIs to import data from the web&quot;,&quot;^F&quot;,&quot;In this chapter, you will gain a deeper understanding of how to import data from the web. You will learn the basics of extracting data from APIs, gain insight on the importance of APIs, and practice extracting data by diving into the OMDB and Library of Congress APIs.&quot;,&quot;^1L&quot;,2,&quot;^L&quot;,&quot;interacting-with-apis-to-import-data-from-the-web-2&quot;,&quot;^1M&quot;,9,&quot;^1N&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing.png&quot;,&quot;^1O&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing_unc.png&quot;,&quot;^O&quot;,&quot;09/12/2021&quot;,&quot;^1P&quot;,&quot;https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/slides/chapter2.pdf&quot;,&quot;^1Q&quot;,null,&quot;xp&quot;,650,&quot;^1R&quot;,2,&quot;^:&quot;,[[&quot;^ &quot;,&quot;^R&quot;,&quot;VideoExercise&quot;,&quot;^E&quot;,&quot;Introduction to APIs and JSONs&quot;,&quot;^1S&quot;,50,&quot;^1L&quot;,1,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/interacting-with-apis-to-import-data-from-the-web-2?ex=1&quot;],[&quot;^ &quot;,&quot;^R&quot;,&quot;PureMultipleChoiceExercise&quot;,&quot;^E&quot;,&quot;Pop quiz: What exactly is a JSON?&quot;,&quot;^1S&quot;,50,&quot;^1L&quot;,2,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/interacting-with-apis-to-import-data-from-the-web-2?ex=2&quot;],[&quot;^ &quot;,&quot;^R&quot;,&quot;NormalExercise&quot;,&quot;^E&quot;,&quot;Loading and exploring a JSON&quot;,&quot;^1S&quot;,100,&quot;^1L&quot;,3,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/interacting-with-apis-to-import-data-from-the-web-2?ex=3&quot;],[&quot;^ &quot;,&quot;^R&quot;,&quot;MultipleChoiceExercise&quot;,&quot;^E&quot;,&quot;Pop quiz: Exploring your JSON&quot;,&quot;^1S&quot;,50,&quot;^1L&quot;,4,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/interacting-with-apis-to-import-data-from-the-web-2?ex=4&quot;],[&quot;^ &quot;,&quot;^R&quot;,&quot;VideoExercise&quot;,&quot;^E&quot;,&quot;APIs and interacting with the world wide web&quot;,&quot;^1S&quot;,50,&quot;^1L&quot;,5,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/interacting-with-apis-to-import-data-from-the-web-2?ex=5&quot;],[&quot;^ &quot;,&quot;^R&quot;,&quot;PureMultipleChoiceExercise&quot;,&quot;^E&quot;,&quot;Pop quiz: What&#39;s an API?&quot;,&quot;^1S&quot;,50,&quot;^1L&quot;,6,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/interacting-with-apis-to-import-data-from-the-web-2?ex=6&quot;],[&quot;^ &quot;,&quot;^R&quot;,&quot;NormalExercise&quot;,&quot;^E&quot;,&quot;API requests&quot;,&quot;^1S&quot;,100,&quot;^1L&quot;,7,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/interacting-with-apis-to-import-data-from-the-web-2?ex=7&quot;],[&quot;^ &quot;,&quot;^R&quot;,&quot;NormalExercise&quot;,&quot;^E&quot;,&quot;JSON\\xe2\\x80\\x93from the web to Python&quot;,&quot;^1S&quot;,100,&quot;^1L&quot;,8,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/interacting-with-apis-to-import-data-from-the-web-2?ex=8&quot;],[&quot;^ &quot;,&quot;^R&quot;,&quot;NormalExercise&quot;,&quot;^E&quot;,&quot;Checking out the Wikipedia API&quot;,&quot;^1S&quot;,100,&quot;^1L&quot;,9,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/interacting-with-apis-to-import-data-from-the-web-2?ex=9&quot;]]],[&quot;^ &quot;,&quot;id&quot;,4140,&quot;^1K&quot;,null,&quot;^E&quot;,&quot;Diving  deep into the Twitter API&quot;,&quot;^F&quot;,&quot;In this chapter, you will consolidate your knowledge of interacting with APIs in a deep dive into the Twitter streaming API. You&#39;ll learn how to stream real-time Twitter data, and how to analyze and visualize it.&quot;,&quot;^1L&quot;,3,&quot;^L&quot;,&quot;diving-deep-into-the-twitter-api&quot;,&quot;^1M&quot;,8,&quot;^1N&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing.png&quot;,&quot;^1O&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing_unc.png&quot;,&quot;^O&quot;,&quot;09/12/2021&quot;,&quot;^1P&quot;,&quot;https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/slides/chapter3.pdf&quot;,&quot;^1Q&quot;,null,&quot;xp&quot;,700,&quot;^1R&quot;,2,&quot;^:&quot;,[[&quot;^ &quot;,&quot;^R&quot;,&quot;VideoExercise&quot;,&quot;^E&quot;,&quot;The Twitter API and Authentication&quot;,&quot;^1S&quot;,50,&quot;^1L&quot;,1,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/diving-deep-into-the-twitter-api?ex=1&quot;],[&quot;^ &quot;,&quot;^R&quot;,&quot;NormalExercise&quot;,&quot;^E&quot;,&quot;API Authentication&quot;,&quot;^1S&quot;,100,&quot;^1L&quot;,2,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/diving-deep-into-the-twitter-api?ex=2&quot;],[&quot;^ &quot;,&quot;^R&quot;,&quot;NormalExercise&quot;,&quot;^E&quot;,&quot;Streaming tweets&quot;,&quot;^1S&quot;,100,&quot;^1L&quot;,3,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/diving-deep-into-the-twitter-api?ex=3&quot;],[&quot;^ &quot;,&quot;^R&quot;,&quot;NormalExercise&quot;,&quot;^E&quot;,&quot;Load and explore your Twitter data&quot;,&quot;^1S&quot;,100,&quot;^1L&quot;,4,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/diving-deep-into-the-twitter-api?ex=4&quot;],[&quot;^ &quot;,&quot;^R&quot;,&quot;NormalExercise&quot;,&quot;^E&quot;,&quot;Twitter data to DataFrame&quot;,&quot;^1S&quot;,100,&quot;^1L&quot;,5,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/diving-deep-into-the-twitter-api?ex=5&quot;],[&quot;^ &quot;,&quot;^R&quot;,&quot;NormalExercise&quot;,&quot;^E&quot;,&quot;A little bit of Twitter text analysis&quot;,&quot;^1S&quot;,100,&quot;^1L&quot;,6,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/diving-deep-into-the-twitter-api?ex=6&quot;],[&quot;^ &quot;,&quot;^R&quot;,&quot;NormalExercise&quot;,&quot;^E&quot;,&quot;Plotting your Twitter data&quot;,&quot;^1S&quot;,100,&quot;^1L&quot;,7,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/diving-deep-into-the-twitter-api?ex=7&quot;],[&quot;^ &quot;,&quot;^R&quot;,&quot;VideoExercise&quot;,&quot;^E&quot;,&quot;Final Thoughts&quot;,&quot;^1S&quot;,50,&quot;^1L&quot;,8,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/diving-deep-into-the-twitter-api?ex=8&quot;]]]]]]]],&quot;^6&quot;,[&quot;^0&quot;,[&quot;^ &quot;,&quot;n&quot;,&quot;PreFetchedRequestRecord&quot;,&quot;v&quot;,[&quot;^ &quot;,&quot;^C&quot;,&quot;SUCCESS&quot;,&quot;^D&quot;,[&quot;^ &quot;,&quot;id&quot;,4135,&quot;^1K&quot;,null,&quot;^E&quot;,&quot;Importing data from the Internet&quot;,&quot;^F&quot;,&quot;The web is a rich source of data from which you can extract various types of insights and findings. In this chapter, you will learn how to get data from the web, whether it is stored in files or in HTML. You&#39;ll also learn the basics of scraping and parsing web data.&quot;,&quot;^1L&quot;,1,&quot;^L&quot;,&quot;importing-data-from-the-internet-1&quot;,&quot;^1M&quot;,12,&quot;^1N&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing.png&quot;,&quot;^1O&quot;,&quot;https://assets.datacamp.com/production/default/badges/missing_unc.png&quot;,&quot;^O&quot;,&quot;09/12/2021&quot;,&quot;^1P&quot;,&quot;https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/slides/chapter1.pdf&quot;,&quot;^1Q&quot;,true,&quot;xp&quot;,1050,&quot;^1R&quot;,3,&quot;^:&quot;,[[&quot;^ &quot;,&quot;^R&quot;,&quot;VideoExercise&quot;,&quot;^E&quot;,&quot;Importing flat files from the web&quot;,&quot;^1S&quot;,50,&quot;^1L&quot;,1,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=1&quot;],[&quot;^ &quot;,&quot;^R&quot;,&quot;NormalExercise&quot;,&quot;^E&quot;,&quot;Importing flat files from the web: your turn!&quot;,&quot;^1S&quot;,100,&quot;^1L&quot;,2,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=2&quot;],[&quot;^ &quot;,&quot;^R&quot;,&quot;NormalExercise&quot;,&quot;^E&quot;,&quot;Opening and reading flat files from the web&quot;,&quot;^1S&quot;,100,&quot;^1L&quot;,3,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=3&quot;],[&quot;^ &quot;,&quot;^R&quot;,&quot;NormalExercise&quot;,&quot;^E&quot;,&quot;Importing non-flat files from the web&quot;,&quot;^1S&quot;,100,&quot;^1L&quot;,4,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=4&quot;],[&quot;^ &quot;,&quot;^R&quot;,&quot;VideoExercise&quot;,&quot;^E&quot;,&quot;HTTP requests to import files from the web&quot;,&quot;^1S&quot;,50,&quot;^1L&quot;,5,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=5&quot;],[&quot;^ &quot;,&quot;^R&quot;,&quot;NormalExercise&quot;,&quot;^E&quot;,&quot;Performing HTTP requests in Python using urllib&quot;,&quot;^1S&quot;,100,&quot;^1L&quot;,6,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=6&quot;],[&quot;^ &quot;,&quot;^R&quot;,&quot;NormalExercise&quot;,&quot;^E&quot;,&quot;Printing HTTP request results in Python using urllib&quot;,&quot;^1S&quot;,100,&quot;^1L&quot;,7,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=7&quot;],[&quot;^ &quot;,&quot;^R&quot;,&quot;NormalExercise&quot;,&quot;^E&quot;,&quot;Performing HTTP requests in Python using requests&quot;,&quot;^1S&quot;,100,&quot;^1L&quot;,8,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=8&quot;],[&quot;^ &quot;,&quot;^R&quot;,&quot;VideoExercise&quot;,&quot;^E&quot;,&quot;Scraping the web in Python&quot;,&quot;^1S&quot;,50,&quot;^1L&quot;,9,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=9&quot;],[&quot;^ &quot;,&quot;^R&quot;,&quot;NormalExercise&quot;,&quot;^E&quot;,&quot;Parsing HTML with BeautifulSoup&quot;,&quot;^1S&quot;,100,&quot;^1L&quot;,10,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=10&quot;],[&quot;^ &quot;,&quot;^R&quot;,&quot;NormalExercise&quot;,&quot;^E&quot;,&quot;Turning a webpage into data using BeautifulSoup: getting the text&quot;,&quot;^1S&quot;,100,&quot;^1L&quot;,11,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=11&quot;],[&quot;^ &quot;,&quot;^R&quot;,&quot;NormalExercise&quot;,&quot;^E&quot;,&quot;Turning a webpage into data using BeautifulSoup: getting the hyperlinks&quot;,&quot;^1S&quot;,100,&quot;^1L&quot;,12,&quot;url&quot;,&quot;https://campus.datacamp.com/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=12&quot;]]]]]],&quot;^:&quot;,[&quot;^0&quot;,[&quot;^ &quot;,&quot;n&quot;,&quot;PreFetchedRequestRecord&quot;,&quot;v&quot;,[&quot;^ &quot;,&quot;^C&quot;,&quot;SUCCESS&quot;,&quot;^D&quot;,[[&quot;^ &quot;,&quot;id&quot;,990668,&quot;^R&quot;,&quot;VideoExercise&quot;,&quot;assignment&quot;,null,&quot;^E&quot;,&quot;Importing flat files from the web&quot;,&quot;sample_code&quot;,&quot;&quot;,&quot;instructions&quot;,null,&quot;^1L&quot;,1,&quot;sct&quot;,&quot;&quot;,&quot;pre_exercise_code&quot;,&quot;&quot;,&quot;solution&quot;,&quot;&quot;,&quot;hint&quot;,null,&quot;attachments&quot;,null,&quot;xp&quot;,50,&quot;possible_answers&quot;,[],&quot;feedbacks&quot;,[],&quot;question&quot;,&quot;&quot;,&quot;video_link&quot;,null,&quot;video_hls&quot;,null,&quot;aspect_ratio&quot;,56.25,&quot;projector_key&quot;,&quot;course_1606_59604c018a6e132016cd26144a12fee0&quot;,&quot;key&quot;,&quot;e36457c7ed&quot;,&quot;language&quot;,&quot;python&quot;,&quot;course_id&quot;,1606,&quot;chapter_id&quot;,4135,&quot;^14&quot;,null,&quot;version&quot;,&quot;v0&quot;,&quot;randomNumber&quot;,0.7355159797845514,&quot;externalId&quot;,990668],[&quot;^ &quot;,&quot;id&quot;,42707,&quot;^R&quot;,&quot;NormalExercise&quot;,&quot;^1T&quot;,&quot;&lt;p&gt;You are about to import your first file from the web! The flat file you will import will be &lt;code&gt;&#39;winequality-red.csv&#39;&lt;/code&gt; from the University of California, Irvine&#39;s &lt;a href=\\\\\\\\&quot;http://archive.ics.uci.edu/ml/index.html\\\\\\\\&quot;&gt;Machine Learning repository&lt;/a&gt;. The flat file contains tabular data of physiochemical properties of red wine, such as pH, alcohol content and citric acid content, along with wine quality rating.&lt;/p&gt;\\\\\\\\n&lt;p&gt;The URL of the file is&lt;/p&gt;\\\\\\\\n&lt;pre&gt;&lt;code&gt;&#39;https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/datasets/winequality-red.csv&#39;\\\\\\\\n&lt;/code&gt;&lt;/pre&gt;\\\\\\\\n&lt;p&gt;After you import it, you&#39;ll check your working directory to confirm that it is there and then you&#39;ll load it into a &lt;code&gt;pandas&lt;/code&gt; DataFrame.&lt;/p&gt;&quot;,&quot;^E&quot;,&quot;Importing flat files from the web: your turn!&quot;,&quot;^1U&quot;,&quot;# Import package\\\\\\\\nfrom ____ import ____\\\\\\\\n\\\\\\\\n# Import pandas\\\\\\\\nimport pandas as pd\\\\\\\\n\\\\\\\\n# Assign url of file: url\\\\\\\\n\\\\\\\\n\\\\\\\\n# Save file locally\\\\\\\\n\\\\\\\\n\\\\\\\\n# Read file into a DataFrame and print its head\\\\\\\\ndf = pd.read_csv(&#39;winequality-red.csv&#39;, sep=&#39;;&#39;)\\\\\\\\nprint(df.head())&quot;,&quot;^1V&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Import the function &lt;code&gt;urlretrieve&lt;/code&gt; from the subpackage &lt;code&gt;urllib.request&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Assign the URL of the file to the variable &lt;code&gt;url&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Use the function &lt;code&gt;urlretrieve()&lt;/code&gt; to save the file locally as &lt;code&gt;&#39;winequality-red.csv&#39;&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Execute the remaining code to load &lt;code&gt;&#39;winequality-red.csv&#39;&lt;/code&gt; in a pandas DataFrame and to print its head to the shell.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;^1L&quot;,2,&quot;sct&quot;,&quot;Ex().has_import(\\\\\\\\&quot;urllib.request.urlretrieve\\\\\\\\&quot;)\\\\\\\\nEx().has_import(\\\\\\\\&quot;pandas\\\\\\\\&quot;)\\\\\\\\nEx().check_object(\\\\\\\\&quot;url\\\\\\\\&quot;).has_equal_value()\\\\\\\\nEx().check_function(\\\\\\\\&quot;urllib.request.urlretrieve\\\\\\\\&quot;).multi(\\\\\\\\n  check_args(0).has_equal_value(),\\\\\\\\n  check_args(1).has_equal_value()\\\\\\\\n)\\\\\\\\nEx().check_correct(\\\\\\\\n  check_object(\\\\\\\\&quot;df\\\\\\\\&quot;).has_equal_value(),\\\\\\\\n  check_function(\\\\\\\\&quot;pandas.read_csv\\\\\\\\&quot;).multi(\\\\\\\\n    check_args(0).has_equal_value(),\\\\\\\\n    check_args(1).has_equal_value()\\\\\\\\n  )\\\\\\\\n)\\\\\\\\nEx().has_printout(0)\\\\\\\\nsuccess_msg(\\\\\\\\&quot;Awesome!\\\\\\\\&quot;)\\\\\\\\n&quot;,&quot;^1W&quot;,&quot;&quot;,&quot;^1X&quot;,&quot;# Import package\\\\\\\\nfrom urllib.request import urlretrieve\\\\\\\\n\\\\\\\\n# Import pandas\\\\\\\\nimport pandas as pd\\\\\\\\n\\\\\\\\n# Assign url of file: url\\\\\\\\nurl = &#39;https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/datasets/winequality-red.csv&#39;\\\\\\\\n\\\\\\\\n# Save file locally\\\\\\\\nurlretrieve(url, &#39;winequality-red.csv&#39;)\\\\\\\\n\\\\\\\\n# Read file into a DataFrame and print its head\\\\\\\\ndf = pd.read_csv(&#39;winequality-red.csv&#39;, sep=&#39;;&#39;)\\\\\\\\nprint(df.head())&quot;,&quot;^1Y&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;To import a function &lt;code&gt;y&lt;/code&gt; from a subpackage &lt;code&gt;x&lt;/code&gt;, execute &lt;code&gt;from x import y&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;This one&#39;s a long URL. Make sure you typed it in correctly!&lt;/li&gt;\\\\\\\\n&lt;li&gt;Pass the &lt;em&gt;url&lt;/em&gt; to import (in the &lt;code&gt;url&lt;/code&gt; object you defined) as the first argument and the &lt;em&gt;filename&lt;/em&gt; for saving the file locally as the second argument to &lt;code&gt;urlretrieve()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You don&#39;t have to change the code for loading &lt;code&gt;&#39;winequality-red.csv&#39;&lt;/code&gt; and printing its head.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;^1Z&quot;,null,&quot;xp&quot;,100,&quot;^1[&quot;,[],&quot;^20&quot;,[],&quot;^21&quot;,&quot;&quot;,&quot;^26&quot;,&quot;python&quot;,&quot;^2:&quot;,0.7979484705551529,&quot;^2;&quot;,42707],[&quot;^ &quot;,&quot;id&quot;,42708,&quot;^R&quot;,&quot;NormalExercise&quot;,&quot;^1T&quot;,&quot;&lt;p&gt;You have just imported a file from the web, saved it locally and loaded it into a DataFrame. If you just wanted to load a file from the web into a DataFrame without first saving it locally, you can do that easily using &lt;code&gt;pandas&lt;/code&gt;. In particular, you can use the function &lt;code&gt;pd.read_csv()&lt;/code&gt; with the URL as the first argument and the separator &lt;code&gt;sep&lt;/code&gt; as the second argument.&lt;/p&gt;\\\\\\\\n&lt;p&gt;The URL of the file, once again, is&lt;/p&gt;\\\\\\\\n&lt;pre&gt;&lt;code&gt;&#39;https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/datasets/winequality-red.csv&#39;\\\\\\\\n&lt;/code&gt;&lt;/pre&gt;&quot;,&quot;^E&quot;,&quot;Opening and reading flat files from the web&quot;,&quot;^1U&quot;,&quot;# Import packages\\\\\\\\nimport matplotlib.pyplot as plt\\\\\\\\nimport pandas as pd\\\\\\\\n\\\\\\\\n# Assign url of file: url\\\\\\\\n\\\\\\\\n\\\\\\\\n# Read file into a DataFrame: df\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print the head of the DataFrame\\\\\\\\nprint(____)\\\\\\\\n\\\\\\\\n# Plot first column of df\\\\\\\\npd.DataFrame.hist(df.ix[:, 0:1])\\\\\\\\nplt.xlabel(&#39;fixed acidity (g(tartaric acid)/dm$^3$)&#39;)\\\\\\\\nplt.ylabel(&#39;count&#39;)\\\\\\\\nplt.show()\\\\\\\\n&quot;,&quot;^1V&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Assign the URL of the file to the variable &lt;code&gt;url&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Read file into a DataFrame &lt;code&gt;df&lt;/code&gt; using &lt;code&gt;pd.read_csv()&lt;/code&gt;, recalling that the separator in the file is &lt;code&gt;&#39;;&#39;&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Print the head of the DataFrame &lt;code&gt;df&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Execute the rest of the code to plot histogram of the first feature in the DataFrame &lt;code&gt;df&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;^1L&quot;,3,&quot;sct&quot;,&quot;Ex().has_import(\\\\\\\\&quot;matplotlib.pyplot\\\\\\\\&quot;)\\\\\\\\nEx().has_import(\\\\\\\\&quot;pandas\\\\\\\\&quot;)\\\\\\\\nEx().check_object(\\\\\\\\&quot;url\\\\\\\\&quot;).has_equal_value()\\\\\\\\nEx().check_correct(\\\\\\\\n  check_object(\\\\\\\\&quot;df\\\\\\\\&quot;).has_equal_value(),\\\\\\\\n  check_function(\\\\\\\\&quot;pandas.read_csv\\\\\\\\&quot;).multi(\\\\\\\\n    check_args(0).has_equal_value(),\\\\\\\\n    check_args(1).has_equal_value()\\\\\\\\n  )\\\\\\\\n)\\\\\\\\nEx().has_printout(0)\\\\\\\\nEx().check_function(\\\\\\\\&quot;pandas.DataFrame.hist\\\\\\\\&quot;).check_args(0).has_equal_value()\\\\\\\\nEx().check_function(\\\\\\\\&quot;matplotlib.pyplot.show\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\nsuccess_msg(\\\\\\\\&quot;Awesome!\\\\\\\\&quot;)\\\\\\\\n&quot;,&quot;^1W&quot;,&quot;&quot;,&quot;^1X&quot;,&quot;# Import packages\\\\\\\\nimport matplotlib.pyplot as plt\\\\\\\\nimport pandas as pd\\\\\\\\n\\\\\\\\n# Assign url of file: url\\\\\\\\nurl = &#39;https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/datasets/winequality-red.csv&#39;\\\\\\\\n\\\\\\\\n# Read file into a DataFrame: df\\\\\\\\ndf = pd.read_csv(url, sep=&#39;;&#39;)\\\\\\\\n\\\\\\\\n# Print the head of the DataFrame\\\\\\\\nprint(df.head())\\\\\\\\n\\\\\\\\n# Plot first column of df\\\\\\\\npd.DataFrame.hist(df.ix[:, 0:1])\\\\\\\\nplt.xlabel(&#39;fixed acidity (g(tartaric acid)/dm$^3$)&#39;)\\\\\\\\nplt.ylabel(&#39;count&#39;)\\\\\\\\nplt.show()\\\\\\\\n&quot;,&quot;^1Y&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Make sure you typed the URL correctly!&lt;/li&gt;\\\\\\\\n&lt;li&gt;Pass the &lt;em&gt;url&lt;/em&gt; (the &lt;code&gt;url&lt;/code&gt; object you defined) as the first argument and the &lt;em&gt;separator&lt;/em&gt; as the second argument to &lt;code&gt;pd.read_csv()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;The &lt;em&gt;head&lt;/em&gt; of a DataFrame can be accessed by using &lt;code&gt;head()&lt;/code&gt; on the DataFrame.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You don&#39;t have to change any of the code for plotting the histograms.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;^1Z&quot;,null,&quot;xp&quot;,100,&quot;^1[&quot;,[],&quot;^20&quot;,[],&quot;^21&quot;,&quot;&quot;,&quot;^26&quot;,&quot;python&quot;,&quot;^2:&quot;,0.3820707880727505,&quot;^2;&quot;,42708],[&quot;^ &quot;,&quot;id&quot;,42709,&quot;^R&quot;,&quot;NormalExercise&quot;,&quot;^1T&quot;,&quot;&lt;p&gt;Congrats! You&#39;ve just loaded a flat file from the web into a DataFrame without first saving it locally using the &lt;code&gt;pandas&lt;/code&gt; function &lt;code&gt;pd.read_csv()&lt;/code&gt;. This function is super cool because it has close relatives that allow you to load all types of files, not only flat ones. In this interactive exercise, you&#39;ll use &lt;code&gt;pd.read_excel()&lt;/code&gt; to import an Excel spreadsheet.&lt;/p&gt;\\\\\\\\n&lt;p&gt;The URL of the spreadsheet is&lt;/p&gt;\\\\\\\\n&lt;pre&gt;&lt;code&gt;&#39;http://s3.amazonaws.com/assets.datacamp.com/course/importing_data_into_r/latitude.xls&#39;\\\\\\\\n&lt;/code&gt;&lt;/pre&gt;\\\\\\\\n&lt;p&gt;Your job is to use &lt;code&gt;pd.read_excel()&lt;/code&gt; to read in all of its sheets, print the sheet names and then print the head of the first sheet &lt;em&gt;using its name, not its index&lt;/em&gt;.&lt;/p&gt;\\\\\\\\n&lt;p&gt;Note that the output of &lt;code&gt;pd.read_excel()&lt;/code&gt; is a Python dictionary with sheet names as keys and corresponding DataFrames as corresponding values.&lt;/p&gt;&quot;,&quot;^E&quot;,&quot;Importing non-flat files from the web&quot;,&quot;^1U&quot;,&quot;# Import package\\\\\\\\nimport pandas as pd\\\\\\\\n\\\\\\\\n# Assign url of file: url\\\\\\\\n\\\\\\\\n\\\\\\\\n# Read in all sheets of Excel file: xls\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print the sheetnames to the shell\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print the head of the first sheet (using its name, NOT its index)\\\\\\\\n\\\\\\\\n&quot;,&quot;^1V&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Assign the URL of the file to the variable &lt;code&gt;url&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Read the file in &lt;code&gt;url&lt;/code&gt; into a dictionary &lt;code&gt;xls&lt;/code&gt; using &lt;code&gt;pd.read_excel()&lt;/code&gt; recalling that, in order to import all sheets you need to pass &lt;code&gt;None&lt;/code&gt; to the argument &lt;code&gt;sheet_name&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Print the names of the sheets in the Excel spreadsheet; these will be the keys of the dictionary &lt;code&gt;xls&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Print the head of the first sheet &lt;em&gt;using the sheet name, not the index of the sheet&lt;/em&gt;! The sheet name is &lt;code&gt;&#39;1700&#39;&lt;/code&gt;&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;^1L&quot;,4,&quot;sct&quot;,&quot;Ex().has_import(&#39;pandas&#39;)\\\\\\\\nEx().check_correct(\\\\\\\\n    has_printout(0),\\\\\\\\n    multi(\\\\\\\\n        check_correct(\\\\\\\\n            check_object(&#39;xls&#39;).is_instance(dict),\\\\\\\\n            check_correct(\\\\\\\\n                check_function(&#39;pandas.read_excel&#39;).multi(\\\\\\\\n                    check_args(0).has_equal_value(),\\\\\\\\n                    check_args(&#39;sheet_name&#39;).has_equal_value()\\\\\\\\n                ),\\\\\\\\n                check_object(&#39;url&#39;).has_equal_value()\\\\\\\\n            )\\\\\\\\n        )\\\\\\\\n    )\\\\\\\\n)\\\\\\\\nEx().has_printout(1)\\\\\\\\nsuccess_msg(\\\\\\\\&quot;Awesome!\\\\\\\\&quot;)&quot;,&quot;^1W&quot;,&quot;&quot;,&quot;^1X&quot;,&quot;# Import package\\\\\\\\nimport pandas as pd\\\\\\\\n\\\\\\\\n# Assign url of file: url\\\\\\\\nurl = &#39;http://s3.amazonaws.com/assets.datacamp.com/course/importing_data_into_r/latitude.xls&#39;\\\\\\\\n\\\\\\\\n# Read in all sheets of Excel file: xls\\\\\\\\nxls = pd.read_excel(url, sheet_name=None)\\\\\\\\n\\\\\\\\n# Print the sheetnames to the shell\\\\\\\\nprint(xls.keys())\\\\\\\\n\\\\\\\\n# Print the head of the first sheet (using its name, NOT its index)\\\\\\\\nprint(xls[&#39;1700&#39;].head())&quot;,&quot;^1Y&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Make sure you typed in the URL correctly!&lt;/li&gt;\\\\\\\\n&lt;li&gt;Pass the &lt;em&gt;url&lt;/em&gt; (the &lt;code&gt;url&lt;/code&gt; object you defined) as the first argument and &lt;code&gt;sheet_name&lt;/code&gt; with its corresponding value as the second argument to &lt;code&gt;pd.read_excel()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;The &lt;em&gt;keys&lt;/em&gt; of a dictionary can be accessed by using &lt;code&gt;keys()&lt;/code&gt; on the dictionary.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You can access a sheet using the format: &lt;em&gt;dictionary&lt;/em&gt;&lt;strong&gt;[&lt;/strong&gt;&lt;em&gt;sheet name or index&lt;/em&gt;&lt;strong&gt;]&lt;/strong&gt;.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;^1Z&quot;,null,&quot;xp&quot;,100,&quot;^1[&quot;,[],&quot;^20&quot;,[],&quot;^21&quot;,&quot;&quot;,&quot;^26&quot;,&quot;python&quot;,&quot;^2:&quot;,0.520461459043172,&quot;^2;&quot;,42709],[&quot;^ &quot;,&quot;id&quot;,990669,&quot;^R&quot;,&quot;VideoExercise&quot;,&quot;^1T&quot;,null,&quot;^E&quot;,&quot;HTTP requests to import files from the web&quot;,&quot;^1U&quot;,&quot;&quot;,&quot;^1V&quot;,null,&quot;^1L&quot;,5,&quot;sct&quot;,&quot;&quot;,&quot;^1W&quot;,&quot;&quot;,&quot;^1X&quot;,&quot;&quot;,&quot;^1Y&quot;,null,&quot;^1Z&quot;,null,&quot;xp&quot;,50,&quot;^1[&quot;,[],&quot;^20&quot;,[],&quot;^21&quot;,&quot;&quot;,&quot;^22&quot;,null,&quot;^23&quot;,null,&quot;^24&quot;,56.25,&quot;^25&quot;,&quot;course_1606_9d15ae176be1800b996f7869a82b8087&quot;,&quot;key&quot;,&quot;e480d1fdcf&quot;,&quot;^26&quot;,&quot;python&quot;,&quot;^27&quot;,1606,&quot;^28&quot;,4135,&quot;^14&quot;,null,&quot;^29&quot;,&quot;v0&quot;,&quot;^2:&quot;,0.7252146440228739,&quot;^2;&quot;,990669],[&quot;^ &quot;,&quot;id&quot;,42711,&quot;^R&quot;,&quot;NormalExercise&quot;,&quot;^1T&quot;,&quot;&lt;p&gt;Now that you know the basics behind HTTP GET requests, it&#39;s time to perform some of your own. In this interactive exercise, you will ping our very own DataCamp servers to perform a GET request to extract information from the first coding exercise of this course, &lt;code&gt;\\\\\\\\&quot;https://campus.datacamp.com/courses/1606/4135?ex=2\\\\\\\\&quot;&lt;/code&gt;.&lt;/p&gt;\\\\\\\\n&lt;p&gt;In the next exercise, you&#39;ll extract the HTML itself. Right now, however, you are going to package and send the request and then catch the response.&lt;/p&gt;&quot;,&quot;^E&quot;,&quot;Performing HTTP requests in Python using urllib&quot;,&quot;^1U&quot;,&quot;# Import packages\\\\\\\\n\\\\\\\\n\\\\\\\\n# Specify the url\\\\\\\\nurl = \\\\\\\\&quot;https://campus.datacamp.com/courses/1606/4135?ex=2\\\\\\\\&quot;\\\\\\\\n\\\\\\\\n# This packages the request: request\\\\\\\\n\\\\\\\\n\\\\\\\\n# Sends the request and catches the response: response\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print the datatype of response\\\\\\\\nprint(type(response))\\\\\\\\n\\\\\\\\n# Be polite and close the response!\\\\\\\\nresponse.close()\\\\\\\\n&quot;,&quot;^1V&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Import the functions &lt;code&gt;urlopen&lt;/code&gt; and &lt;code&gt;Request&lt;/code&gt; from the subpackage &lt;code&gt;urllib.request&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Package the request to the url &lt;code&gt;\\\\\\\\&quot;https://campus.datacamp.com/courses/1606/4135?ex=2\\\\\\\\&quot;&lt;/code&gt; using the function &lt;code&gt;Request()&lt;/code&gt; and assign it to &lt;code&gt;request&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Send the request and catch the response in the variable &lt;code&gt;response&lt;/code&gt; with  the function &lt;code&gt;urlopen()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Run the rest of the code to see the datatype of &lt;code&gt;response&lt;/code&gt; and to close the connection!&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;^1L&quot;,6,&quot;sct&quot;,&quot;\\\\\\\\n# Test: import urlopen, Request\\\\\\\\nimport_msg = \\\\\\\\&quot;Did you correctly import the required packages?\\\\\\\\&quot;\\\\\\\\nEx().has_import(\\\\\\\\n    \\\\\\\\&quot;urllib.request.urlopen\\\\\\\\&quot;,\\\\\\\\n    not_imported_msg=import_msg\\\\\\\\n)\\\\\\\\nEx().has_import(\\\\\\\\n    \\\\\\\\&quot;urllib.request.Request\\\\\\\\&quot;,\\\\\\\\n    not_imported_msg=import_msg\\\\\\\\n)\\\\\\\\n\\\\\\\\n# Test: Predefined code\\\\\\\\npredef_msg = \\\\\\\\&quot;You don&#39;t have to change any of the predefined code.\\\\\\\\&quot;\\\\\\\\nEx().check_object(\\\\\\\\&quot;url\\\\\\\\&quot;, missing_msg=predef_msg).has_equal_value(incorrect_msg = predef_msg)\\\\\\\\n\\\\\\\\n# Test: call to Request() and &#39;request&#39; variable\\\\\\\\nEx().check_function(\\\\\\\\&quot;urllib.request.Request\\\\\\\\&quot;).check_args(0).has_equal_value()\\\\\\\\nEx().check_object(\\\\\\\\&quot;request\\\\\\\\&quot;)\\\\\\\\n  \\\\\\\\n# Test: call to urlopen() and &#39;response&#39; variable\\\\\\\\nEx().check_function(\\\\\\\\&quot;urllib.request.urlopen\\\\\\\\&quot;).check_args(0).has_equal_ast()\\\\\\\\nEx().check_object(\\\\\\\\&quot;response\\\\\\\\&quot;),\\\\\\\\n\\\\\\\\n# Test: Predefined code\\\\\\\\nEx().has_printout(0)\\\\\\\\nEx().check_function(\\\\\\\\&quot;response.close\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\nsuccess_msg(\\\\\\\\&quot;Awesome!\\\\\\\\&quot;)\\\\\\\\n&quot;,&quot;^1W&quot;,&quot;&quot;,&quot;^1X&quot;,&quot;# Import packages\\\\\\\\nfrom urllib.request import urlopen, Request\\\\\\\\n\\\\\\\\n# Specify the url\\\\\\\\nurl = \\\\\\\\&quot;https://campus.datacamp.com/courses/1606/4135?ex=2\\\\\\\\&quot;\\\\\\\\n\\\\\\\\n# This packages the request: request\\\\\\\\nrequest = Request(url)\\\\\\\\n\\\\\\\\n# Sends the request and catches the response: response\\\\\\\\nresponse = urlopen(request)\\\\\\\\n\\\\\\\\n# Print the datatype of response\\\\\\\\nprint(type(response))\\\\\\\\n\\\\\\\\n# Be polite and close the response!\\\\\\\\nresponse.close()\\\\\\\\n&quot;,&quot;^1Y&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;To import two functions in one line, import the first function as usual and add a comma &lt;code&gt;,&lt;/code&gt; followed by the second function.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Pass the &lt;em&gt;url&lt;/em&gt; (already in the &lt;code&gt;url&lt;/code&gt; object defined) as an argument to &lt;code&gt;Request()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Pass &lt;code&gt;request&lt;/code&gt; as an argument to &lt;code&gt;urlopen()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You don&#39;t have to modify the code for printing the datatype of &lt;code&gt;response&lt;/code&gt; and closing the connection.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;^1Z&quot;,null,&quot;xp&quot;,100,&quot;^1[&quot;,[],&quot;^20&quot;,[],&quot;^21&quot;,&quot;&quot;,&quot;^26&quot;,&quot;python&quot;,&quot;^2:&quot;,0.1300355334847203,&quot;^2;&quot;,42711],[&quot;^ &quot;,&quot;id&quot;,42712,&quot;^R&quot;,&quot;NormalExercise&quot;,&quot;^1T&quot;,&quot;&lt;p&gt;You have just packaged and sent a GET request to &lt;code&gt;\\\\\\\\&quot;https://campus.datacamp.com/courses/1606/4135?ex=2\\\\\\\\&quot;&lt;/code&gt; and then caught the response. You saw that such a response is a &lt;code&gt;http.client.HTTPResponse&lt;/code&gt; object. The question remains: what can you do with this response?&lt;/p&gt;\\\\\\\\n&lt;p&gt;Well, as it came from an HTML page, you could &lt;em&gt;read&lt;/em&gt; it to extract the HTML and, in fact, such a &lt;code&gt;http.client.HTTPResponse&lt;/code&gt; object has an associated &lt;code&gt;read()&lt;/code&gt; method. In this exercise, you&#39;ll build on your previous great work to extract the response and print the HTML.&lt;/p&gt;&quot;,&quot;^E&quot;,&quot;Printing HTTP request results in Python using urllib&quot;,&quot;^1U&quot;,&quot;# Import packages\\\\\\\\nfrom urllib.request import urlopen, Request\\\\\\\\n\\\\\\\\n# Specify the url\\\\\\\\nurl = \\\\\\\\&quot;https://campus.datacamp.com/courses/1606/4135?ex=2\\\\\\\\&quot;\\\\\\\\n\\\\\\\\n# This packages the request\\\\\\\\nrequest = Request(url)\\\\\\\\n\\\\\\\\n# Sends the request and catches the response: response\\\\\\\\n\\\\\\\\n\\\\\\\\n# Extract the response: html\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print the html\\\\\\\\n\\\\\\\\n\\\\\\\\n# Be polite and close the response!\\\\\\\\nresponse.close()&quot;,&quot;^1V&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Send the request and catch the response in the variable &lt;code&gt;response&lt;/code&gt; with the function &lt;code&gt;urlopen()&lt;/code&gt;, as in the previous exercise.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Extract the response using the &lt;code&gt;read()&lt;/code&gt; method and store the result in the variable &lt;code&gt;html&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Print the string &lt;code&gt;html&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Hit submit to perform all of the above and to close the response: be tidy!&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;^1L&quot;,7,&quot;sct&quot;,&quot;\\\\\\\\n# Test: Predefined code\\\\\\\\npredef_msg = \\\\\\\\&quot;You don&#39;t have to change any of the predefined code.\\\\\\\\&quot;\\\\\\\\nEx().has_import(\\\\\\\\n    \\\\\\\\&quot;urllib.request.urlopen\\\\\\\\&quot;,\\\\\\\\n    not_imported_msg=predef_msg\\\\\\\\n)\\\\\\\\n\\\\\\\\nEx().has_import(\\\\\\\\n    \\\\\\\\&quot;urllib.request.Request\\\\\\\\&quot;,\\\\\\\\n    not_imported_msg=predef_msg\\\\\\\\n)\\\\\\\\n\\\\\\\\nEx().check_object(\\\\\\\\&quot;url\\\\\\\\&quot;).has_equal_value()\\\\\\\\n\\\\\\\\n# Test: call to Request() and &#39;request&#39; variable\\\\\\\\nEx().check_function(\\\\\\\\&quot;urllib.request.Request\\\\\\\\&quot;).check_args(0).has_equal_value()\\\\\\\\nEx().check_object(\\\\\\\\&quot;request\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\n# Test: call to urlopen() and &#39;response&#39; variable\\\\\\\\nEx().check_function(\\\\\\\\&quot;urllib.request.urlopen\\\\\\\\&quot;).check_args(0).has_equal_ast()\\\\\\\\nEx().check_object(\\\\\\\\&quot;response\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\n# Test: call to urlopen() and &#39;response&#39; variable\\\\\\\\nEx().check_function(\\\\\\\\&quot;response.read\\\\\\\\&quot;)\\\\\\\\nEx().check_object(\\\\\\\\&quot;html\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\n# Test: call to print()\\\\\\\\nEx().check_function(&#39;print&#39;).check_args(0).has_equal_ast()\\\\\\\\n\\\\\\\\n# Test: Predefined code\\\\\\\\nEx().check_function(\\\\\\\\&quot;response.close\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\nsuccess_msg(\\\\\\\\&quot;Awesome!\\\\\\\\&quot;)\\\\\\\\n&quot;,&quot;^1W&quot;,&quot;&quot;,&quot;^1X&quot;,&quot;# Import packages\\\\\\\\nfrom urllib.request import urlopen, Request\\\\\\\\n\\\\\\\\n# Specify the url\\\\\\\\nurl = \\\\\\\\&quot;https://campus.datacamp.com/courses/1606/4135?ex=2\\\\\\\\&quot;\\\\\\\\n\\\\\\\\n# This packages the request\\\\\\\\nrequest = Request(url)\\\\\\\\n\\\\\\\\n# Sends the request and catches the response: response\\\\\\\\nresponse = urlopen(request)\\\\\\\\n\\\\\\\\n# Extract the response: html\\\\\\\\nhtml = response.read()\\\\\\\\n\\\\\\\\n# Print the html\\\\\\\\nprint(html)\\\\\\\\n\\\\\\\\n# Be polite and close the response!\\\\\\\\nresponse.close()&quot;,&quot;^1Y&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Pass &lt;code&gt;request&lt;/code&gt; as an argument to &lt;code&gt;urlopen()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Apply the method &lt;code&gt;read()&lt;/code&gt; to the response object &lt;code&gt;response&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Simply pass &lt;code&gt;html&lt;/code&gt; to the &lt;code&gt;print()&lt;/code&gt; function.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You don&#39;t have to modify the code for closing the response.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;^1Z&quot;,null,&quot;xp&quot;,100,&quot;^1[&quot;,[],&quot;^20&quot;,[],&quot;^21&quot;,&quot;&quot;,&quot;^26&quot;,&quot;python&quot;,&quot;^2:&quot;,0.126809206142678,&quot;^2;&quot;,42712],[&quot;^ &quot;,&quot;id&quot;,42713,&quot;^R&quot;,&quot;NormalExercise&quot;,&quot;^1T&quot;,&quot;&lt;p&gt;Now that you&#39;ve got your head and hands around making HTTP requests using the urllib package, you&#39;re going to figure out how to do the same using the higher-level requests library. You&#39;ll once again be pinging DataCamp servers for their &lt;code&gt;\\\\\\\\&quot;http://www.datacamp.com/teach/documentation\\\\\\\\&quot;&lt;/code&gt; page.&lt;/p&gt;\\\\\\\\n&lt;p&gt;Note that unlike in the previous exercises using urllib, you don&#39;t have to close the connection when using requests!&lt;/p&gt;&quot;,&quot;^E&quot;,&quot;Performing HTTP requests in Python using requests&quot;,&quot;^1U&quot;,&quot;# Import package\\\\\\\\n\\\\\\\\n\\\\\\\\n# Specify the url: url\\\\\\\\n\\\\\\\\n\\\\\\\\n# Packages the request, send the request and catch the response: r\\\\\\\\n\\\\\\\\n\\\\\\\\n# Extract the response: text\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print the html\\\\\\\\nprint(text)&quot;,&quot;^1V&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Import the package &lt;code&gt;requests&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Assign the URL of interest to the variable &lt;code&gt;url&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Package the request to the URL, send the request and catch the response with a single function &lt;code&gt;requests.get()&lt;/code&gt;, assigning the response to the variable &lt;code&gt;r&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Use the &lt;code&gt;text&lt;/code&gt; attribute of the object &lt;code&gt;r&lt;/code&gt; to return the HTML of the webpage as a string; store the result in a variable &lt;code&gt;text&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Hit submit to print the HTML of the webpage.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;^1L&quot;,8,&quot;sct&quot;,&quot;\\\\\\\\n# Test: import requests\\\\\\\\nEx().has_import(\\\\\\\\&quot;requests\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\n# Test: &#39;url&#39; variable\\\\\\\\nEx().check_object(\\\\\\\\&quot;url\\\\\\\\&quot;).has_equal_value()\\\\\\\\n\\\\\\\\n# Test: call to requests.get() and &#39;r&#39; variable\\\\\\\\nEx().check_function(\\\\\\\\&quot;requests.get\\\\\\\\&quot;).check_args(0).has_equal_value()\\\\\\\\nEx().check_object(\\\\\\\\&quot;r\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\n# Test: &#39;text&#39; variable\\\\\\\\nEx().has_code(\\\\\\\\&quot;r.text\\\\\\\\&quot;, pattern = False, not_typed_msg=\\\\\\\\&quot;Have you used `r.text` to create `text`?\\\\\\\\&quot;)\\\\\\\\nEx().check_object(\\\\\\\\&quot;text\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\n# Test: Predefined code\\\\\\\\nEx().check_function(&#39;print&#39;).check_args(0).has_equal_ast()\\\\\\\\n\\\\\\\\nsuccess_msg(\\\\\\\\&quot;Awesome!\\\\\\\\&quot;)\\\\\\\\n&quot;,&quot;^1W&quot;,&quot;&quot;,&quot;^1X&quot;,&quot;# Import package\\\\\\\\nimport requests\\\\\\\\n\\\\\\\\n# Specify the url: url\\\\\\\\nurl = \\\\\\\\&quot;http://www.datacamp.com/teach/documentation\\\\\\\\&quot;\\\\\\\\n\\\\\\\\n# Packages the request, send the request and catch the response: r\\\\\\\\nr = requests.get(url)\\\\\\\\n\\\\\\\\n# Extract the response: text\\\\\\\\ntext = r.text\\\\\\\\n\\\\\\\\n# Print the html\\\\\\\\nprint(text)&quot;,&quot;^1Y&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;To import a package &lt;code&gt;x&lt;/code&gt;, execute &lt;code&gt;import x&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Did you type in the URL correctly?&lt;/li&gt;\\\\\\\\n&lt;li&gt;Pass the &lt;em&gt;url&lt;/em&gt; (the &lt;code&gt;url&lt;/code&gt; object you defined) as an argument to &lt;code&gt;requests.get()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You can access the &lt;code&gt;text&lt;/code&gt; attribute of the object &lt;code&gt;r&lt;/code&gt; by executing &lt;code&gt;r.text&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You don&#39;t have to modify the code for printing the HTML of the webpage.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;^1Z&quot;,null,&quot;xp&quot;,100,&quot;^1[&quot;,[],&quot;^20&quot;,[],&quot;^21&quot;,&quot;&quot;,&quot;^26&quot;,&quot;python&quot;,&quot;^2:&quot;,0.9953768252991049,&quot;^2;&quot;,42713],[&quot;^ &quot;,&quot;id&quot;,990670,&quot;^R&quot;,&quot;VideoExercise&quot;,&quot;^1T&quot;,null,&quot;^E&quot;,&quot;Scraping the web in Python&quot;,&quot;^1U&quot;,&quot;&quot;,&quot;^1V&quot;,null,&quot;^1L&quot;,9,&quot;sct&quot;,&quot;&quot;,&quot;^1W&quot;,&quot;&quot;,&quot;^1X&quot;,&quot;&quot;,&quot;^1Y&quot;,null,&quot;^1Z&quot;,null,&quot;xp&quot;,50,&quot;^1[&quot;,[],&quot;^20&quot;,[],&quot;^21&quot;,&quot;&quot;,&quot;^22&quot;,null,&quot;^23&quot;,null,&quot;^24&quot;,56.25,&quot;^25&quot;,&quot;course_1606_9d1f8a331d1200c7e1bdbfcaf3a7a491&quot;,&quot;key&quot;,&quot;da43858012&quot;,&quot;^26&quot;,&quot;python&quot;,&quot;^27&quot;,1606,&quot;^28&quot;,4135,&quot;^14&quot;,null,&quot;^29&quot;,&quot;v0&quot;,&quot;^2:&quot;,0.57087711082384,&quot;^2;&quot;,990670],[&quot;^ &quot;,&quot;id&quot;,42715,&quot;^R&quot;,&quot;NormalExercise&quot;,&quot;^1T&quot;,&quot;&lt;p&gt;In this interactive exercise, you&#39;ll learn how to use the BeautifulSoup package to &lt;em&gt;parse&lt;/em&gt;, &lt;em&gt;prettify&lt;/em&gt; and &lt;em&gt;extract&lt;/em&gt; information from HTML. You&#39;ll scrape the data from the webpage of Guido van Rossum, Python&#39;s very own &lt;a href=\\\\\\\\&quot;https://en.wikipedia.org/wiki/Benevolent_dictator_for_life\\\\\\\\&quot;&gt;Benevolent Dictator for Life&lt;/a&gt;. In the following exercises, you&#39;ll prettify the HTML and then extract the text and the hyperlinks.&lt;/p&gt;\\\\\\\\n&lt;p&gt;The URL of interest is &lt;code&gt;url = &#39;https://www.python.org/~guido/&#39;&lt;/code&gt;.&lt;/p&gt;&quot;,&quot;^E&quot;,&quot;Parsing HTML with BeautifulSoup&quot;,&quot;^1U&quot;,&quot;# Import packages\\\\\\\\nimport requests\\\\\\\\nfrom ____ import ____\\\\\\\\n\\\\\\\\n# Specify url: url\\\\\\\\n\\\\\\\\n\\\\\\\\n# Package the request, send the request and catch the response: r\\\\\\\\n\\\\\\\\n\\\\\\\\n# Extracts the response as html: html_doc\\\\\\\\n\\\\\\\\n\\\\\\\\n# Create a BeautifulSoup object from the HTML: soup\\\\\\\\n\\\\\\\\n\\\\\\\\n# Prettify the BeautifulSoup object: pretty_soup\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print the response\\\\\\\\nprint(pretty_soup)&quot;,&quot;^1V&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Import the function &lt;code&gt;BeautifulSoup&lt;/code&gt; from the package &lt;code&gt;bs4&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Assign the URL of interest to the variable &lt;code&gt;url&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Package the request to the URL, send the request and catch the response with a single function &lt;code&gt;requests.get()&lt;/code&gt;, assigning the response to the variable &lt;code&gt;r&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Use the &lt;code&gt;text&lt;/code&gt; attribute of the object &lt;code&gt;r&lt;/code&gt; to return the HTML of the webpage as a string; store the result in a variable &lt;code&gt;html_doc&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Create a BeautifulSoup object &lt;code&gt;soup&lt;/code&gt; from the resulting HTML using the function &lt;code&gt;BeautifulSoup()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Use the method &lt;code&gt;prettify()&lt;/code&gt; on &lt;code&gt;soup&lt;/code&gt; and assign the result to &lt;code&gt;pretty_soup&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Hit submit to print to prettified HTML to your shell!&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;^1L&quot;,10,&quot;sct&quot;,&quot;# Test: Predefined code\\\\\\\\npredef_msg = \\\\\\\\&quot;You don&#39;t have to change any of the predefined code.\\\\\\\\&quot;\\\\\\\\nEx().has_import(\\\\\\\\n    \\\\\\\\&quot;requests\\\\\\\\&quot;,\\\\\\\\n    not_imported_msg=predef_msg\\\\\\\\n)\\\\\\\\n\\\\\\\\n# Test: import BeautifulSoup\\\\\\\\nimport_msg = \\\\\\\\&quot;Did you correctly import the required packages?\\\\\\\\&quot;\\\\\\\\nEx().has_import(\\\\\\\\n    \\\\\\\\&quot;bs4.BeautifulSoup\\\\\\\\&quot;,\\\\\\\\n    not_imported_msg=import_msg\\\\\\\\n)\\\\\\\\n\\\\\\\\n# Test: &#39;url&#39; variable\\\\\\\\nEx().check_object(\\\\\\\\&quot;url\\\\\\\\&quot;).has_equal_value()\\\\\\\\n\\\\\\\\n# Test: call to requests.get() and &#39;r&#39; variable\\\\\\\\nEx().check_function(\\\\\\\\&quot;requests.get\\\\\\\\&quot;).check_args(0).has_equal_value()\\\\\\\\nEx().check_object(\\\\\\\\&quot;r\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\n\\\\\\\\n# Test: &#39;html_doc&#39; variable\\\\\\\\nEx().check_correct(\\\\\\\\n  check_object(\\\\\\\\&quot;html_doc\\\\\\\\&quot;).has_equal_value(),\\\\\\\\n  has_code(\\\\\\\\&quot;r.text\\\\\\\\&quot;, pattern = False, not_typed_msg=\\\\\\\\&quot;Have you used `r.text` to create `html_doc`?\\\\\\\\&quot;)\\\\\\\\n)\\\\\\\\n\\\\\\\\n# Test: call to BeautifulSoup() and &#39;soup&#39; variable\\\\\\\\nEx().check_correct(\\\\\\\\n  check_object(\\\\\\\\&quot;soup\\\\\\\\&quot;).has_equal_value(),\\\\\\\\n  check_function(\\\\\\\\&quot;bs4.BeautifulSoup\\\\\\\\&quot;).check_args(0).has_equal_value()\\\\\\\\n  )\\\\\\\\n\\\\\\\\n# Test: call to prettify() and &#39;pretty_soup&#39; variable\\\\\\\\nEx().check_correct(\\\\\\\\n  check_object(\\\\\\\\&quot;pretty_soup\\\\\\\\&quot;).has_equal_value(),\\\\\\\\n  check_function(\\\\\\\\&quot;soup.prettify\\\\\\\\&quot;)\\\\\\\\n  )\\\\\\\\n\\\\\\\\n# Test: Predefined code\\\\\\\\nEx().has_printout(0)\\\\\\\\n\\\\\\\\nsuccess_msg(\\\\\\\\&quot;Awesome!\\\\\\\\&quot;)\\\\\\\\n&quot;,&quot;^1W&quot;,&quot;&quot;,&quot;^1X&quot;,&quot;# Import packages\\\\\\\\nimport requests\\\\\\\\nfrom bs4 import BeautifulSoup\\\\\\\\n\\\\\\\\n# Specify url: url\\\\\\\\nurl = &#39;https://www.python.org/~guido/&#39;\\\\\\\\n\\\\\\\\n# Package the request, send the request and catch the response: r\\\\\\\\nr = requests.get(url)\\\\\\\\n\\\\\\\\n# Extracts the response as html: html_doc\\\\\\\\nhtml_doc = r.text\\\\\\\\n\\\\\\\\n# Create a BeautifulSoup object from the HTML: soup\\\\\\\\nsoup = BeautifulSoup(html_doc)\\\\\\\\n\\\\\\\\n# Prettify the BeautifulSoup object: pretty_soup\\\\\\\\npretty_soup = soup.prettify()\\\\\\\\n\\\\\\\\n# Print the response\\\\\\\\nprint(pretty_soup)&quot;,&quot;^1Y&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;To import a function &lt;code&gt;y&lt;/code&gt; from a package &lt;code&gt;x&lt;/code&gt;, execute &lt;code&gt;from x import y&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Check the URL to make sure that you typed it in correctly.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Pass the &lt;em&gt;url&lt;/em&gt; (the &lt;code&gt;url&lt;/code&gt; object you defined) as an argument to &lt;code&gt;requests.get()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You can access the &lt;code&gt;text&lt;/code&gt; attribute of the object &lt;code&gt;r&lt;/code&gt; by executing &lt;code&gt;r.text&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Pass the extracted &lt;em&gt;HTML&lt;/em&gt; as an argument to &lt;code&gt;BeautifulSoup()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;To use the &lt;code&gt;prettify()&lt;/code&gt; method on the BeautifulSoup object &lt;code&gt;soup&lt;/code&gt;, execute &lt;code&gt;soup.prettify()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You don&#39;t have to modify the code to print the prettified HTML.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;^1Z&quot;,null,&quot;xp&quot;,100,&quot;^1[&quot;,[],&quot;^20&quot;,[],&quot;^21&quot;,&quot;&quot;,&quot;^26&quot;,&quot;python&quot;,&quot;^2:&quot;,0.36228885515222475,&quot;^2;&quot;,42715],[&quot;^ &quot;,&quot;id&quot;,42716,&quot;^R&quot;,&quot;NormalExercise&quot;,&quot;^1T&quot;,&quot;&lt;p&gt;As promised, in the following exercises, you&#39;ll learn the basics of extracting information from HTML soup. In this exercise, you&#39;ll figure out how to extract the text from the BDFL&#39;s webpage, along with printing the webpage&#39;s title.&lt;/p&gt;&quot;,&quot;^E&quot;,&quot;Turning a webpage into data using BeautifulSoup: getting the text&quot;,&quot;^1U&quot;,&quot;# Import packages\\\\\\\\nimport requests\\\\\\\\nfrom bs4 import BeautifulSoup\\\\\\\\n\\\\\\\\n# Specify url: url\\\\\\\\nurl = &#39;https://www.python.org/~guido/&#39;\\\\\\\\n\\\\\\\\n# Package the request, send the request and catch the response: r\\\\\\\\nr = requests.get(url)\\\\\\\\n\\\\\\\\n# Extract the response as html: html_doc\\\\\\\\nhtml_doc = r.text\\\\\\\\n\\\\\\\\n# Create a BeautifulSoup object from the HTML: soup\\\\\\\\n\\\\\\\\n\\\\\\\\n# Get the title of Guido&#39;s webpage: guido_title\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print the title of Guido&#39;s webpage to the shell\\\\\\\\n\\\\\\\\n\\\\\\\\n# Get Guido&#39;s text: guido_text\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print Guido&#39;s text to the shell\\\\\\\\nprint(guido_text)&quot;,&quot;^1V&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;In the sample code, the HTML response object &lt;code&gt;html_doc&lt;/code&gt; has already been created: your first task is to Soupify it using the function &lt;code&gt;BeautifulSoup()&lt;/code&gt; and to assign the resulting soup to the variable &lt;code&gt;soup&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Extract the title from the HTML soup &lt;code&gt;soup&lt;/code&gt; using the attribute &lt;code&gt;title&lt;/code&gt; and assign the result to &lt;code&gt;guido_title&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Print the title of Guido&#39;s webpage to the shell using the &lt;code&gt;print()&lt;/code&gt; function.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Extract the text from the HTML soup &lt;code&gt;soup&lt;/code&gt; using the method &lt;code&gt;get_text()&lt;/code&gt; and assign to &lt;code&gt;guido_text&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Hit submit to print the text from Guido&#39;s webpage to the shell.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;^1L&quot;,11,&quot;sct&quot;,&quot;# Test: Predefined code\\\\\\\\npredef_msg = \\\\\\\\&quot;You don&#39;t have to change any of the predefined code.\\\\\\\\&quot;\\\\\\\\nEx().has_import(\\\\\\\\n    \\\\\\\\&quot;requests\\\\\\\\&quot;,\\\\\\\\n    not_imported_msg=predef_msg\\\\\\\\n)\\\\\\\\n\\\\\\\\n# Test: import BeautifulSoup\\\\\\\\nEx().has_import(\\\\\\\\n    \\\\\\\\&quot;bs4.BeautifulSoup\\\\\\\\&quot;,\\\\\\\\n    not_imported_msg=predef_msg\\\\\\\\n)\\\\\\\\n\\\\\\\\n# Test: &#39;url&#39; variable\\\\\\\\nEx().check_object(\\\\\\\\&quot;url\\\\\\\\&quot;).has_equal_value()\\\\\\\\n\\\\\\\\n# Test: call to requests.get() and &#39;r&#39; variable\\\\\\\\nEx().check_function(\\\\\\\\&quot;requests.get\\\\\\\\&quot;).check_args(0).has_equal_value()\\\\\\\\nEx().check_object(\\\\\\\\&quot;r\\\\\\\\&quot;)\\\\\\\\n\\\\\\\\n\\\\\\\\n# Test: &#39;html_doc&#39; variable\\\\\\\\nEx().check_correct(\\\\\\\\n  check_object(\\\\\\\\&quot;html_doc\\\\\\\\&quot;).has_equal_value(),\\\\\\\\n  has_code(\\\\\\\\&quot;r.text\\\\\\\\&quot;, pattern = False, not_typed_msg=\\\\\\\\&quot;Have you used `r.text` to create `html_doc`?\\\\\\\\&quot;)\\\\\\\\n)\\\\\\\\n\\\\\\\\n# Test: call to BeautifulSoup() and &#39;soup&#39; variable\\\\\\\\nEx().check_correct(\\\\\\\\n  check_object(\\\\\\\\&quot;soup\\\\\\\\&quot;).has_equal_value(),\\\\\\\\n  check_function(\\\\\\\\&quot;bs4.BeautifulSoup\\\\\\\\&quot;).check_args(0).has_equal_value()\\\\\\\\n  )\\\\\\\\n\\\\\\\\n# Test: &#39;guido_title&#39; variable\\\\\\\\nEx().check_correct(\\\\\\\\n  check_object(\\\\\\\\&quot;guido_title\\\\\\\\&quot;).has_equal_value(),\\\\\\\\n  has_code(\\\\\\\\&quot;soup.title\\\\\\\\&quot;, pattern = False, not_typed_msg=\\\\\\\\&quot;Have you used `soup.title` to create `guido_title`?\\\\\\\\&quot;)\\\\\\\\n)\\\\\\\\n\\\\\\\\n# Test: call to print()\\\\\\\\nEx().has_printout(0)\\\\\\\\n\\\\\\\\n# Test: call to soup.get_text() and &#39;guido_text&#39; variable\\\\\\\\nEx().check_correct(\\\\\\\\n  check_object(\\\\\\\\&quot;guido_text\\\\\\\\&quot;).has_equal_value(),\\\\\\\\n  check_function(\\\\\\\\&quot;soup.get_text\\\\\\\\&quot;)\\\\\\\\n  )\\\\\\\\n\\\\\\\\n# Test: Predefined code\\\\\\\\nEx().has_printout(1)\\\\\\\\n\\\\\\\\nsuccess_msg(\\\\\\\\&quot;Awesome!\\\\\\\\&quot;)\\\\\\\\n&quot;,&quot;^1W&quot;,&quot;&quot;,&quot;^1X&quot;,&quot;# Import packages\\\\\\\\nimport requests\\\\\\\\nfrom bs4 import BeautifulSoup\\\\\\\\n\\\\\\\\n# Specify url: url\\\\\\\\nurl = &#39;https://www.python.org/~guido/&#39;\\\\\\\\n\\\\\\\\n# Package the request, send the request and catch the response: r\\\\\\\\nr = requests.get(url)\\\\\\\\n\\\\\\\\n# Extract the response as html: html_doc\\\\\\\\nhtml_doc = r.text\\\\\\\\n\\\\\\\\n# Create a BeautifulSoup object from the HTML: soup\\\\\\\\nsoup = BeautifulSoup(html_doc)\\\\\\\\n\\\\\\\\n# Get the title of Guido&#39;s webpage: guido_title\\\\\\\\nguido_title = soup.title\\\\\\\\n\\\\\\\\n# Print the title of Guido&#39;s webpage to the shell\\\\\\\\nprint(guido_title)\\\\\\\\n\\\\\\\\n# Get Guido&#39;s text: guido_text\\\\\\\\nguido_text = soup.get_text()\\\\\\\\n\\\\\\\\n# Print Guido&#39;s text to the shell\\\\\\\\nprint(guido_text)&quot;,&quot;^1Y&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Pass the &lt;em&gt;HTML response object&lt;/em&gt; as an argument to &lt;code&gt;BeautifulSoup()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You can access the &lt;code&gt;title&lt;/code&gt; attribute of the object &lt;code&gt;soup&lt;/code&gt; by executing &lt;code&gt;soup.title&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;The object that contains the title of Guido&#39;s webpage is &lt;code&gt;guido_title&lt;/code&gt;; pass this as an argument to &lt;code&gt;print()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Use the method &lt;code&gt;get_text()&lt;/code&gt; on the HTML soup &lt;code&gt;soup&lt;/code&gt; by executing &lt;code&gt;soup.get_text()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;You don&#39;t have to modify the code to print the text from Guido&#39;s webpage.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;^1Z&quot;,null,&quot;xp&quot;,100,&quot;^1[&quot;,[],&quot;^20&quot;,[],&quot;^21&quot;,&quot;&quot;,&quot;^26&quot;,&quot;python&quot;,&quot;^2:&quot;,0.025243375882895602,&quot;^2;&quot;,42716],[&quot;^ &quot;,&quot;id&quot;,42717,&quot;^R&quot;,&quot;NormalExercise&quot;,&quot;^1T&quot;,&quot;&lt;p&gt;In this exercise, you&#39;ll figure out how to extract the URLs of the hyperlinks from the BDFL&#39;s webpage. In the process, you&#39;ll become close friends with the soup method &lt;code&gt;find_all()&lt;/code&gt;.&lt;/p&gt;&quot;,&quot;^E&quot;,&quot;Turning a webpage into data using BeautifulSoup: getting the hyperlinks&quot;,&quot;^1U&quot;,&quot;# Import packages\\\\\\\\nimport requests\\\\\\\\nfrom bs4 import BeautifulSoup\\\\\\\\n\\\\\\\\n# Specify url\\\\\\\\nurl = &#39;https://www.python.org/~guido/&#39;\\\\\\\\n\\\\\\\\n# Package the request, send the request and catch the response: r\\\\\\\\nr = requests.get(url)\\\\\\\\n\\\\\\\\n# Extracts the response as html: html_doc\\\\\\\\nhtml_doc = r.text\\\\\\\\n\\\\\\\\n# create a BeautifulSoup object from the HTML: soup\\\\\\\\nsoup = BeautifulSoup(html_doc)\\\\\\\\n\\\\\\\\n# Print the title of Guido&#39;s webpage\\\\\\\\nprint(soup.title)\\\\\\\\n\\\\\\\\n# Find all &#39;a&#39; tags (which define hyperlinks): a_tags\\\\\\\\n\\\\\\\\n\\\\\\\\n# Print the URLs to the shell\\\\\\\\nfor ____ in ____:\\\\\\\\n    ____&quot;,&quot;^1V&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Use the method &lt;code&gt;find_all()&lt;/code&gt; to find all hyperlinks in &lt;code&gt;soup&lt;/code&gt;, remembering that hyperlinks are defined by the HTML tag &lt;code&gt;&amp;lt;a&amp;gt;&lt;/code&gt; but passed to &lt;code&gt;find_all()&lt;/code&gt; without angle brackets; store the result in the variable &lt;code&gt;a_tags&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;The variable &lt;code&gt;a_tags&lt;/code&gt; is a results set: your job now is to enumerate over it, using a &lt;code&gt;for&lt;/code&gt; loop and to print the actual URLs of the hyperlinks; to do this, for every element &lt;code&gt;link&lt;/code&gt; in &lt;code&gt;a_tags&lt;/code&gt;, you want to &lt;code&gt;print()&lt;/code&gt; &lt;code&gt;link.get(&#39;href&#39;)&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;^1L&quot;,12,&quot;sct&quot;,&quot;predef_msg = \\\\\\\\&quot;You don&#39;t have to change any of the predefined code.\\\\\\\\&quot;\\\\\\\\nEx().has_import(\\\\\\\\&quot;requests\\\\\\\\&quot;)\\\\\\\\nEx().has_import(\\\\\\\\&quot;bs4.BeautifulSoup\\\\\\\\&quot;)\\\\\\\\nEx().check_object(\\\\\\\\&quot;url\\\\\\\\&quot;).has_equal_value(incorrect_msg = predef_msg)\\\\\\\\nEx().check_function(\\\\\\\\&quot;requests.get\\\\\\\\&quot;).check_args(0).has_equal_ast()\\\\\\\\nEx().check_object(\\\\\\\\&quot;html_doc\\\\\\\\&quot;).has_equal_value(incorrect_msg = predef_msg)\\\\\\\\nEx().check_object(\\\\\\\\&quot;soup\\\\\\\\&quot;).has_equal_value(incorrect_msg = predef_msg)\\\\\\\\nEx().has_printout(0)\\\\\\\\n\\\\\\\\nEx().check_correct(\\\\\\\\n    check_object(\\\\\\\\&quot;a_tags\\\\\\\\&quot;),\\\\\\\\n    check_function(\\\\\\\\&quot;soup.find_all\\\\\\\\&quot;).check_args(0).has_equal_value()\\\\\\\\n)\\\\\\\\nEx().check_for_loop().multi(\\\\\\\\n        check_iter().has_equal_value(incorrect_msg = \\\\\\\\&quot;You have to iterate over `a_tags`\\\\\\\\&quot;),\\\\\\\\n        check_body().set_context(&#39;&lt;a href=\\\\\\\\&quot;pics.html\\\\\\\\&quot;&gt;&lt;img border=\\\\\\\\&quot;0\\\\\\\\&quot; src=\\\\\\\\&quot;images/IMG_2192.jpg\\\\\\\\&quot;/&gt;&lt;/a&gt;&#39;).check_function(\\\\\\\\&quot;print\\\\\\\\&quot;).check_args(0).check_function(\\\\\\\\&quot;link.get\\\\\\\\&quot;).check_args(0).has_equal_value()\\\\\\\\n    )\\\\\\\\n\\\\\\\\nsuccess_msg(\\\\\\\\&quot;Awesome!\\\\\\\\&quot;)&quot;,&quot;^1W&quot;,&quot;&quot;,&quot;^1X&quot;,&quot;# Import packages\\\\\\\\nimport requests\\\\\\\\nfrom bs4 import BeautifulSoup\\\\\\\\n\\\\\\\\n# Specify url\\\\\\\\nurl = &#39;https://www.python.org/~guido/&#39;\\\\\\\\n\\\\\\\\n# Package the request, send the request and catch the response: r\\\\\\\\nr = requests.get(url)\\\\\\\\n\\\\\\\\n# Extracts the response as html: html_doc\\\\\\\\nhtml_doc = r.text\\\\\\\\n\\\\\\\\n# create a BeautifulSoup object from the HTML: soup\\\\\\\\nsoup = BeautifulSoup(html_doc)\\\\\\\\n\\\\\\\\n# Print the title of Guido&#39;s webpage\\\\\\\\nprint(soup.title)\\\\\\\\n\\\\\\\\n# Find all &#39;a&#39; tags (which define hyperlinks): a_tags\\\\\\\\na_tags = soup.find_all(&#39;a&#39;)\\\\\\\\n\\\\\\\\n# Print the URLs to the shell\\\\\\\\nfor link in a_tags:\\\\\\\\n    print(link.get(&#39;href&#39;))&quot;,&quot;^1Y&quot;,&quot;&lt;ul&gt;\\\\\\\\n&lt;li&gt;Pass the &lt;em&gt;HTML tag&lt;/em&gt; to find (without the angle brackets &lt;code&gt;&amp;lt;&amp;gt;&lt;/code&gt;) as a string argument to &lt;code&gt;find_all()&lt;/code&gt;.&lt;/li&gt;\\\\\\\\n&lt;li&gt;Recall that the &lt;code&gt;for&lt;/code&gt; loop recipe is: &lt;code&gt;for&lt;/code&gt; &lt;em&gt;loop variable&lt;/em&gt; &lt;code&gt;in&lt;/code&gt; &lt;em&gt;results set&lt;/em&gt;&lt;code&gt;:&lt;/code&gt;. Don&#39;t forget to pass &lt;code&gt;link.get(&#39;href&#39;)&lt;/code&gt; as an argument to &lt;code&gt;print()&lt;/code&gt; inside the &lt;code&gt;for&lt;/code&gt; loop body.&lt;/li&gt;\\\\\\\\n&lt;/ul&gt;&quot;,&quot;^1Z&quot;,null,&quot;xp&quot;,100,&quot;^1[&quot;,[],&quot;^20&quot;,[],&quot;^21&quot;,&quot;&quot;,&quot;^26&quot;,&quot;python&quot;,&quot;^2:&quot;,0.6835808699100132,&quot;^2;&quot;,42717]]]]],&quot;activeImage&quot;,[&quot;^0&quot;,[&quot;^ &quot;,&quot;n&quot;,&quot;PreFetchedRequestRecord&quot;,&quot;v&quot;,[&quot;^ &quot;,&quot;^C&quot;,&quot;SUCCESS&quot;,&quot;^D&quot;,&quot;course-1606-master:d5b250978170fe950a6ca0a26b6049af-20211209132516524&quot;]]],&quot;sharedImage&quot;,[&quot;^0&quot;,[&quot;^ &quot;,&quot;n&quot;,&quot;PreFetchedRequestRecord&quot;,&quot;v&quot;,[&quot;^ &quot;,&quot;^C&quot;,&quot;NOT_FETCHED&quot;,&quot;^D&quot;,null]]]]]],&quot;settings&quot;,[&quot;^2&quot;,[&quot;uiTheme&quot;,&quot;DARK&quot;,&quot;feedbackRatingStatus&quot;,&quot;NONE&quot;]],&quot;streakInfo&quot;,[&quot;^ &quot;,&quot;^R&quot;,&quot;StreakUnknown&quot;],&quot;systemStatus&quot;,[&quot;^2&quot;,[&quot;indicator&quot;,&quot;none&quot;,&quot;description&quot;,&quot;No status has been fetched from the Status Page.&quot;]],&quot;user&quot;,[&quot;^2&quot;,[&quot;status&quot;,&quot;not_initiate&quot;,&quot;settings&quot;,[&quot;^2&quot;,[]]]]]]]\";</script><div id=\"root\"><div class=\"theme progress-indicator--visible\"><style data-emotion=\"css p4fmi8\">.css-p4fmi8{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;background-color:#F7F3EB;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;height:50px;padding-left:10px;padding-right:10px;position:relative;z-index:15;}</style><header data-cy=\"alpa-navbar\" class=\"css-p4fmi8\"><style data-emotion=\"css yp9swi\">.css-yp9swi{-webkit-flex:1;-ms-flex:1;flex:1;}</style><div class=\"css-yp9swi\"><style data-emotion=\"css ew67gc\">.css-ew67gc{border:0;border-width:0;padding:6px;}</style><style data-emotion=\"css 1jcub2v\">.css-1jcub2v{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;border:0;border-radius:4px;border-style:solid;border-width:2px;cursor:pointer;display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;position:relative;-webkit-text-decoration:none;text-decoration:none;text-transform:capitalize;-webkit-transition:0.15s;transition:0.15s;vertical-align:baseline;white-space:nowrap;background-color:transparent;border-color:#05192D;color:#05192D;padding:8px;border:0;border-width:0;padding:6px;}.css-1jcub2v:active{-webkit-transform:perspective(1px) scale(0.975);-moz-transform:perspective(1px) scale(0.975);-ms-transform:perspective(1px) scale(0.975);transform:perspective(1px) scale(0.975);}.css-1jcub2v:disabled,.css-1jcub2v:hover:disabled,.css-1jcub2v:active:disabled{-webkit-transform:none;-moz-transform:none;-ms-transform:none;transform:none;}.css-1jcub2v:focus{outline:0;}.css-1jcub2v:hover{background-color:rgba(5, 25, 45, 0.15);border-color:#05192D;color:#05192D;}</style><a aria-label=\"landing\" class=\"css-1jcub2v\" data-cy=\"header-logo\" data-testid=\"alpa-navbar-logo\"><svg height=\"28\" viewbox=\"0 0 173 36\" width=\"134.55555555555554\" aria-hidden=\"true\" color=\"currentColor\" title=\"\"><g fill=\"none\" fill-rule=\"evenodd\"><path d=\"M42.56 27.1a5.694 5.694 0 110-11.39 5.694 5.694 0 010 11.39m5.704-20.623v8.853a8.334 8.334 0 100 12.148v1.836h2.632V6.477h-2.632zm73.28 20.622a5.694 5.694 0 110-11.389 5.694 5.694 0 010 11.39m8.333-5.695v-8.247h-2.63v2.172a8.334 8.334 0 100 12.148v1.836h2.631v-7.91h-.001zm20.987-7.634a1.296 1.296 0 011.109-.622h.507c1.075 0 1.947.872 1.947 1.947v14.218h-2.686V17.269c-1.239 2-5.674 9.25-7.003 11.424a1.296 1.296 0 01-1.108.62h-.548a1.298 1.298 0 01-1.298-1.297V17.238a1909.582 1909.582 0 00-7.31 11.954l-.074.122h-2.574v-16.16h2.684v.033l-.062 11.147 6.438-10.56a1.3 1.3 0 011.11-.622h.51c1.073 0 1.944.869 1.947 1.942 0 2.972.014 8.383.014 9.17l6.397-10.493zm-37.92 12.541a8.331 8.331 0 11.21-9.502l-2.524 1.312a5.533 5.533 0 10-.379 6.88l2.693 1.31zm51.542.8a5.693 5.693 0 01-5.68-5.352v-.682a5.694 5.694 0 115.684 6.036m0-14.028a8.298 8.298 0 00-5.684 2.24v-2.168h-2.632V35.91h2.632v-8.4a8.333 8.333 0 105.684-14.425M75.277 15.68v9.938c0 .589.478 1.067 1.067 1.067h3.064v2.629h-3.062a3.7 3.7 0 01-3.696-3.696l-.01-9.938h-2.838v-2.56h2.838V8.702h2.635v4.428h4.672v2.55h-4.67zm12.757 11.418a5.694 5.694 0 110-11.39 5.694 5.694 0 010 11.39m5.702-13.941v2.173a8.334 8.334 0 100 12.148v1.836h2.632v-16.16l-2.632.003zM60.285 27.099a5.694 5.694 0 110-11.389 5.694 5.694 0 010 11.39m5.702-13.942v2.172a8.334 8.334 0 100 12.148v1.836h2.63v-16.16l-2.63.004z\" fill=\"#05192D\"/><path d=\"M11.7 8.514v8.332L2.857 21.89V3.44l8.841 5.074zm2.86 17.507v-7.51l11.84-6.757-2.88-1.65-8.96 5.112V7.68a1.44 1.44 0 00-.718-1.242L3.056.256A2.066 2.066 0 000 2.07v21.184a2.067 2.067 0 002.971 1.866l.082-.042 8.64-4.932v6.72c.002.513.276.987.721 1.243L23.502 34.4l2.88-1.651L14.56 26.02z\" fill=\"#05192D\"/></g></svg></a></div><style data-emotion=\"css 1f915o0\">.css-1f915o0{-webkit-box-pack:\\'initial\\';-ms-flex-pack:\\'initial\\';-webkit-justify-content:\\'initial\\';justify-content:\\'initial\\';}</style><div class=\"css-1f915o0\"><div class=\"dc-nav-course__container\"><style data-emotion=\"css 1nxd4b6\">.css-1nxd4b6{border:0;height:36px;-webkit-box-pack:initial;-ms-flex-pack:initial;-webkit-justify-content:initial;justify-content:initial;}</style><nav class=\"dc-nav-course css-1nxd4b6\"><style data-emotion=\"css ftus1d\">.css-ftus1d{z-index:1;border-width:2px;border-radius:4px 0px 0px 4px;}</style><style data-emotion=\"css 1sdywd0\">.css-1sdywd0{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;border:0;border-radius:4px;border-style:solid;border-width:2px;cursor:pointer;display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;position:relative;-webkit-text-decoration:none;text-decoration:none;text-transform:capitalize;-webkit-transition:0.15s;transition:0.15s;vertical-align:baseline;white-space:nowrap;background-color:transparent;border-color:#05192D;color:#05192D;padding:8px;z-index:1;border-width:2px;border-radius:4px 0px 0px 4px;}.css-1sdywd0:active{-webkit-transform:perspective(1px) scale(0.975);-moz-transform:perspective(1px) scale(0.975);-ms-transform:perspective(1px) scale(0.975);transform:perspective(1px) scale(0.975);}.css-1sdywd0:disabled,.css-1sdywd0:hover:disabled,.css-1sdywd0:active:disabled{-webkit-transform:none;-moz-transform:none;-ms-transform:none;transform:none;}.css-1sdywd0:focus{outline:0;}.css-1sdywd0:hover{background-color:rgba(5, 25, 45, 0.15);border-color:#05192D;color:#05192D;}</style><a aria-label=\"Go to previous exercise\" class=\"css-1sdywd0\" href=\"/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=1\" data-cy=\"header-previous\"><svg viewbox=\"0 0 18 18\" aria-hidden=\"true\" height=\"16\" role=\"img\" width=\"16\"><path fill=\"currentColor\" d=\"M4.42 8L16 7.998a1 1 0 010 2L4.41 10l3.285 3.296a.998.998 0 11-1.417 1.41l-4.93-4.948A.998.998 0 011.36 8.23l4.933-4.938a1 1 0 011.414 0c.39.391.39 1.025 0 1.416L4.42 7.999z\" fill-rule=\"evenodd\"/></svg></a><style data-emotion=\"css 96nxkt\">.css-96nxkt{border-radius:0;margin:0 -2px;border-width:2px;}</style><style data-emotion=\"css b29ve4\">.css-b29ve4{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;border:0;border-radius:4px;border-style:solid;border-width:2px;cursor:pointer;display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;position:relative;-webkit-text-decoration:none;text-decoration:none;text-transform:capitalize;-webkit-transition:0.15s;transition:0.15s;vertical-align:baseline;white-space:nowrap;background-color:transparent;border-color:#05192D;color:#05192D;padding:0 15px;border-radius:0;margin:0 -2px;border-width:2px;}.css-b29ve4:active{-webkit-transform:perspective(1px) scale(0.975);-moz-transform:perspective(1px) scale(0.975);-ms-transform:perspective(1px) scale(0.975);transform:perspective(1px) scale(0.975);}.css-b29ve4:disabled,.css-b29ve4:hover:disabled,.css-b29ve4:active:disabled{-webkit-transform:none;-moz-transform:none;-ms-transform:none;transform:none;}.css-b29ve4:focus{outline:0;}.css-b29ve4:hover{background-color:rgba(5, 25, 45, 0.15);border-color:#05192D;color:#05192D;}</style><button class=\"css-b29ve4\" type=\"button\" data-cy=\"header-outline\"><svg viewbox=\"0 0 18 18\" aria-hidden=\"true\" height=\"16\" role=\"img\" width=\"16\"><path fill=\"currentColor\" d=\"M4 6a1 1 0 110-2h10a1 1 0 010 2H4zm0 4a1 1 0 110-2h10a1 1 0 010 2H4zm0 4a1 1 0 010-2h10a1 1 0 010 2H4z\" fill-rule=\"evenodd\"/></svg><style data-emotion=\"css aib9ji\">.css-aib9ji{font-size:14px;line-height:32px;color:#05192D;font-weight:bold;margin-left:8px;}</style><style data-emotion=\"css vvk465\">.css-vvk465{-webkit-font-smoothing:antialiased;color:#05192D;font-family:Studio-Feixen-Sans,Arial,sans-serif;font-style:normal;font-size:14px;font-weight:400;font-size:14px;line-height:32px;color:#05192D;font-weight:bold;margin-left:8px;}</style><span class=\"css-vvk465\">Course Outline</span></button><style data-emotion=\"css q5k7z8\">.css-q5k7z8{z-index:1;border-width:2px;border-radius:0px 4px 4px 0px;}</style><style data-emotion=\"css 11zm6tc\">.css-11zm6tc{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;border:0;border-radius:4px;border-style:solid;border-width:2px;cursor:pointer;display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;position:relative;-webkit-text-decoration:none;text-decoration:none;text-transform:capitalize;-webkit-transition:0.15s;transition:0.15s;vertical-align:baseline;white-space:nowrap;background-color:transparent;border-color:#05192D;color:#05192D;padding:8px;z-index:1;border-width:2px;border-radius:0px 4px 4px 0px;}.css-11zm6tc:active{-webkit-transform:perspective(1px) scale(0.975);-moz-transform:perspective(1px) scale(0.975);-ms-transform:perspective(1px) scale(0.975);transform:perspective(1px) scale(0.975);}.css-11zm6tc:disabled,.css-11zm6tc:hover:disabled,.css-11zm6tc:active:disabled{-webkit-transform:none;-moz-transform:none;-ms-transform:none;transform:none;}.css-11zm6tc:focus{outline:0;}.css-11zm6tc:hover{background-color:rgba(5, 25, 45, 0.15);border-color:#05192D;color:#05192D;}</style><a aria-label=\"Go to next exercise\" class=\"css-11zm6tc\" href=\"/courses/intermediate-importing-data-in-python/importing-data-from-the-internet-1?ex=3\" data-cy=\"header-next\"><svg viewbox=\"0 0 18 18\" aria-hidden=\"true\" height=\"16\" role=\"img\" width=\"16\"><path fill=\"currentColor\" d=\"M13.58 10L2 10.002a1 1 0 010-2L13.59 8l-3.285-3.296a.998.998 0 111.417-1.41l4.93 4.948a.998.998 0 01-.012 1.527l-4.933 4.938a1 1 0 01-1.414 0 1.002 1.002 0 010-1.416l3.287-3.29z\" fill-rule=\"evenodd\"/></svg></a></nav></div></div><style data-emotion=\"css s01fge\">.css-s01fge{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex:1;-ms-flex:1;flex:1;-webkit-box-pack:end;-ms-flex-pack:end;-webkit-justify-content:flex-end;justify-content:flex-end;}</style><nav class=\"css-s01fge\"><style data-emotion=\"css 1dskn3o\">.css-1dskn3o{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex:1;-ms-flex:1;flex:1;-webkit-box-pack:end;-ms-flex-pack:end;-webkit-justify-content:flex-end;justify-content:flex-end;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}</style><nav class=\"css-1dskn3o\"><div data-cy=\"header-session\" class=\"dc-u-fx dc-u-fx-aic dc-u-mr-8\"><svg viewbox=\"0 0 18 18\" aria-hidden=\"false\" height=\"16\" role=\"img\" width=\"16\"><title>Session Ready</title><path fill=\"#03EF62\" d=\"M9 18A9 9 0 119 0a9 9 0 010 18z\" fill-rule=\"evenodd\"/></svg></div><style data-emotion=\"css 15xw4wa\">.css-15xw4wa{border:none;color:#05192D;}</style><style data-emotion=\"css 1rlvrrk\">.css-1rlvrrk{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;border:0;border-radius:4px;border-style:solid;border-width:2px;cursor:pointer;display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;position:relative;-webkit-text-decoration:none;text-decoration:none;text-transform:capitalize;-webkit-transition:0.15s;transition:0.15s;vertical-align:baseline;white-space:nowrap;background-color:transparent;border-color:#05192D;color:#05192D;padding:8px;border:none;color:#05192D;}.css-1rlvrrk:active{-webkit-transform:perspective(1px) scale(0.975);-moz-transform:perspective(1px) scale(0.975);-ms-transform:perspective(1px) scale(0.975);transform:perspective(1px) scale(0.975);}.css-1rlvrrk:disabled,.css-1rlvrrk:hover:disabled,.css-1rlvrrk:active:disabled{-webkit-transform:none;-moz-transform:none;-ms-transform:none;transform:none;}.css-1rlvrrk:focus{outline:0;}.css-1rlvrrk:hover{background-color:rgba(5, 25, 45, 0.15);border-color:#05192D;color:#05192D;}</style><button aria-label=\"Show Video\" class=\"css-1rlvrrk\" type=\"button\" data-cy=\"header-video\"><svg viewbox=\"0 0 18 18\" aria-hidden=\"true\" height=\"16\" role=\"img\" width=\"16\"><path fill=\"currentColor\" d=\"M13 6.3l3.331-2.998A1 1 0 0118 4.045v9.91a1 1 0 01-1.669.743L13 11.7V14c0 .552-.485 1-1.083 1H1.083C.485 15 0 14.552 0 14V4c0-.552.485-1 1.083-1h10.834C12.515 3 13 3.448 13 4v2.3zm0 2.69v.02l3 2.7V6.29l-3 2.7zM2 5v8h9V5H2z\" fill-rule=\"evenodd\"/></svg></button><button aria-label=\"Show Slides\" class=\"css-1rlvrrk\" type=\"button\" data-cy=\"header-slides\"><svg viewbox=\"0 0 18 18\" aria-hidden=\"true\" height=\"16\" role=\"img\" width=\"16\"><path fill=\"currentColor\" d=\"M14 9.004H9.996a2 2 0 01-2-2V2H4v14h10V9.004zm1.828-2.815A1.938 1.938 0 0116 7v9a2 2 0 01-2 2H4a2 2 0 01-2-2V2a2 2 0 012-2h5.003a2 2 0 011.415.586l4.997 5a2 2 0 01.413.603zm-1.832.815l-4-4v4h4z\" fill-rule=\"evenodd\"/></svg></button><button aria-label=\"Provide Feedback\" class=\"css-1rlvrrk\" type=\"button\" data-cy=\"header-issue\" data-test-id=\"header-report-issue-button\"><svg viewbox=\"0 0 18 18\" aria-hidden=\"true\" height=\"16\" role=\"img\" width=\"16\"><path fill=\"currentColor\" d=\"M9 16A7 7 0 109 2a7 7 0 000 14zm0 2A9 9 0 119 0a9 9 0 010 18zm0-4a1 1 0 110-2 1 1 0 010 2zM8 5a1 1 0 112 0v5a1 1 0 01-2 0V5z\" fill-rule=\"evenodd\"/></svg></button></nav></nav></header><main class=\"exercise-area\"><div data-cy=\"server-side-loader-placeholder\"><aside class=\"exercise--sidebar\" style=\"width:40%\"><div class=\"exercise--sidebar-content\"><div class=\"listview__outer\"><div class=\"listview__inner\"><div class=\"listview__section\"><div><div role=\"button\" class=\"listview__header\"><div class=\"exercise--sidebar-header\"><h5 class=\"dc-panel__title\"><svg aria-label=\"exercise icon\" class=\"dc-icon-exercise dc-u-color-navy dc-u-mr-8\" fill=\"currentColor\" height=\"12\" role=\"Img\" width=\"12\"><use xlink:href=\"/static/media/symbols.e369b265.svg#exercise\"/></svg>Exercise</h5></div></div></div><div class=\"listview__content\"><div class=\"exercise--assignment exercise--typography\"><h1 class=\"exercise--title\">Importing flat files from the web: your turn!</h1><div class><p>You are about to import your first file from the web! The flat file you will import will be <code>&apos;winequality-red.csv&apos;</code> from the University of California, Irvine&apos;s <a href=\"http://archive.ics.uci.edu/ml/index.html\">Machine Learning repository</a>. The flat file contains tabular data of physiochemical properties of red wine, such as pH, alcohol content and citric acid content, along with wine quality rating.</p>\\n<p>The URL of the file is</p>\\n<pre><code>&apos;https://s3.amazonaws.com/assets.datacamp.com/production/course_1606/datasets/winequality-red.csv&apos;\\n</code></pre>\\n<p>After you import it, you&apos;ll check your working directory to confirm that it is there and then you&apos;ll load it into a <code>pandas</code> DataFrame.</p></div></div></div></div><div class=\"listview__section\" style=\"min-height:calc(100% - 33px)\"><div><div role=\"button\" class=\"listview__header\"><div class=\"exercise--sidebar-header\"><h5 class=\"dc-panel__title\"><svg aria-label=\"checkmark_circle icon\" class=\"dc-icon-checkmark_circle dc-u-color-navy dc-u-mr-8\" fill=\"currentColor\" height=\"12\" role=\"Img\" width=\"12\"><use xlink:href=\"/static/media/symbols.e369b265.svg#checkmark_circle\"/></svg>Instructions</h5><style data-emotion=\"css 6996zu\">.css-6996zu{border-radius:4px;display:inline-block;text-transform:uppercase;background-color:#FCCE0D;color:#05192D;font-size:12px;line-height:18px;padding-left:4px;padding-right:4px;}</style><style data-emotion=\"css 1o8nzjk\">.css-1o8nzjk{-webkit-font-smoothing:antialiased;color:#05192D;font-family:Studio-Feixen-Sans,Arial,sans-serif;font-style:normal;font-weight:800;line-height:1.5;border-radius:4px;display:inline-block;text-transform:uppercase;background-color:#FCCE0D;color:#05192D;font-size:12px;line-height:18px;padding-left:4px;padding-right:4px;}</style><strong class=\"css-1o8nzjk\">100 XP</strong></div></div></div><div class=\"listview__content\"><div><div class><div class=\"exercise--instructions exercise--typography\"><div class=\"exercise--instructions__content\"><ul>\\n<li>Import the function <code>urlretrieve</code> from the subpackage <code>urllib.request</code>.</li>\\n<li>Assign the URL of the file to the variable <code>url</code>.</li>\\n<li>Use the function <code>urlretrieve()</code> to save the file locally as <code>&apos;winequality-red.csv&apos;</code>.</li>\\n<li>Execute the remaining code to load <code>&apos;winequality-red.csv&apos;</code> in a pandas DataFrame and to print its head to the shell.</li>\\n</ul></div><div style=\"margin:16px -15px 0\"><section class=\"dc-sct-feedback\" tabindex=\"-1\"><div></div><nav class=\"dc-sct-feedback__nav\"><style data-emotion=\"css 6is1tf\">.css-6is1tf{padding-left:16px;}</style><div class=\"css-6is1tf\"><style data-emotion=\"css 12j1yck\">.css-12j1yck{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;border:0;border-radius:4px;border-style:solid;border-width:2px;cursor:pointer;display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;position:relative;-webkit-text-decoration:none;text-decoration:none;text-transform:capitalize;-webkit-transition:0.15s;transition:0.15s;vertical-align:baseline;white-space:nowrap;background-color:transparent;border-color:#05192D;color:#05192D;padding:0 15px;}.css-12j1yck:active{-webkit-transform:perspective(1px) scale(0.975);-moz-transform:perspective(1px) scale(0.975);-ms-transform:perspective(1px) scale(0.975);transform:perspective(1px) scale(0.975);}.css-12j1yck:disabled,.css-12j1yck:hover:disabled,.css-12j1yck:active:disabled{-webkit-transform:none;-moz-transform:none;-ms-transform:none;transform:none;}.css-12j1yck:focus{outline:0;}.css-12j1yck:hover{background-color:rgba(5, 25, 45, 0.15);border-color:#05192D;color:#05192D;}</style><button class=\"css-12j1yck\" type=\"button\" data-cy=\"exercise-show-hint\"><svg viewbox=\"0 0 18 18\" aria-hidden=\"true\" height=\"16\" role=\"img\" width=\"16\"><path fill=\"currentColor\" d=\"M9 0a7 7 0 014.95 11.95l-.001-.001c-.794.795-.949 1.1-.949 2.051a1 1 0 01-2 0c0-1.548.396-2.325 1.535-3.467l.04-.037a5 5 0 10-7.11.037C6.605 11.675 7 12.453 7 14a1 1 0 01-2 0c0-.951-.155-1.256-.949-2.051A7 7 0 019 0zm0 7a1 1 0 011 1v6a1 1 0 01-2 0V8a1 1 0 011-1zm0 11c-1.657 0-3-.895-3-2h6c0 1.105-1.343 2-3 2z\" fill-rule=\"evenodd\"/></svg><style data-emotion=\"css aib9ji\">.css-aib9ji{font-size:14px;line-height:32px;color:#05192D;font-weight:bold;margin-left:8px;}</style><style data-emotion=\"css vvk465\">.css-vvk465{-webkit-font-smoothing:antialiased;color:#05192D;font-family:Studio-Feixen-Sans,Arial,sans-serif;font-style:normal;font-size:14px;font-weight:400;font-size:14px;line-height:32px;color:#05192D;font-weight:bold;margin-left:8px;}</style><span class=\"css-vvk465\">Take Hint (-30 XP)</span></button></div></nav></section></div></div></div></div></div></div></div></div></div></aside><section class=\"exercise--content\" style=\"width:60%\"><div class=\"exercise-waiting\"><div class=\"global-spinner dc-u-fx-jcc dc-u-fx\"><style data-emotion=\"css 1f2mbny\">.css-1f2mbny{height:70px;width:70px;}</style><div class=\"css-1f2mbny\"><style data-emotion=\"css 1idrum8\">.css-1idrum8{-webkit-animation:animation-1pv1bkr cubic-bezier(0.65, 0, 0.55, 1) 2s infinite alternate;animation:animation-1pv1bkr cubic-bezier(0.65, 0, 0.55, 1) 2s infinite alternate;margin:auto;width:76.65%;will-change:clip-path;}@-webkit-keyframes animation-1pv1bkr{0%,6%{-webkit-clip-path:polygon(0% -12%, 0% -12%, 169% 63%, 169% 63%);clip-path:polygon(0% -12%, 0% -12%, 169% 63%, 169% 63%);}100%{-webkit-clip-path:polygon(0% -12%, 0% 308%, 169% 383%, 169% 63%);clip-path:polygon(0% -12%, 0% 308%, 169% 383%, 169% 63%);}}@keyframes animation-1pv1bkr{0%,6%{-webkit-clip-path:polygon(0% -12%, 0% -12%, 169% 63%, 169% 63%);clip-path:polygon(0% -12%, 0% -12%, 169% 63%, 169% 63%);}100%{-webkit-clip-path:polygon(0% -12%, 0% 308%, 169% 383%, 169% 63%);clip-path:polygon(0% -12%, 0% 308%, 169% 383%, 169% 63%);}}</style><div class=\"css-1idrum8\"><style data-emotion=\"css 1j8nxo animation-1h2cwi2\">.css-1j8nxo{-webkit-animation:animation-1h2cwi2 cubic-bezier(0, 0, 0.85, 1) 2s infinite alternate;animation:animation-1h2cwi2 cubic-bezier(0, 0, 0.85, 1) 2s infinite alternate;will-change:clip-path;}@-webkit-keyframes animation-1h2cwi2{0%,71%{-webkit-clip-path:polygon(0% 0%, 0% 78.5%, 100% 34.5%, 100% -44%);clip-path:polygon(0% 0%, 0% 78.5%, 100% 34.5%, 100% -44%);}96%,100%{-webkit-clip-path:polygon(0% 0%, 0% 140%, 100% 96%, 100% -44%);clip-path:polygon(0% 0%, 0% 140%, 100% 96%, 100% -44%);}}@keyframes animation-1h2cwi2{0%,71%{-webkit-clip-path:polygon(0% 0%, 0% 78.5%, 100% 34.5%, 100% -44%);clip-path:polygon(0% 0%, 0% 78.5%, 100% 34.5%, 100% -44%);}96%,100%{-webkit-clip-path:polygon(0% 0%, 0% 140%, 100% 96%, 100% -44%);clip-path:polygon(0% 0%, 0% 140%, 100% 96%, 100% -44%);}}</style><div class=\"css-1j8nxo\"><style data-emotion=\"css 5h5b50\">.css-5h5b50{-webkit-clip-path:polygon(-0.1% -10%, 169% 65%, -0.1% 139%);clip-path:polygon(-0.1% -10%, 169% 65%, -0.1% 139%);}</style><div class=\"css-5h5b50\"><style data-emotion=\"css 4zleql\">.css-4zleql{display:block;}</style><svg version=\"1.1\" viewbox=\"0 0 2640 3444\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" class=\"css-4zleql\"><title>Loading</title><style data-emotion=\"css jy99qt animation-co7x2c\">.css-jy99qt{-webkit-animation:animation-co7x2c cubic-bezier(0.65, 0, 0.55, 1) 2s infinite alternate;animation:animation-co7x2c cubic-bezier(0.65, 0, 0.55, 1) 2s infinite alternate;stroke-dasharray:9800;stroke-dashoffset:9800;will-change:stroke-dashoffset;}@-webkit-keyframes animation-co7x2c{100%{stroke-dashoffset:0;}}@keyframes animation-co7x2c{100%{stroke-dashoffset:0;}}</style><path d=\"M0 0 M2569 1056L143 2447V149l1175 673v1867l1248 715\" fill=\"none\" stroke=\"#05192d\" stroke-linejoin=\"round\" stroke-width=\"300\" class=\"css-jy99qt\"/></svg></div></div></div></div></div><noscript></noscript></div></section></div><style data-emotion=\"css dhfy3a 7y8jxc 14w24v3 1yuhvjn\">.css-dhfy3a{-webkit-animation-name:animation-2ijyvo;animation-name:animation-2ijyvo;-webkit-animation-timing-function:cubic-bezier(0.23, 1, 0.32, 1);animation-timing-function:cubic-bezier(0.23, 1, 0.32, 1);}@-webkit-keyframes animation-2ijyvo{50%{opacity:1;}from{opacity:0;-webkit-transform:scale3d(0.3, 0.3, 0.3);-moz-transform:scale3d(0.3, 0.3, 0.3);-ms-transform:scale3d(0.3, 0.3, 0.3);transform:scale3d(0.3, 0.3, 0.3);}}@keyframes animation-2ijyvo{50%{opacity:1;}from{opacity:0;-webkit-transform:scale3d(0.3, 0.3, 0.3);-moz-transform:scale3d(0.3, 0.3, 0.3);-ms-transform:scale3d(0.3, 0.3, 0.3);transform:scale3d(0.3, 0.3, 0.3);}}.css-7y8jxc{-webkit-animation-name:animation-1phn0oq;animation-name:animation-1phn0oq;-webkit-animation-timing-function:cubic-bezier(0.755, 0.05, 0.855, 0.06);animation-timing-function:cubic-bezier(0.755, 0.05, 0.855, 0.06);}@-webkit-keyframes animation-1phn0oq{50%{opacity:0;-webkit-transform:scale3d(0.3, 0.3, 0.3);-moz-transform:scale3d(0.3, 0.3, 0.3);-ms-transform:scale3d(0.3, 0.3, 0.3);transform:scale3d(0.3, 0.3, 0.3);}from{opacity:1;}to{opacity:0;}}@keyframes animation-1phn0oq{50%{opacity:0;-webkit-transform:scale3d(0.3, 0.3, 0.3);-moz-transform:scale3d(0.3, 0.3, 0.3);-ms-transform:scale3d(0.3, 0.3, 0.3);transform:scale3d(0.3, 0.3, 0.3);}from{opacity:1;}to{opacity:0;}}.css-14w24v3{left:50%;position:fixed;top:0;-webkit-transform:translateX(-50%);-moz-transform:translateX(-50%);-ms-transform:translateX(-50%);transform:translateX(-50%);z-index:999;}.css-14w24v3 .Toastify__progress-bar{-webkit-animation:animation-qqoh2i linear 1;animation:animation-qqoh2i linear 1;}@-webkit-keyframes animation-qqoh2i{0%{-webkit-transform:scaleX(1);-moz-transform:scaleX(1);-ms-transform:scaleX(1);transform:scaleX(1);}100%{-webkit-transform:scaleX(0);-moz-transform:scaleX(0);-ms-transform:scaleX(0);transform:scaleX(0);}}@keyframes animation-qqoh2i{0%{-webkit-transform:scaleX(1);-moz-transform:scaleX(1);-ms-transform:scaleX(1);transform:scaleX(1);}100%{-webkit-transform:scaleX(0);-moz-transform:scaleX(0);-ms-transform:scaleX(0);transform:scaleX(0);}}.css-1yuhvjn{margin-top:16px;}</style><div class=\"Toastify\"></div></main><div class=\"exercise-footer\"><ul data-cy=\"progress-container\" class=\"dc-progress-indicator\"><li class=\"dc-progress-indicator__item\"><a href=\"javascript:void(0)\" class=\"dc-progress-indicator__bar\"><div class=\"dc-progress-indicator__fill\" style=\"width:0%\"></div></a></li><li class=\"dc-progress-indicator__item\"><a href=\"javascript:void(0)\" class=\"dc-progress-indicator__bar\"><div class=\"dc-progress-indicator__fill\" style=\"width:0%\"></div></a></li><li class=\"dc-progress-indicator__item\"><a href=\"javascript:void(0)\" class=\"dc-progress-indicator__bar\"><div class=\"dc-progress-indicator__fill\" style=\"width:0%\"></div></a></li></ul></div><style data-emotion=\"css zs9gal 13qqqtf 728dx5 1d9ftqx atcdtd 728dx5 d3v9zr\">.css-zs9gal{opacity:1!important;-webkit-transform:none!important;-moz-transform:none!important;-ms-transform:none!important;transform:none!important;}.css-13qqqtf{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;min-width:0;opacity:0;outline:none;position:relative;-webkit-transform:scale(0.5);-moz-transform:scale(0.5);-ms-transform:scale(0.5);transform:scale(0.5);-webkit-transition:0.4s cubic-bezier(0.19, 1, 0.22, 1);transition:0.4s cubic-bezier(0.19, 1, 0.22, 1);box-sizing:border-box;max-height:100%;padding:8px;width:496px;}.css-728dx5{opacity:0!important;}.css-1d9ftqx{opacity:1!important;}.css-atcdtd{-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;background-color:rgba(5, 25, 45, 0.8);bottom:0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;left:0;opacity:0;position:fixed;right:0;top:0;-webkit-transition:opacity 0.6s cubic-bezier(0.19, 1, 0.22, 1);transition:opacity 0.6s cubic-bezier(0.19, 1, 0.22, 1);z-index:900;}.css-d3v9zr{overflow:hidden;}</style></div></div><script>window.MathJax={options:{ignoreHtmlClass:\"tex2jax_ignore\",processHtmlClass:\"tex2jax_process\"},tex:{autoload:{color:[],colorV2:[\"color\"]},packages:{\"[+]\":[\"noerrors\"]}},loader:{load:[\"[tex]/noerrors\"]}}</script><script src=\"/campus/mathjax@3/es5/tex-chtml.js\" id=\"MathJax-script\"></script><script>!function(e){function t(t){for(var n,o,f=t[0],d=t[1],u=t[2],i=0,l=[];i<f.length;i++)o=f[i],Object.prototype.hasOwnProperty.call(c,o)&&c[o]&&l.push(c[o][0]),c[o]=0;for(n in d)Object.prototype.hasOwnProperty.call(d,n)&&(e[n]=d[n]);for(s&&s(t);l.length;)l.shift()();return a.push.apply(a,u||[]),r()}function r(){for(var e,t=0;t<a.length;t++){for(var r=a[t],n=!0,o=1;o<r.length;o++){var d=r[o];0!==c[d]&&(n=!1)}n&&(a.splice(t--,1),e=f(f.s=r[0]))}return e}var n={},o={14:0},c={14:0},a=[];function f(t){if(n[t])return n[t].exports;var r=n[t]={i:t,l:!1,exports:{}};return e[t].call(r.exports,r,r.exports,f),r.l=!0,r.exports}f.e=function(e){var t=[];o[e]?t.push(o[e]):0!==o[e]&&{1:1,2:1,5:1,8:1,9:1,10:1,13:1,18:1,19:1,21:1,22:1}[e]&&t.push(o[e]=new Promise((function(t,r){for(var n=\"static/css/\"+({7:\"console-monaco\",8:\"dnde\",9:\"ee\",10:\"idee\",12:\"monaco\",13:\"rde\",15:\"streak-popover\",16:\"xterm\"}[e]||e)+\".\"+{0:\"31d6cfe0\",1:\"e8c7adfe\",2:\"4d201c8f\",3:\"31d6cfe0\",4:\"31d6cfe0\",5:\"a014e8d0\",6:\"31d6cfe0\",7:\"31d6cfe0\",8:\"b5d0672e\",9:\"2bf50755\",10:\"f128b9fc\",12:\"31d6cfe0\",13:\"2299b9ab\",15:\"31d6cfe0\",16:\"31d6cfe0\",18:\"4846b048\",19:\"3d94959c\",20:\"31d6cfe0\",21:\"3134f6c1\",22:\"e16192f0\",23:\"31d6cfe0\",24:\"31d6cfe0\",25:\"31d6cfe0\",26:\"31d6cfe0\",27:\"31d6cfe0\",28:\"31d6cfe0\"}[e]+\".chunk.css\",c=f.p+n,a=document.getElementsByTagName(\"link\"),d=0;d<a.length;d++){var u=(s=a[d]).getAttribute(\"data-href\")||s.getAttribute(\"href\");if(\"stylesheet\"===s.rel&&(u===n||u===c))return t()}var i=document.getElementsByTagName(\"style\");for(d=0;d<i.length;d++){var s;if((u=(s=i[d]).getAttribute(\"data-href\"))===n||u===c)return t()}var l=document.createElement(\"link\");l.rel=\"stylesheet\",l.type=\"text/css\",l.onload=t,l.onerror=function(t){var n=t&&t.target&&t.target.src||c,a=new Error(\"Loading CSS chunk \"+e+\" failed.\\\\n(\"+n+\")\");a.code=\"CSS_CHUNK_LOAD_FAILED\",a.request=n,delete o[e],l.parentNode.removeChild(l),r(a)},l.href=c,document.getElementsByTagName(\"head\")[0].appendChild(l)})).then((function(){o[e]=0})));var r=c[e];if(0!==r)if(r)t.push(r[2]);else{var n=new Promise((function(t,n){r=c[e]=[t,n]}));t.push(r[2]=n);var a,d=document.createElement(\"script\");d.charset=\"utf-8\",d.timeout=120,f.nc&&d.setAttribute(\"nonce\",f.nc),d.src=function(e){return f.p+\"static/js/\"+({7:\"console-monaco\",8:\"dnde\",9:\"ee\",10:\"idee\",12:\"monaco\",13:\"rde\",15:\"streak-popover\",16:\"xterm\"}[e]||e)+\".\"+{0:\"6dcde146\",1:\"0fb4c9d6\",2:\"bcebfb80\",3:\"30a227e7\",4:\"81ae6e95\",5:\"74aefcf0\",6:\"e727f6ea\",7:\"ec54d7d3\",8:\"c4f96519\",9:\"a39d5c34\",10:\"b274cccd\",12:\"611d2df1\",13:\"7354f3e4\",15:\"956b63a6\",16:\"5bc4fe32\",18:\"08603b6c\",19:\"fb336a6e\",20:\"c466e1d2\",21:\"01901adc\",22:\"2aff9213\",23:\"ff01226b\",24:\"98d596ca\",25:\"7508ad44\",26:\"277605f0\",27:\"53e3c33a\",28:\"49c82727\"}[e]+\".chunk.js\"}(e);var u=new Error;a=function(t){d.onerror=d.onload=null,clearTimeout(i);var r=c[e];if(0!==r){if(r){var n=t&&(\"load\"===t.type?\"missing\":t.type),o=t&&t.target&&t.target.src;u.message=\"Loading chunk \"+e+\" failed.\\\\n(\"+n+\": \"+o+\")\",u.name=\"ChunkLoadError\",u.type=n,u.request=o,r[1](u)}c[e]=void 0}};var i=setTimeout((function(){a({type:\"timeout\",target:d})}),12e4);d.onerror=d.onload=a,document.head.appendChild(d)}return Promise.all(t)},f.m=e,f.c=n,f.d=function(e,t,r){f.o(e,t)||Object.defineProperty(e,t,{enumerable:!0,get:r})},f.r=function(e){\"undefined\"!=typeof Symbol&&Symbol.toStringTag&&Object.defineProperty(e,Symbol.toStringTag,{value:\"Module\"}),Object.defineProperty(e,\"__esModule\",{value:!0})},f.t=function(e,t){if(1&t&&(e=f(e)),8&t)return e;if(4&t&&\"object\"==typeof e&&e&&e.__esModule)return e;var r=Object.create(null);if(f.r(r),Object.defineProperty(r,\"default\",{enumerable:!0,value:e}),2&t&&\"string\"!=typeof e)for(var n in e)f.d(r,n,function(t){return e[t]}.bind(null,n));return r},f.n=function(e){var t=e&&e.__esModule?function(){return e.default}:function(){return e};return f.d(t,\"a\",t),t},f.o=function(e,t){return Object.prototype.hasOwnProperty.call(e,t)},f.p=\"/campus/\",f.oe=function(e){throw console.error(e),e};var d=this[\"webpackJsonpcampus-app-v2\"]=this[\"webpackJsonpcampus-app-v2\"]||[],u=d.push.bind(d);d.push=t,d=d.slice();for(var i=0;i<d.length;i++)t(d[i]);var s=u;r()}([])</script><script src=\"/campus/static/js/17.dbee202b.chunk.js\"></script><script src=\"/campus/static/js/main.9cead79c.chunk.js\"></script><script type=\"text/javascript\">(function(){window[\\'__CF$cv$params\\']={r:\\'6cc911774ed9cfa3\\',m:\\'LyB4I3pQulHx80uaJ5VDE6RVK0mcUsBb7UyhtSGzvGs-1642018645-0-AWUvi7tIkmePUOPPG/TMrV4GAObEAcHXbmjxxbAZhMreFMKTEysPkRMGRioUgTnCg9FAoCcE9kmnNkLDwtapwWqeGuf/x/KchRCHqN2hz7dYg3uvecV2PUsUjD9Qf0bjpS9BBmjm+NRglBphqKPh9qo=\\',s:[0xec248aedab,0xe105336afe],}})();</script></body></html>'\n"
     ]
    }
   ],
   "source": [
    "#Printing HTTP request results in Python using urllib\n",
    "\n",
    "#You have just packaged and sent a GET request to \n",
    "#\"https://campus.datacamp.com/courses/1606/4135?ex=2\" \n",
    "#and then caught the response. You saw that such a \n",
    "#response is a http.client.HTTPResponse object. The \n",
    "#question remains: what can you do with this response?\n",
    "\n",
    "#Well, as it came from an HTML page, you could read it to \n",
    "#extract the HTML and, in fact, such a \n",
    "#http.client.HTTPResponse object has an associated \n",
    "#read() method. In this exercise, you'll build on your \n",
    "#previous great work to extract the response and print the \n",
    "#HTML.\n",
    "\n",
    "#Instructions\n",
    "\n",
    "#Send the request and catch the response in the variable \n",
    "#response with the function urlopen(), as in the\n",
    "# previous exercise.\n",
    "\n",
    "#Extract the response using the read() method and store \n",
    "#the result in the variable html.\n",
    "\n",
    "#Print the string html.\n",
    "\n",
    "#Hit submit to perform all of the above and to close the \n",
    "#response: be tidy!\n",
    "\n",
    "# Import packages\n",
    "from urllib.request import urlopen, Request\n",
    "\n",
    "# Specify the url\n",
    "url = \"https://campus.datacamp.com/courses/1606/4135?ex=2\"\n",
    "\n",
    "# This packages the request\n",
    "request = Request(url)\n",
    "\n",
    "# Sends the request and catches the response: response\n",
    "response=urlopen(request)\n",
    "\n",
    "# Extract the response: html\n",
    "html=response.read()\n",
    "\n",
    "# Print the html\n",
    "print(html)\n",
    "\n",
    "# Be polite and close the response!\n",
    "response.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "10e57d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE HTML>\n",
      "<html lang=\"en-US\">\n",
      "<head>\n",
      "  <meta charset=\"UTF-8\" />\n",
      "  <meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\" />\n",
      "  <meta http-equiv=\"X-UA-Compatible\" content=\"IE=Edge,chrome=1\" />\n",
      "  <meta name=\"robots\" content=\"noindex, nofollow\" />\n",
      "  <meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" />\n",
      "  <title>Just a moment...</title>\n",
      "  <style type=\"text/css\">\n",
      "    html, body {width: 100%; height: 100%; margin: 0; padding: 0;}\n",
      "    body {background-color: #ffffff; color: #000000; font-family:-apple-system, system-ui, BlinkMacSystemFont, \"Segoe UI\", Roboto, Oxygen, Ubuntu, \"Helvetica Neue\",Arial, sans-serif; font-size: 16px; line-height: 1.7em;-webkit-font-smoothing: antialiased;}\n",
      "    h1 { text-align: center; font-weight:700; margin: 16px 0; font-size: 32px; color:#000000; line-height: 1.25;}\n",
      "    p {font-size: 20px; font-weight: 400; margin: 8px 0;}\n",
      "    p, .attribution, {text-align: center;}\n",
      "    #spinner {margin: 0 auto 30px auto; display: block;}\n",
      "    .attribution {margin-top: 32px;}\n",
      "    @keyframes fader     { 0% {opacity: 0.2;} 50% {opacity: 1.0;} 100% {opacity: 0.2;} }\n",
      "    @-webkit-keyframes fader { 0% {opacity: 0.2;} 50% {opacity: 1.0;} 100% {opacity: 0.2;} }\n",
      "    #cf-bubbles > .bubbles { animation: fader 1.6s infinite;}\n",
      "    #cf-bubbles > .bubbles:nth-child(2) { animation-delay: .2s;}\n",
      "    #cf-bubbles > .bubbles:nth-child(3) { animation-delay: .4s;}\n",
      "    .bubbles { background-color: #f58220; width:20px; height: 20px; margin:2px; border-radius:100%; display:inline-block; }\n",
      "    a { color: #2c7cb0; text-decoration: none; -moz-transition: color 0.15s ease; -o-transition: color 0.15s ease; -webkit-transition: color 0.15s ease; transition: color 0.15s ease; }\n",
      "    a:hover{color: #f4a15d}\n",
      "    .attribution{font-size: 16px; line-height: 1.5;}\n",
      "    .ray_id{display: block; margin-top: 8px;}\n",
      "    #cf-wrapper #challenge-form { padding-top:25px; padding-bottom:25px; }\n",
      "    #cf-hcaptcha-container { text-align:center;}\n",
      "    #cf-hcaptcha-container iframe { display: inline-block;}\n",
      "  </style>\n",
      "\n",
      "      <meta http-equiv=\"refresh\" content=\"35\">\n",
      "  <script type=\"text/javascript\">\n",
      "    //<![CDATA[\n",
      "    (function(){\n",
      "      \n",
      "      window._cf_chl_opt={\n",
      "        cvId: \"2\",\n",
      "        cType: \"non-interactive\",\n",
      "        cNounce: \"38228\",\n",
      "        cRay: \"6cc91380fa58fdf1\",\n",
      "        cHash: \"f5ca9fae9138173\",\n",
      "        cPMDTk: \"\",\n",
      "        cFPWv: \"b\",\n",
      "        cTTimeMs: \"1000\",\n",
      "        cRq: {\n",
      "          ru: \"aHR0cHM6Ly93d3cuZGF0YWNhbXAuY29tL3RlYWNoL2RvY3VtZW50YXRpb24=\",\n",
      "          ra: \"cHl0aG9uLXJlcXVlc3RzLzIuMjYuMA==\",\n",
      "          rm: \"R0VU\",\n",
      "          d: \"6KW6Ghlo9c9csHZyvEXOuS7i8E1iBcbJBtkFLnoU9KSWw+nikFR9jP/2jieZjJSOFed+6srth/jNmI/VrkVwHfqAww2/qel0HXjHogEHDyMW/1+b+s+ipzfzc6zFvU7DW4dxrYKq6Nq4RdrJmhp+efdcRiNQA37bVhvmuG6g3RTzSyyxzVLdzDDH5Rn//LYyxWfll7UdxWS/xD3Py8Sr68vxR0JW7j18QMXN3+xHpIv4G9WDdaN0NqdoGJlwRh8LgaQBON88zBcSCNajfvwRlJRgWe7LF9+cuQatE5nYmPIvvMcf/h/sWA2GfN4sn3VpTnHE0ZdlYjlS5LMF3IcXIjwi6IPKjKoyaKYHs8X9ab6rhWm2Z3m6OgbIxCdoO4tTc0NeSkJUMHyqvqAA1OocsVZndGUWVwR8zIfEzyRTBMcilLBH416m6fEW4WqxcCfynwccqiA6z1azfDX6D5V7S5xBbf0gLLSwtNbe5UCTprPEkHk7/Fh3pGToQNc7NkS9L/74aKpIIcUwhbfa1xzUX3wjLXgf6fmD5CF33QVnaeeriDDhBrLmYrasYFtolB4y\",\n",
      "          t: \"MTY0MjAxODcyOS4xMzQwMDA=\",\n",
      "          m: \"n6oCyHz9MS+WBynjMgmLZmZCAmbMVlQxkJevu/Zxm/8=\",\n",
      "          i1: \"MzyaWIbKKvAwP48+LN5czA==\",\n",
      "          i2: \"DZgSpCbu/BbgPPqjxgl7hw==\",\n",
      "          zh: \"hzfiqo9hugT9sHeHQ1zy81NCL/S0295H0+GuRnkSV9o=\",\n",
      "          uh: \"xaa5dII6Z3KyYGzGAu/zTXOfAYzLW3WlpO4dxW/Wc8c=\",\n",
      "          hh: \"rAZnIHiyrNuZ60h9aAZNML8izDilqmOSNuCtac1WqPs=\",\n",
      "        }\n",
      "      }\n",
      "      window._cf_chl_enter = function(){window._cf_chl_opt.p=1};\n",
      "      \n",
      "    })();\n",
      "    //]]>\n",
      "  </script>\n",
      "  \n",
      "\n",
      "</head>\n",
      "<body>\n",
      "  <table width=\"100%\" height=\"100%\" cellpadding=\"20\">\n",
      "    <tr>\n",
      "      <td align=\"center\" valign=\"middle\">\n",
      "          <div class=\"cf-browser-verification cf-im-under-attack\">\n",
      "  <noscript>\n",
      "    <h1 data-translate=\"turn_on_js\" style=\"color:#bd2426;\">Please turn JavaScript on and reload the page.</h1>\n",
      "  </noscript>\n",
      "  <div id=\"cf-content\" style=\"display:none\">\n",
      "    \n",
      "    <div id=\"cf-bubbles\">\n",
      "      <div class=\"bubbles\"></div>\n",
      "      <div class=\"bubbles\"></div>\n",
      "      <div class=\"bubbles\"></div>\n",
      "    </div>\n",
      "    <h1><span data-translate=\"checking_browser\">Checking your browser before accessing</span> www.datacamp.com.</h1>\n",
      "    \n",
      "    <div id=\"no-cookie-warning\" class=\"cookie-warning\" data-translate=\"turn_on_cookies\" style=\"display:none\">\n",
      "      <p data-translate=\"turn_on_cookies\" style=\"color:#bd2426;\">Please enable Cookies and reload the page.</p>\n",
      "    </div>\n",
      "    <p data-translate=\"process_is_automatic\">This process is automatic. Your browser will redirect to your requested content shortly.</p>\n",
      "    <p data-translate=\"allow_5_secs\" id=\"cf-spinner-allow-5-secs\" >Please allow up to 5 seconds&hellip;</p>\n",
      "    <p data-translate=\"redirecting\" id=\"cf-spinner-redirecting\" style=\"display:none\">Redirecting&hellip;</p>\n",
      "  </div>\n",
      "   \n",
      "  <form class=\"challenge-form\" id=\"challenge-form\" action=\"/teach/documentation?__cf_chl_jschl_tk__=L4o1SQXLzG3J0GD_xwSQ1mjBakP.352uSn_FeeaTbXQ-1642018729-0-gaNycGzNB-U\" method=\"POST\" enctype=\"application/x-www-form-urlencoded\">\n",
      "    <input type=\"hidden\" name=\"md\" value=\"ozbe9oCLUHYZGwZF5ZD41F5AQMzjq4z9nUpyQ95xXt8-1642018729-0-AW3TQEJoIiiLgS42hhuAmLcfxvM68YTbh_rQYAyiE-hcQy0tcAoXeE70GDpZbBI5pL5yDLjU1uHrrin8FM0v5sW86g7hMeJ3ttdyoClvLXMGwiSspnQQaaPLX7BDjAEMQ41SgTsp7FZ2z7lRMAPf7RyNjk9e6enyZ_tsHPiG966sXAmsPr8Jxe35ah8BtFHziDzzS1IXZWrqjKB6pfbP3QvMveWh5Y4CpPlem92yIUZr9riAZD_3WmAdlkkuGqVOoaN27-YG9Ha9eXKhhKsMsst6IbPhvQgyMfsll37AtSvMkrctXm6KnZpPwJrGcPErmPz25I-3kAvjZgLRw-YR8xqQ4PZjF1562wuBlE70R_QBpfn4l7zcRZe7i3wgcpzoWojohXjLeH8wRHRFOMVDaS49hmbuJwXR28t1j0RDKGpdZK2uNMZb1g9GGyuqH0C3Y0bfYRBL-FiL1excOGhoHzFBpzU_q9zXkctBFrodhj92n_-t0PE08v4qU7JQNqGRb7Ur2hl03fJ20wVpnXVmkEkSvrB5c3u6PHL4RB0_LFUeQktt5jmHL7eW_blRMddP6KhiR1rembzvk4D4PIhN5MPBNX9VIwpXnCba7FA6MiiKkFgMklehQ_2xnkw_xF0KwxR6V7XwnGEvOaHVgAyUQT5zBfPjeDoBwp5Lj8_w5-qH\" />\n",
      "    <input type=\"hidden\" name=\"r\" value=\"eU4Vy9d3N9DiqJhCpwYtZ42ZSUx06177ZZaWN2FDkDQ-1642018729-0-AUOKtkw5bGj4sopjLTtlH31X5fr5QTrXz0D4l/keRa7DPInOKclzaJ2C7fWGKV/j4a1JM/HPoIjALYGc/q3aAs/GoLRWTUwyWtfqigu0baITwRoALz0rDTClEtIE1sfuM44/mdtjK9su0r+FQYXdvcjv4MHnUJCBvTZa6hJxR4QMUiRgRApnt8OJfIcl+U39z8PqjYadu3jbpZyI6yhjy6da92yTt+4hBC6gswV4D2fBY5vocPSycFxW3WcXit6ngjc0ogW2f3tWQ0SmVtB5YRrJ5w+O5GzXYu81xnuRyIsEdx+6AS8aWcSldMzNMzv3KnTEcbWUDlcm3zbHqAm3IhxWV3gKIYxHfGx54VqFbw1UJQDWmrxyNONrCCyNLM+ofSxWeWPeijTY/JphORGCrD69UKYzA/yPK1UwaGMob9DkE7Vjbdh5TIbK1mbyrsxCZXWsC8cYTE5atar0pfvyZlPavSAKM1Jag/LthciVKABb3uc6dcpNRTld0BMbQrnm//dySHc5dRP9egPjj4L2+senX75vC3tYZblqDw4+d4hVsI1ncl9kUnAW2CafUhkMkoJsPHs2i2mV/GXhjTK7HYyf/rybKd6RS0kXlYtTKLZKoh/h03FDyccnhXpziN9KTrMPjfY9/lDV6USLs/fKUTpMzWiwhrv63+x8Ys+TUjUG5wRsVOwxiV/VeQYdR+6sLXLmsACxph4iPJfTzv3m4g3iFlwR+wNk+QkGJ13SNloRWJ/cmY299I1LroqcxGnPXf7rAxPC4VSMNIl3k6kpE6UqIAKUv0CEqhUh38fA2kqmIeArqjxAw3rKkWDQIOJae8/ZA1WDw2mGvUWBfZ9BVUQ7PyINpOsngcH78e4bCJMbBT8Fnpi/b6NRAKaWz/Py3hN778z/xS5ZJzGV2lwUBXZEG6dOAmuDX3WhnnrB2Hvkc4MHkXkO89mDAuYXu2MO5zVkFb8pQ+QFWDgXDZdrqOk6qWwUSAISmdMxmWIqd8pVVeocW5cqzXuOVnCGb6+KA73D7613lwXSf5Yh6ozHMTSwX28KMCqYRWcBclh97/xzDgmJDzvweGShWyNHrtrPlB2eoIdOVaAKKT1uXMUZO5wwrA/q8LJrpq2rSYKFYwe6wzv5EWeV0EisNprexQHHu4U9FlBcM6Y6xw5wR7Uk5vPcTLK/IhYpGqfBboGTxGDDuv/uXPHqLURERM8cUnjDrVF6g+7qMgeXglTMzFd7B4Wpkgh+fq9xVPoWG4M2Lxjn+nw4ynRhc9B8Cduto2luKmzITdIhRwOHp9MWeX/3A2H6yoAE2AQzwXNhm3cBetzDf+OuuYErrlIBs3x/yY0UIW431iZpu1xclhMUG916Ck1GvPEjPdtaFZELa59bDmtghahGrtavCH26PuDTVJRakF7afWd/qd79N/oGAE8EQWRtFOscJtN97PqxBCVy9/vFivy9q8oMCGrty5ccbuy7I1raNICcAkDtjRo8eboOo6LmTVKACPEIXdZTj8VzKT5VWnFUobjcjxU6A5cYroJbQkP/hS8TKk/i2BElCglPpjcrrflv2+dwmJ7hnWUNsFosDl41Ir1UL6gUQOFQfXkB1xw+lpkUDd78N0F1btValxYGNrywXm+F0XjpBgld1teamRdP58pABMgp5D0fzP0e1RFelMIBwk2ksS/HlKNgnIWST4rs9LndGhVFmoT604cvYDZnHMqFiw/20TqolYp0RzY9zrvSx9XAQEtA596TWdysnqe3JOSiVv9WJM2jhWSNKg7knbQp4UpGUpfX0PIIU2G5NO2cablyQcPC6UgL4d8PBji1gVHf0TslhX3Spa6EwkgKmaep9weFwzWtoHQZj5JC+iJfVS7Qbs3ghtlkJ9sjfqAmTmxzS/EctBTfwM77NMQ+3Lp1nqpQiqPDm7TJ+ML+KGW8R+g6uZdrBnI/FdAhSuvvzR95MWzAcO9VNhLw\"/>\n",
      "    <input type=\"hidden\" value=\"666cfade50af674de4187fd2c796d02c\" id=\"jschl-vc\" name=\"jschl_vc\"/>\n",
      "    <!-- <input type=\"hidden\" value=\"\" id=\"jschl-vc\" name=\"jschl_vc\"/> -->\n",
      "    <input type=\"hidden\" name=\"pass\" value=\"1642018730.134-DTcKb83m6a\"/>\n",
      "    <input type=\"hidden\" id=\"jschl-answer\" name=\"jschl_answer\"/>\n",
      "  </form>\n",
      "     \n",
      "    <script type=\"text/javascript\">\n",
      "      //<![CDATA[\n",
      "      (function(){\n",
      "          var a = document.getElementById('cf-content');\n",
      "          a.style.display = 'block';\n",
      "          var isIE = /(MSIE|Trident\\/|Edge\\/)/i.test(window.navigator.userAgent);\n",
      "          var trkjs = isIE ? new Image() : document.createElement('img');\n",
      "          trkjs.setAttribute(\"src\", \"/cdn-cgi/images/trace/jschal/js/transparent.gif?ray=6cc91380fa58fdf1\");\n",
      "          trkjs.id = \"trk_jschal_js\";\n",
      "          trkjs.setAttribute(\"alt\", \"\");\n",
      "          document.body.appendChild(trkjs);\n",
      "          var cpo=document.createElement('script');\n",
      "          cpo.type='text/javascript';\n",
      "          cpo.src=\"/cdn-cgi/challenge-platform/h/b/orchestrate/jsch/v1?ray=6cc91380fa58fdf1\";\n",
      "          \n",
      "          document.getElementsByTagName('head')[0].appendChild(cpo);\n",
      "        }());\n",
      "      //]]>\n",
      "    </script>\n",
      "  \n",
      "\n",
      "  \n",
      "  <div id=\"trk_jschal_nojs\" style=\"background-image:url('/cdn-cgi/images/trace/jschal/nojs/transparent.gif?ray=6cc91380fa58fdf1')\"> </div>\n",
      "</div>\n",
      "\n",
      "          \n",
      "          <div class=\"attribution\">\n",
      "            DDoS protection by <a rel=\"noopener noreferrer\" href=\"https://www.cloudflare.com/5xx-error-landing/\" target=\"_blank\">Cloudflare</a>\n",
      "            <br />\n",
      "            <span class=\"ray_id\">Ray ID: <code>6cc91380fa58fdf1</code></span>\n",
      "          </div>\n",
      "      </td>\n",
      "     \n",
      "    </tr>\n",
      "  </table>\n",
      "</body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Performing HTTP requests in Python using requests\n",
    "\n",
    "#Now that you've got your head and hands around making\n",
    " #HTTP requests using the urllib package, you're going to \n",
    "#figure out how to do the same using the higher-level \n",
    "#requests library. You'll once again be pinging DataCamp \n",
    "#servers for their \n",
    "#\"http://www.datacamp.com/teach/documentation\" page.\n",
    "\n",
    "#Note that unlike in the previous exercises using urllib,\n",
    " #you don't have to close the connection when using requests!\n",
    "\n",
    "#Instructions\n",
    "\n",
    "#Import the package requests.\n",
    "\n",
    "#Assign the URL of interest to the variable url.\n",
    "\n",
    "#Package the request to the URL, send the request and \n",
    "#catch the response with a single function \n",
    "#requests.get(), assigning the response to the variable \n",
    "#r.\n",
    "\n",
    "#Use the text attribute of the object r to return the \n",
    "#HTML of the webpage as a string; store the result in a \n",
    "#variable text.\n",
    "\n",
    "#Hit submit to print the HTML of the webpage.\n",
    "\n",
    "# Import package\n",
    "import requests\n",
    "\n",
    "# Specify the url: url\n",
    "url=\"http://www.datacamp.com/teach/documentation\"\n",
    "\n",
    "# Packages the request, send the request and catch the response: r\n",
    "r=requests.get(url)\n",
    "\n",
    "# Extract the response: text\n",
    "text=r.text\n",
    "\n",
    "# Print the html\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4d74fc38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      " <head>\n",
      "  <title>\n",
      "   Guido's Personal Home Page\n",
      "  </title>\n",
      " </head>\n",
      " <body bgcolor=\"#FFFFFF\" text=\"#000000\">\n",
      "  <!-- Built from main -->\n",
      "  <h1>\n",
      "   <a href=\"pics.html\">\n",
      "    <img border=\"0\" src=\"images/IMG_2192.jpg\"/>\n",
      "   </a>\n",
      "   Guido van Rossum - Personal Home Page\n",
      "   <a href=\"pics.html\">\n",
      "    <img border=\"0\" height=\"216\" src=\"images/guido-headshot-2019.jpg\" width=\"270\"/>\n",
      "   </a>\n",
      "  </h1>\n",
      "  <p>\n",
      "   <a href=\"http://www.washingtonpost.com/wp-srv/business/longterm/microsoft/stories/1998/raymond120398.htm\">\n",
      "    <i>\n",
      "     \"Gawky and proud of it.\"\n",
      "    </i>\n",
      "   </a>\n",
      "  </p>\n",
      "  <h3>\n",
      "   <a href=\"images/df20000406.jpg\">\n",
      "    Who I Am\n",
      "   </a>\n",
      "  </h3>\n",
      "  <p>\n",
      "   Read\n",
      "my\n",
      "   <a href=\"http://neopythonic.blogspot.com/2016/04/kings-day-speech.html\">\n",
      "    \"King's\n",
      "Day Speech\"\n",
      "   </a>\n",
      "   for some inspiration.\n",
      "  </p>\n",
      "  <p>\n",
      "   I am the author of the\n",
      "   <a href=\"http://www.python.org\">\n",
      "    Python\n",
      "   </a>\n",
      "   programming language.  See also my\n",
      "   <a href=\"Resume.html\">\n",
      "    resume\n",
      "   </a>\n",
      "   and my\n",
      "   <a href=\"Publications.html\">\n",
      "    publications list\n",
      "   </a>\n",
      "   , a\n",
      "   <a href=\"bio.html\">\n",
      "    brief bio\n",
      "   </a>\n",
      "   , assorted\n",
      "   <a href=\"http://legacy.python.org/doc/essays/\">\n",
      "    writings\n",
      "   </a>\n",
      "   ,\n",
      "   <a href=\"http://legacy.python.org/doc/essays/ppt/\">\n",
      "    presentations\n",
      "   </a>\n",
      "   and\n",
      "   <a href=\"interviews.html\">\n",
      "    interviews\n",
      "   </a>\n",
      "   (all about Python), some\n",
      "   <a href=\"pics.html\">\n",
      "    pictures of me\n",
      "   </a>\n",
      "   ,\n",
      "   <a href=\"http://neopythonic.blogspot.com\">\n",
      "    my new blog\n",
      "   </a>\n",
      "   , and\n",
      "my\n",
      "   <a href=\"http://www.artima.com/weblogs/index.jsp?blogger=12088\">\n",
      "    old\n",
      "blog\n",
      "   </a>\n",
      "   on Artima.com.  I am\n",
      "   <a href=\"https://twitter.com/gvanrossum\">\n",
      "    @gvanrossum\n",
      "   </a>\n",
      "   on Twitter.\n",
      "  </p>\n",
      "  <p>\n",
      "   I am retired, working on personal projects (and maybe a book).\n",
      "I have worked for Dropbox, Google, Elemental Security, Zope\n",
      "Corporation, BeOpen.com, CNRI, CWI, and SARA.  (See\n",
      "my\n",
      "   <a href=\"Resume.html\">\n",
      "    resume\n",
      "   </a>\n",
      "   .)  I created Python while at CWI.\n",
      "  </p>\n",
      "  <h3>\n",
      "   How to Reach Me\n",
      "  </h3>\n",
      "  <p>\n",
      "   You can send email for me to guido (at) python.org.\n",
      "I read everything sent there, but I receive too much email to respond\n",
      "to everything.\n",
      "  </p>\n",
      "  <h3>\n",
      "   My Name\n",
      "  </h3>\n",
      "  <p>\n",
      "   My name often poses difficulties for Americans.\n",
      "  </p>\n",
      "  <p>\n",
      "   <b>\n",
      "    Pronunciation:\n",
      "   </b>\n",
      "   in Dutch, the \"G\" in Guido is a hard G,\n",
      "pronounced roughly like the \"ch\" in Scottish \"loch\".  (Listen to the\n",
      "   <a href=\"guido.au\">\n",
      "    sound clip\n",
      "   </a>\n",
      "   .)  However, if you're\n",
      "American, you may also pronounce it as the Italian \"Guido\".  I'm not\n",
      "too worried about the associations with mob assassins that some people\n",
      "have. :-)\n",
      "  </p>\n",
      "  <p>\n",
      "   <b>\n",
      "    Spelling:\n",
      "   </b>\n",
      "   my last name is two words, and I'd like to keep it\n",
      "that way, the spelling on some of my credit cards notwithstanding.\n",
      "Dutch spelling rules dictate that when used in combination with my\n",
      "first name, \"van\" is not capitalized: \"Guido van Rossum\".  But when my\n",
      "last name is used alone to refer to me, it is capitalized, for\n",
      "example: \"As usual, Van Rossum was right.\"\n",
      "  </p>\n",
      "  <p>\n",
      "   <b>\n",
      "    Alphabetization:\n",
      "   </b>\n",
      "   in America, I show up in the alphabet under\n",
      "\"V\".  But in Europe, I show up under \"R\".  And some of my friends put\n",
      "me under \"G\" in their address book...\n",
      "  </p>\n",
      "  <h3>\n",
      "   More Hyperlinks\n",
      "  </h3>\n",
      "  <ul>\n",
      "   <li>\n",
      "    Here's a collection of\n",
      "    <a href=\"http://legacy.python.org/doc/essays/\">\n",
      "     essays\n",
      "    </a>\n",
      "    relating to Python\n",
      "that I've written, including the foreword I wrote for Mark Lutz' book\n",
      "\"Programming Python\".\n",
      "    <p>\n",
      "    </p>\n",
      "   </li>\n",
      "   <li>\n",
      "    I own the official\n",
      "    <a href=\"images/license.jpg\">\n",
      "     <img align=\"center\" border=\"0\" height=\"75\" src=\"images/license_thumb.jpg\" width=\"100\"/>\n",
      "     Python license.\n",
      "    </a>\n",
      "    <p>\n",
      "    </p>\n",
      "   </li>\n",
      "  </ul>\n",
      "  <h3>\n",
      "   The Audio File Formats FAQ\n",
      "  </h3>\n",
      "  <p>\n",
      "   I was the original creator and maintainer of the Audio File Formats\n",
      "FAQ.  It is now maintained by Chris Bagwell\n",
      "at\n",
      "   <a href=\"http://www.cnpbagwell.com/audio-faq\">\n",
      "    http://www.cnpbagwell.com/audio-faq\n",
      "   </a>\n",
      "   .  And here is a link to\n",
      "   <a href=\"http://sox.sourceforge.net/\">\n",
      "    SOX\n",
      "   </a>\n",
      "   , to which I contributed\n",
      "some early code.\n",
      "  </p>\n",
      "  <hr/>\n",
      "  <a href=\"images/internetdog.gif\">\n",
      "   \"On the Internet, nobody knows you're\n",
      "a dog.\"\n",
      "  </a>\n",
      "  <hr/>\n",
      " </body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "#Parsing HTML with BeautifulSoup\n",
    "\n",
    "#In this interactive exercise, you'll learn how to use the \n",
    "#BeautifulSoup package to parse, prettify and extract \n",
    "#information from HTML. You'll scrape the data from the \n",
    "#webpage of Guido van Rossum, Python's very own \n",
    "#Benevolent Dictator for Life. In the following exercises, \n",
    "#you'll prettify the HTML and then extract the text and the \n",
    "#hyperlinks.\n",
    "\n",
    "#The URL of interest is \n",
    "#url = 'https://www.python.org/~guido/'.\n",
    "\n",
    "#Instructions\n",
    "\n",
    "#Import the function BeautifulSoup from the package bs4.\n",
    "\n",
    "#Assign the URL of interest to the variable url.\n",
    "\n",
    "#Package the request to the URL, send the request and \n",
    "#catch the response with a single function \n",
    "#requests.get(), assigning the response to the variable \n",
    "#r.\n",
    "\n",
    "#Use the text attribute of the object r to return the \n",
    "#HTML of the webpage as a string; store the result in a \n",
    "#variable html_doc.\n",
    "\n",
    "#Create a BeautifulSoup object soup from the resulting \n",
    "#HTML using the function BeautifulSoup().\n",
    "\n",
    "#Use the method prettify() on soup and assign the \n",
    "#result to pretty_soup.\n",
    "\n",
    "#Hit submit to print to prettified HTML to your shell!\n",
    "\n",
    "# Import packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Specify url: url\n",
    "url='https://www.python.org/~guido/'\n",
    "\n",
    "# Package the request, send the request and catch the response: r\n",
    "r=requests.get(url)\n",
    "\n",
    "# Extracts the response as html: html_doc\n",
    "html_doc=r.text\n",
    "\n",
    "# Create a BeautifulSoup object from the HTML: soup\n",
    "soup=BeautifulSoup(html_doc)\n",
    "\n",
    "# Prettify the BeautifulSoup object: pretty_soup\n",
    "pretty_soup=soup.prettify()\n",
    "\n",
    "# Print the response\n",
    "print(pretty_soup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb2a2c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>Guido's Personal Home Page</title>\n",
      "pics.html\n",
      "pics.html\n",
      "http://www.washingtonpost.com/wp-srv/business/longterm/microsoft/stories/1998/raymond120398.htm\n",
      "images/df20000406.jpg\n",
      "http://neopythonic.blogspot.com/2016/04/kings-day-speech.html\n",
      "http://www.python.org\n",
      "Resume.html\n",
      "Publications.html\n",
      "bio.html\n",
      "http://legacy.python.org/doc/essays/\n",
      "http://legacy.python.org/doc/essays/ppt/\n",
      "interviews.html\n",
      "pics.html\n",
      "http://neopythonic.blogspot.com\n",
      "http://www.artima.com/weblogs/index.jsp?blogger=12088\n",
      "https://twitter.com/gvanrossum\n",
      "Resume.html\n",
      "guido.au\n",
      "http://legacy.python.org/doc/essays/\n",
      "images/license.jpg\n",
      "http://www.cnpbagwell.com/audio-faq\n",
      "http://sox.sourceforge.net/\n",
      "images/internetdog.gif\n"
     ]
    }
   ],
   "source": [
    "#Turning a webpage into data using \n",
    "#BeautifulSoup: getting the hyperlinks\n",
    "\n",
    "#In this exercise, you'll figure out how to extract the URLs of \n",
    "#the hyperlinks from the BDFL's webpage. In the process, \n",
    "#you'll become close friends with the soup method \n",
    "#find_all().\n",
    "\n",
    "#Instructions\n",
    "\n",
    "#Use the method find_all() to find all hyperlinks in \n",
    "#soup, remembering that hyperlinks are defined by the \n",
    "#HTML tag <a> but passed to find_all() without angle \n",
    "#brackets; store the result in the variable a_tags.\n",
    "\n",
    "#The variable a_tags is a results set: your job now is to \n",
    "#enumerate over it, using a for loop and to print the \n",
    "#actual URLs of the hyperlinks; to do this, for every \n",
    "#element link in a_tags, you want to print() \n",
    "#link.get('href').\n",
    "\n",
    "# Import packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Specify url\n",
    "url = 'https://www.python.org/~guido/'\n",
    "\n",
    "# Package the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Extracts the response as html: html_doc\n",
    "html_doc = r.text\n",
    "\n",
    "# create a BeautifulSoup object from the HTML: soup\n",
    "soup = BeautifulSoup(html_doc)\n",
    "\n",
    "# Print the title of Guido's webpage\n",
    "print(soup.title)\n",
    "\n",
    "# Find all 'a' tags (which define hyperlinks): a_tags\n",
    "a_tags=soup.find_all('a')\n",
    "\n",
    "# Print the URLs to the shell\n",
    "for link in a_tags:\n",
    "    print(link.get('href'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "932a5b5d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'a_movie.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\LUISHE~1\\AppData\\Local\\Temp/ipykernel_15972/4134862914.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# Load JSON: json_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"a_movie.json\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0mjson_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'a_movie.json'"
     ]
    }
   ],
   "source": [
    "#Loading and exploring a JSON\n",
    "\n",
    "#Now that you know what a JSON is, you'll load one into \n",
    "#your Python environment and explore it yourself. Here, \n",
    "#you'll load the JSON 'a_movie.json' into the variable \n",
    "#json_data, which will be a dictionary. You'll then explore \n",
    "#the JSON contents by printing the key-value pairs of \n",
    "#json_data to the shell.\n",
    "\n",
    "#Instructions\n",
    "\n",
    "#Load the JSON 'a_movie.json' into the variable \n",
    "#json_data within the context provided by the with \n",
    "#statement. To do so, use the function json.load() within \n",
    "#the context manager.\n",
    "\n",
    "#Use a for loop to print all key-value pairs in the \n",
    "#dictionary json_data. Recall that you can access a \n",
    "#value in a dictionary using the syntax: dictionary[key].\n",
    "\n",
    "# Load JSON: json_data\n",
    "with open(\"a_movie.json\") as json_file:\n",
    "    json_data=json.load(json_file)\n",
    "\n",
    "# Print each key-value pair in json_data\n",
    "for k in json_data.keys():\n",
    "    print(k + ': ', json_data[k])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "192ecb38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"Title\":\"The Social Network\",\"Year\":\"2010\",\"Rated\":\"PG-13\",\"Released\":\"01 Oct 2010\",\"Runtime\":\"120 min\",\"Genre\":\"Biography, Drama\",\"Director\":\"David Fincher\",\"Writer\":\"Aaron Sorkin, Ben Mezrich\",\"Actors\":\"Jesse Eisenberg, Andrew Garfield, Justin Timberlake\",\"Plot\":\"As Harvard student Mark Zuckerberg creates the social networking site that would become known as Facebook, he is sued by the twins who claimed he stole their idea, and by the co-founder who was later squeezed out of the business.\",\"Language\":\"English, French\",\"Country\":\"United States\",\"Awards\":\"Won 3 Oscars. 172 wins & 186 nominations total\",\"Poster\":\"https://m.media-amazon.com/images/M/MV5BOGUyZDUxZjEtMmIzMC00MzlmLTg4MGItZWJmMzBhZjE0Mjc1XkEyXkFqcGdeQXVyMTMxODk2OTU@._V1_SX300.jpg\",\"Ratings\":[{\"Source\":\"Internet Movie Database\",\"Value\":\"7.7/10\"},{\"Source\":\"Rotten Tomatoes\",\"Value\":\"96%\"},{\"Source\":\"Metacritic\",\"Value\":\"95/100\"}],\"Metascore\":\"95\",\"imdbRating\":\"7.7\",\"imdbVotes\":\"663,951\",\"imdbID\":\"tt1285016\",\"Type\":\"movie\",\"DVD\":\"11 Jan 2011\",\"BoxOffice\":\"$96,962,694\",\"Production\":\"N/A\",\"Website\":\"N/A\",\"Response\":\"True\"}\n"
     ]
    }
   ],
   "source": [
    "#API requests\n",
    "\n",
    "#Now it's your turn to pull some movie data down from the \n",
    "#Open Movie Database (OMDB) using their API. The movie \n",
    "#you'll query the API about is The Social Network. Recall \n",
    "#that, in the video, to query the API about the movie \n",
    "#Hackers, Hugo's query string was \n",
    "#'http://www.omdbapi.com/?t=hackers' and had a single \n",
    "#argument t=hackers.\n",
    "\n",
    "#Note: recently, OMDB has changed their API: you now also \n",
    "#have to specify an API key. This means you'll have to add \n",
    "#another argument to the URL: apikey=72bc447a.\n",
    "\n",
    "#Instructions\n",
    "\n",
    "#Import the requests package.\n",
    "\n",
    "#Assign to the variable url the URL of interest in order to \n",
    "#query 'http://www.omdbapi.com' for the data \n",
    "#corresponding to the movie The Social Network. The \n",
    "#query string should have two arguments: \n",
    "#apikey=72bc447a and t=the+social+network. You can \n",
    "#combine them as follows:\n",
    "# apikey=72bc447a&t=the+social+network.\n",
    "\n",
    "#Print the text of the response object r by using its text \n",
    "#attribute and passing the result to the print() function.\n",
    "\n",
    "# Import requests package\n",
    "import requests\n",
    "\n",
    "# Assign URL to variable: url\n",
    "url='http://www.omdbapi.com/?apikey=72bc447a&t=the+social+network'\n",
    "\n",
    "# Package the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Print the text of the response\n",
    "print(r.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f283d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title:  The Social Network\n",
      "Year:  2010\n",
      "Rated:  PG-13\n",
      "Released:  01 Oct 2010\n",
      "Runtime:  120 min\n",
      "Genre:  Biography, Drama\n",
      "Director:  David Fincher\n",
      "Writer:  Aaron Sorkin, Ben Mezrich\n",
      "Actors:  Jesse Eisenberg, Andrew Garfield, Justin Timberlake\n",
      "Plot:  As Harvard student Mark Zuckerberg creates the social networking site that would become known as Facebook, he is sued by the twins who claimed he stole their idea, and by the co-founder who was later squeezed out of the business.\n",
      "Language:  English, French\n",
      "Country:  United States\n",
      "Awards:  Won 3 Oscars. 172 wins & 186 nominations total\n",
      "Poster:  https://m.media-amazon.com/images/M/MV5BOGUyZDUxZjEtMmIzMC00MzlmLTg4MGItZWJmMzBhZjE0Mjc1XkEyXkFqcGdeQXVyMTMxODk2OTU@._V1_SX300.jpg\n",
      "Ratings:  [{'Source': 'Internet Movie Database', 'Value': '7.7/10'}, {'Source': 'Rotten Tomatoes', 'Value': '96%'}, {'Source': 'Metacritic', 'Value': '95/100'}]\n",
      "Metascore:  95\n",
      "imdbRating:  7.7\n",
      "imdbVotes:  663,951\n",
      "imdbID:  tt1285016\n",
      "Type:  movie\n",
      "DVD:  11 Jan 2011\n",
      "BoxOffice:  $96,962,694\n",
      "Production:  N/A\n",
      "Website:  N/A\n",
      "Response:  True\n"
     ]
    }
   ],
   "source": [
    "#JSONâ€“from the web to Python\n",
    "\n",
    "#Wow, congrats! You've just queried your first API \n",
    "#programmatically in Python and printed the text of the \n",
    "#response to the shell. However, as you know, your response \n",
    "#is actually a JSON, so you can do one step better and \n",
    "#decode the JSON. You can then print the key-value pairs \n",
    "#of the resulting dictionary. That's what you're going to do \n",
    "#now!\n",
    "\n",
    "#Instructions\n",
    "\n",
    "#Pass the variable url to the requests.get() function in \n",
    "#order to send the relevant request and catch the \n",
    "#response, assigning the resultant response message to \n",
    "#the variable r.\n",
    "\n",
    "#Apply the json() method to the response object r and \n",
    "#store the resulting dictionary in the variable json_data.\n",
    "\n",
    "#Hit Submit Answer to print the key-value pairs of the \n",
    "#dictionary json_data to the shell.\n",
    "\n",
    "# Import package\n",
    "import requests\n",
    "\n",
    "# Assign URL to variable: url\n",
    "url = 'http://www.omdbapi.com/?apikey=72bc447a&t=social+network'\n",
    "\n",
    "# Package the request, send the request and catch the response: r\n",
    "r=requests.get(url)\n",
    "\n",
    "# Decode the JSON data into a dictionary: json_data\n",
    "json_data=r.json()\n",
    "\n",
    "# Print each key-value pair in json_data\n",
    "for k in json_data.keys():\n",
    "    print(k + ': ', json_data[k])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73d9bc5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r1033289096\">\n",
      "<p class=\"mw-empty-elt\">\n",
      "</p>\n",
      "<p><b>Pizza</b> (<small>Italian:Â </small><span title=\"Representation in the International Phonetic Alphabet (IPA)\">[Ëˆpittsa]</span>, <small>Neapolitan:Â </small><span title=\"Representation in the International Phonetic Alphabet (IPA)\">[ËˆpittsÉ™]</span>) is a dish of  Italian origin consisting of a usually round, flat base of leavened wheat-based dough topped with tomatoes, cheese, and often various other ingredients (such as anchovies, mushrooms, onions, olives, pineapple, meat, etc.), which is then baked at a high temperature, traditionally in a wood-fired oven. A small pizza is sometimes called a pizzetta. A person who makes pizza is known as a <b>pizzaiolo</b>.\n",
      "</p><p>In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced, and is eaten with the use of a knife and fork. In casual settings, however, it is cut into wedges to be eaten while held in the hand.\n",
      "</p><p>The term <i>pizza</i> was first recorded in the 10th century in a Latin manuscript from the Southern Italian town of Gaeta in Lazio, on the border with Campania. Modern pizza was invented in Naples, and the dish and its variants have since become popular in many countries. It has become one of the most popular foods in the world and a common fast food item in Europe, North America and Australasia; available at pizzerias (restaurants specializing in pizza),  restaurants offering Mediterranean cuisine, via pizza delivery, and as street food. Various food companies sell ready-baked pizzas, which may be frozen, in grocery stores, to be reheated in a home oven.\n",
      "</p><p>In 2017, the world pizza market was US$128 billion, and in the US it was $44 billion spread over 76,000 pizzerias.  Overall, 13% of the U.S. population aged 2 years and over consumed pizza on any given day.</p><p>The <i>Associazione Verace Pizza Napoletana</i> (lit. True Neapolitan Pizza Association) is a non-profit organization founded in 1984 with headquarters in Naples that aims to promote traditional Neapolitan pizza. In 2009, upon Italy's request, Neapolitan pizza was registered with the European Union as a Traditional Speciality Guaranteed dish, and in 2017 the art of its making was included on UNESCO's list of intangible cultural heritage.</p>\n",
      "\n",
      "<!-- \n",
      "NewPP limit report\n",
      "Parsed by mw1378\n",
      "Cached time: 20220113163855\n",
      "Cache expiry: 1814400\n",
      "Reduced expiry: false\n",
      "Complications: [varyâ€revisionâ€exists, varyâ€revisionâ€sha1]\n",
      "CPU time usage: 0.539 seconds\n",
      "Real time usage: 0.674 seconds\n",
      "Preprocessor visited node count: 1100/1000000\n",
      "Postâ€expand include size: 43804/2097152 bytes\n",
      "Template argument size: 1196/2097152 bytes\n",
      "Highest expansion depth: 17/40\n",
      "Expensive parser function count: 5/500\n",
      "Unstrip recursion depth: 0/20\n",
      "Unstrip postâ€expand size: 21164/5000000 bytes\n",
      "Lua time usage: 0.339/10.000 seconds\n",
      "Lua memory usage: 5225979/52428800 bytes\n",
      "Number of Wikibase entities loaded: 1/400\n",
      "-->\n",
      "<!--\n",
      "Transclusion expansion time report (%,ms,calls,template)\n",
      "100.00%  595.972      1 -total\n",
      " 21.88%  130.397      1 Template:OED\n",
      " 18.16%  108.225      1 Template:Infobox_food\n",
      " 16.80%  100.149      1 Template:Infobox\n",
      " 12.85%   76.562      1 Template:Pp-semi-indef\n",
      " 11.36%   67.726      1 Template:Unbulleted_list\n",
      " 10.89%   64.879      1 Template:Short_description\n",
      " 10.80%   64.393      1 Template:Pizza\n",
      " 10.29%   61.303      1 Template:Sidebar_with_collapsible_lists\n",
      "  6.64%   39.568      1 Template:Pagetype\n",
      "-->\n"
     ]
    }
   ],
   "source": [
    "#Checking out the Wikipedia API\n",
    "\n",
    "#You're doing so well and having so much fun that we're \n",
    "#going to throw one more API at you: the Wikipedia API \n",
    "#(documented here). You'll figure out how to find and \n",
    "#extract information from the Wikipedia page for Pizza. \n",
    "#What gets a bit wild here is that your query will return \n",
    "#nested JSONs, that is, JSONs with JSONs, but Python can \n",
    "#handle that because it will translate them into dictionaries \n",
    "#within dictionaries.\n",
    "\n",
    "#The URL that requests the relevant query from the \n",
    "#Wikipedia API is\n",
    "\n",
    "#https://en.wikipedia.org/w/api.php?action=query&prop=extracts&format=json&exintro=&titles=pizza\n",
    "\n",
    "#Instructions\n",
    "\n",
    "#Assign the relevant URL to the variable url.\n",
    "\n",
    "#Apply the json() method to the response object r and \n",
    "#store the resulting dictionary in the variable json_data.\n",
    "\n",
    "#The variable pizza_extract holds the HTML of an \n",
    "#extract from Wikipedia's Pizza page as a string; use the \n",
    "#function print() to print this string to the shell.\n",
    "\n",
    "# Import package\n",
    "import requests\n",
    "\n",
    "# Assign URL to variable: url\n",
    "url='https://en.wikipedia.org/w/api.php?action=query&prop=extracts&format=json&exintro=&titles=pizza'\n",
    "\n",
    "\n",
    "# Package the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Decode the JSON data into a dictionary: json_data\n",
    "json_data=r.json()\n",
    "\n",
    "# Print the Wikipedia page extract\n",
    "pizza_extract = json_data['query']['pages']['24768']['extract']\n",
    "print(pizza_extract)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efe8031d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#API Authentication\n",
    "\n",
    "#The package tweepy is great at handling all the Twitter \n",
    "#API OAuth Authentication details for you. All you need to \n",
    "#do is pass it your authentication credentials. In this \n",
    "#interactive exercise, we have created some mock \n",
    "#authentication credentials (if you wanted to replicate this \n",
    "#at home, you would need to create a Twitter App as Hugo \n",
    "#detailed in the video). Your task is to pass these credentials \n",
    "#to tweepy's OAuth handler.\n",
    "\n",
    "#Instructions\n",
    "\n",
    "#Import the package tweepy.\n",
    "\n",
    "#Pass the parameters consumer_key and \n",
    "#consumer_secret to the function \n",
    "#tweepy.OAuthHandler().\n",
    "\n",
    "#Complete the passing of OAuth credentials to the OAuth \n",
    "#handler auth by applying to it the method \n",
    "#set_access_token(), along with arguments \n",
    "#access_token and access_token_secret.\n",
    "\n",
    "# Import package\n",
    "import tweepy\n",
    "\n",
    "# Store OAuth authentication credentials in relevant variables\n",
    "access_token = \"1092294848-aHN7DcRP9B4VMTQIhwqOYiB14YkW92fFO8k8EPy\"\n",
    "access_token_secret = \"X4dHmhPfaksHcQ7SCbmZa2oYBBVSD2g8uIHXsp5CTaksx\"\n",
    "consumer_key = \"nZ6EA0FxZ293SxGNg8g8aP0HM\"\n",
    "consumer_secret = \"fJGEodwe3KiKUnsYJC3VRndj7jevVvXbK2D5EiJ2nehafRgA6i\"\n",
    "\n",
    "# Pass OAuth details to tweepy's OAuth handler\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token,access_token_secret)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cfc2ec33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from tweepy import Stream\n",
    "from tweepy.streaming import Stream\n",
    "\n",
    "class MyStreamListener(tweepy.Stream):\n",
    "    def __init__(self, api=None):\n",
    "        super(MyStreamListener, self).__init__()\n",
    "        self.num_tweets = 0\n",
    "        self.file = open(\"tweets.txt\", \"w\")\n",
    "\n",
    "    def on_status(self, status):\n",
    "        tweet = status._json\n",
    "        self.file.write( json.dumps(tweet) + '\\n' )\n",
    "        self.num_tweets += 1\n",
    "        if self.num_tweets < 100:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        self.file.close()\n",
    "\n",
    "    def on_error(self, status):\n",
    "        print(status)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7066df08",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Stream.__init__() missing 4 required positional arguments: 'consumer_key', 'consumer_secret', 'access_token', and 'access_token_secret'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\LUISHE~1\\AppData\\Local\\Temp/ipykernel_15972/1760788670.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# Initialize Stream listener\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0ml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMyStreamListener\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;31m# Create your Stream object with authentication\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\LUISHE~1\\AppData\\Local\\Temp/ipykernel_15972/1412009232.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, api)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mMyStreamListener\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweepy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStream\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mapi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMyStreamListener\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_tweets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"tweets.txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"w\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Stream.__init__() missing 4 required positional arguments: 'consumer_key', 'consumer_secret', 'access_token', and 'access_token_secret'"
     ]
    }
   ],
   "source": [
    "#Streaming tweets\n",
    "\n",
    "#Now that you have set up your authentication credentials, \n",
    "#it is time to stream some tweets! We have already defined \n",
    "#the tweet stream listener class, MyStreamListener, just as \n",
    "#Hugo did in the introductory video. You can find the code \n",
    "#for the tweet stream listener class here.\n",
    "\n",
    "#Your task is to create the Streamobject and to filter \n",
    "#tweets according to particular keywords.\n",
    "\n",
    "#Instructions\n",
    "\n",
    "#Create your Stream object with authentication by \n",
    "#passing tweepy.Stream() the authentication handler \n",
    "#auth and the Stream listener l;\n",
    "\n",
    "#To filter Twitter streams, pass to the track argument in \n",
    "#stream.filter() a list containing the desired keywords \n",
    "#'clinton', 'trump', 'sanders', and 'cruz'.\n",
    "\n",
    "\n",
    "# Initialize Stream listener\n",
    "l = MyStreamListener()\n",
    "\n",
    "# Create your Stream object with authentication\n",
    "stream = tweepy.Stream(auth, l)\n",
    "\n",
    "# Filter Twitter Streams to capture data by the keywords:\n",
    "stream.filter(track=['clinton','trump','sanders','cruz'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4d2bf94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['in_reply_to_user_id', 'created_at', 'filter_level', 'truncated', 'possibly_sensitive', 'timestamp_ms', 'user', 'text', 'extended_entities', 'in_reply_to_status_id', 'entities', 'favorited', 'retweeted', 'is_quote_status', 'id', 'favorite_count', 'retweeted_status', 'in_reply_to_status_id_str', 'in_reply_to_user_id_str', 'id_str', 'in_reply_to_screen_name', 'coordinates', 'lang', 'place', 'contributors', 'geo', 'retweet_count', 'source'])\n"
     ]
    }
   ],
   "source": [
    "#Load and explore your Twitter data\n",
    "\n",
    "#Now that you've got your Twitter data sitting locally in a \n",
    "#text file, it's time to explore it! This is what you'll do in the \n",
    "#next few interactive exercises. In this exercise, you'll read \n",
    "#the Twitter data into a list: tweets_data.\n",
    "\n",
    "#Be aware that this is real data from Twitter and as such \n",
    "#there is always a risk that it may contain profanity or other \n",
    "#offensive content (in this exercise, and any following \n",
    "#exercises that also use real Twitter data).\n",
    "\n",
    "#Instructions\n",
    "\n",
    "#Assign the filename 'tweets.txt' to the variable \n",
    "#tweets_data_path.\n",
    "\n",
    "#Initialize tweets_data as an empty list to store the \n",
    "#tweets in.\n",
    "\n",
    "#Within the for loop initiated by \n",
    "#for line in tweets_file:, load each tweet into a \n",
    "#variable, tweet, using json.loads(), then append \n",
    "#tweet to tweets_data using the append() method.\n",
    "\n",
    "#Hit submit and check out the keys of the first tweet \n",
    "#dictionary printed to the shell.\n",
    "\n",
    "# Import package\n",
    "import json\n",
    "\n",
    "# String of path to file: tweets_data_path\n",
    "tweets_data_path='tweets.txt'\n",
    "\n",
    "# Initialize empty list to store tweets: tweets_data\n",
    "tweets_data=[]\n",
    "\n",
    "# Open connection to file\n",
    "tweets_file = open(tweets_data_path, \"r\")\n",
    "\n",
    "# Read in tweets and store in list: tweets_data\n",
    "for line in tweets_file:\n",
    "    tweet=json.loads(line)\n",
    "    tweets_data.append(tweet)\n",
    "\n",
    "# Close connection to file\n",
    "tweets_file.close()\n",
    "\n",
    "# Print the keys of the first tweet dict\n",
    "print(tweets_data[0].keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a71e7b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text lang\n",
      "0  RT @bpolitics: .@krollbondrating's Christopher...   en\n",
      "1  RT @HeidiAlpine: @dmartosko Cruz video found.....   en\n",
      "2  Njihuni me ZonjÃ«n Trump !!! | Ekskluzive https...   et\n",
      "3  Your an idiot she shouldn't have tried to grab...   en\n",
      "4  RT @AlanLohner: The anti-American D.C. elites ...   en\n"
     ]
    }
   ],
   "source": [
    "#Twitter data to DataFrame\n",
    "\n",
    "#Now you have the Twitter data in a list of dictionaries, \n",
    "#tweets_data, where each dictionary corresponds to a \n",
    "#single tweet. Next, you're going to extract the text and \n",
    "#language of each tweet. The text in a tweet, t1, is stored \n",
    "#as the value t1['text']; similarly, the language is stored \n",
    "#in t1['lang']. Your task is to build a DataFrame in which \n",
    "#each row is a tweet and the columns are 'text' and 'lang'.\n",
    "\n",
    "#Instructions\n",
    "\n",
    "#Use pd.DataFrame() to construct a DataFrame of tweet \n",
    "#texts and languages; to do so, the first argument should \n",
    "#be tweets_data, a list of dictionaries. The second \n",
    "#argument to pd.DataFrame() is a list of the keys you \n",
    "#wish to have as columns. Assign the result of the \n",
    "#pd.DataFrame() call to df.\n",
    "\n",
    "# Import package\n",
    "import pandas as pd\n",
    "\n",
    "# Build DataFrame of tweet texts and languages\n",
    "df = pd.DataFrame(tweets_data, columns=['text','lang'])\n",
    "\n",
    "# Print head of DataFrame\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d37f7fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A little bit of Twitter text analysis\n",
    "\n",
    "#Now that you have your DataFrame of tweets set up, \n",
    "#you're going to do a bit of text analysis to count how many \n",
    "#tweets contain the words 'clinton', 'trump', \n",
    "#'sanders' and 'cruz'. In the pre-exercise code, we have \n",
    "#defined the following function word_in_text(), which will \n",
    "#tell you whether the first argument (a word) occurs within \n",
    "#the 2nd argument (a tweet).\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "def word_in_text(word, text):\n",
    "    word = word.lower()\n",
    "    text = text.lower()\n",
    "    match = re.search(word, text)\n",
    "\n",
    "    if match:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "#You're going to iterate over the rows of the DataFrame and \n",
    "#calculate how many tweets contain each of our keywords! \n",
    "#The list of objects for each candidate has been initialized \n",
    "#to 0.\n",
    "\n",
    "#Instructions\n",
    "\n",
    "#Within the for loop \n",
    "#for index, row in df.iterrows():, the code currently \n",
    "#increases the value of clinton by 1 each time a tweet \n",
    "#(text row) mentioning 'Clinton' is encountered; complete \n",
    "#the code so that the same happens for trump, sanders\n",
    "# and cruz.\n",
    "\n",
    "# Initialize list to store tweet counts\n",
    "[clinton, trump, sanders, cruz] = [0, 0, 0, 0]\n",
    "\n",
    "# Iterate through df, counting the number of tweets in which\n",
    "# each candidate is mentioned\n",
    "for index, row in df.iterrows():\n",
    "    clinton += word_in_text('clinton', row['text'])\n",
    "    trump += word_in_text('trump', row['text'])\n",
    "    sanders += word_in_text('sanders', row['text'])\n",
    "    cruz += word_in_text('cruz', row['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c04a70e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luis hernandez\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plotting your Twitter data\n",
    "\n",
    "#Now that you have the number of tweets that each \n",
    "#candidate was mentioned in, you can plot a bar chart of \n",
    "#this data. You'll use the statistical data visualization library \n",
    "#seaborn, which you may not have seen before, but we'll \n",
    "#guide you through. You'll first import seaborn as sns. \n",
    "#You'll then construct a barplot of the data using \n",
    "#sns.barplot, passing it two arguments:\n",
    "\n",
    "#1. a list of labels and\n",
    "#2. a list containing the variables you wish to plot (\n",
    "#clinton, trump and so on.)\n",
    "\n",
    "#Hopefully, you'll see that Trump was unreasonably \n",
    "#represented! We have already run the previous exercise \n",
    "#solutions in your environment.\n",
    "\n",
    "#Instructions\n",
    "\n",
    "#Import both matplotlib.pyplot and seaborn using the \n",
    "#aliases plt and sns, respectively.\n",
    "\n",
    "#Complete the arguments of sns.barplot:\n",
    "\n",
    "#The first argument should be the list of labels to \n",
    "#appear on the x-axis (created in the previous step).\n",
    "\n",
    "#The second argument should be a list of the variables\n",
    " #you wish to plot, as produced in the previous exercise\n",
    " #(i.e. a list containing clinton, trump, etc).\n",
    "\n",
    "# Import packages\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Set seaborn style\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "# Create a list of labels:cd\n",
    "cd = ['clinton', 'trump', 'sanders', 'cruz']\n",
    "\n",
    "# Plot the bar chart\n",
    "ax = sns.barplot(cd,[clinton,trump,sanders,cruz])\n",
    "ax.set(ylabel=\"count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2affe4d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
